<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-fruit.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-fruit.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-fruit.png">
  <link rel="mask-icon" href="/images/logo.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://msgsxj.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="前言: 本文从优化的角度整理了随机梯度法中的常用的一类算法: 噪声缩减方法, 这里的噪声指的是对随机样本计算梯度带来的噪声. 主要参考了Léon Bottou的综述Optimization Methods for Large-Scale Machine Learning、Jorge Nocedal的Numerical Optimization(2nd edition)以及这篇博客.">
<meta property="og:type" content="article">
<meta property="og:title" content="随机梯度法中的噪声减小方法">
<meta property="og:url" content="https://msgsxj.cn/2019/10/28/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%AD%E7%9A%84%E5%99%AA%E5%A3%B0%E5%87%8F%E5%B0%8F%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="msgsxj&#39;blog">
<meta property="og:description" content="前言: 本文从优化的角度整理了随机梯度法中的常用的一类算法: 噪声缩减方法, 这里的噪声指的是对随机样本计算梯度带来的噪声. 主要参考了Léon Bottou的综述Optimization Methods for Large-Scale Machine Learning、Jorge Nocedal的Numerical Optimization(2nd edition)以及这篇博客.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://msgsxj.cn/pictures/SVRG.png">
<meta property="og:image" content="https://msgsxj.cn/pictures/SAGA.png">
<meta property="og:image" content="https://msgsxj.cn/pictures/SARAH.png">
<meta property="og:image" content="https://msgsxj.cn/pictures/SARAH+.png">
<meta property="article:published_time" content="2019-10-28T08:00:18.000Z">
<meta property="article:modified_time" content="2020-03-30T04:38:36.093Z">
<meta property="article:author" content="msgsxj">
<meta property="article:tag" content="SG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://msgsxj.cn/pictures/SVRG.png">

<link rel="canonical" href="https://msgsxj.cn/2019/10/28/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%AD%E7%9A%84%E5%99%AA%E5%A3%B0%E5%87%8F%E5%B0%8F%E6%96%B9%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>随机梯度法中的噪声减小方法 | msgsxj'blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-148974741-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-148974741-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?1bc9ef392b6302f7b4918f79882fbe82";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">msgsxj'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://msgsxj.cn/2019/10/28/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%AD%E7%9A%84%E5%99%AA%E5%A3%B0%E5%87%8F%E5%B0%8F%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="msgsxj">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="msgsxj'blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          随机梯度法中的噪声减小方法
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-28 16:00:18" itemprop="dateCreated datePublished" datetime="2019-10-28T16:00:18+08:00">2019-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-30 12:38:36" itemprop="dateModified" datetime="2020-03-30T12:38:36+08:00">2020-03-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">优化算法</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>前言: 本文从优化的角度整理了随机梯度法中的常用的一类算法: 噪声缩减方法, 这里的噪声指的是对随机样本计算梯度带来的噪声. 主要参考了Léon Bottou的综述<a href="https://arxiv.org/pdf/1606.04838.pdf" target="_blank" rel="noopener">Optimization Methods for Large-Scale Machine Learning</a>、Jorge Nocedal的Numerical Optimization(2nd edition)以及<a href="http://caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">这篇博客</a>. <a id="more"></a></p>
<h2 id="机器学习中的优化">机器学习中的优化</h2>
<h3 id="一些记号">一些记号</h3>
<p>首先给出两个基本记号, 这是机器学习中要考虑的两种优化基本函数, 前者是定义在<span class="math inline">\(n\)</span>个样本上的函数, 而后者是定义在样本的概率空间上的函数.</p>
<ul>
<li><span class="math inline">\(R_n(w)=\frac{1}{n}\sum_{i=1}^nf_i(w)\)</span>表示经验风险.</li>
<li><span class="math inline">\(R(w)=\mathbb{E}[f(w;\xi)]\)</span>表示期望风险.</li>
</ul>
<p>而关于最优解<span class="math inline">\(w\)</span>有三种:</p>
<ul>
<li>第一种是<span class="math inline">\(w_{*}\in \arg\min R(w)\)</span>.</li>
<li>第二种是<span class="math inline">\(w_{n}\in \arg\min R_n(w)\)</span>.</li>
<li>第三种是实际运行算法返回的最优解<span class="math inline">\(\tilde{w}_n\)</span>, 其受制于具体优化算法的计算.</li>
</ul>
<p><strong>注意</strong>: 下面用<span class="math inline">\(F(w)\)</span>表示<span class="math inline">\(R_n(w)\)</span>或者<span class="math inline">\(R(w)\)</span>, 意在区分只适用<span class="math inline">\(R_n(w)\)</span>和<span class="math inline">\(R_n(w), R(w)\)</span>均适用的算法</p>
<h3 id="梯度下降法">梯度下降法</h3>
<p>梯度下降法(gradient descent, GD), 只适用于<span class="math inline">\(R_n(w)\)</span>, 在强凸前提下其收敛速度为线性[Numerical Optimization TH3.4]:<span class="math display">\[R_n(w_k)-R_*\le O(\rho^k)\]</span>其中<span class="math inline">\(\rho\in(0,1)\)</span>. 因此GD的计算复杂度为:<span class="math display">\[O(n\kappa\log(1/\epsilon))\]</span>因为<span class="math inline">\(O(\rho^k)=\epsilon \Rightarrow k=O(\log(1/\epsilon))\)</span>且每次迭代要计算<span class="math inline">\(n\)</span>个样本的梯度.</p>
<p><strong>注意</strong>: <span class="math inline">\(\kappa=L/\mu\)</span>为优化中常用的常数, 可参见[leon bottou]中的Assumption4.1 4.3</p>
<h3 id="随机梯度">随机梯度</h3>
<p>随机梯度(stochastic gradient, SG), 为梯度下降法的随机版本, 但因为每次迭代不能保证下降, 因此没有descent的字眼, 它是适用于<span class="math inline">\(F(w)\)</span>的算法. 在强凸(步长满足<span class="math inline">\(\alpha_k=\frac{\beta}{\gamma+k}\)</span>)及期望的意义下其收敛速率为次线性[leon bottou TH4.7] :<span class="math display">\[\mathbb{E}[F(w_k)-F_*]\le \frac{\nu}{\gamma+k}\]</span>其中<span class="math inline">\(\beta, \nu\)</span>为常数. 因此SG的计算复杂度为:<span class="math display">\[O(\kappa/\epsilon)\]</span>因为<span class="math inline">\(\frac{\nu}{\gamma+k}=\epsilon \Rightarrow k=\frac{\nu}{\epsilon}-\gamma\)</span></p>
<h2 id="噪声减小方法">噪声减小方法</h2>
<h3 id="迭代平均方法">迭代平均方法</h3>
<p>先来看最容易想到的一类方法, 迭代平均方法, 它直接基于SG因此适用<span class="math inline">\(F(w)\)</span>. 考虑SG迭代序列<span class="math display">\[w_{k+1}\leftarrow w_k-\alpha_k g(w_k,\xi_k)\]</span>这里的<span class="math inline">\(\xi_k\)</span>为随机变量, 指代第<span class="math inline">\(k\)</span>步随机抽取的样本, <span class="math inline">\(g(w_k,\xi_k)\)</span>表示样本<span class="math inline">\(\xi_k\)</span>对应的梯度. 迭代平均法新建了一个序列:<span class="math display">\[\tilde{w}_{k+1}\leftarrow \frac{1}{k+1}\sum_{j=1}^{k+1}w_{j}\]</span>若步长衰减速度为<span class="math inline">\(O(1/k^a)\)</span>, <span class="math inline">\(a\in(\frac{1}{2},1)\)</span>. 在强凸的前提下有:<span class="math display">\[\mathbb{E}[\Vert w_k-w_+\Vert_2^2] = O(1/k^a)\]</span>且(注:这里的<span class="math inline">\(w_+\)</span>指<span class="math inline">\(w\)</span>的最优值点)<span class="math display">\[\mathbb{E}[\Vert \tilde{w}_k-w_+\Vert_2^2] = O(1/k)\]</span></p>
<h3 id="动态样本大小方法">动态样本大小方法</h3>
<p>动态样本大小方法同样直接基于SG因此适用<span class="math inline">\(F(w)\)</span>. 下面介绍一个核心的结果, 简言之就是, 若能把每步迭代的方向的方差控制住:<span class="math display">\[\mathbb{V}_{\xi_k}[g(w_k, \xi_k)]\le M \zeta^{k-1}\]</span>在强凸(步长固定)的前提下, 可得到线性收敛[leon bottou TH5.1]:<span class="math display">\[\mathbb{E}[F(w_k)-F_*]\le \omega\rho^{k-1}\]</span>其中<span class="math inline">\(\rho&lt;1, \omega\)</span>为常数. 再结合统计中的一个结果: <span class="math inline">\(n_k\)</span>个独立同分布样本的方差为单个样本的方差的<span class="math inline">\(1/n_k\)</span>. 因此只需要动态改变每次迭代所用样本个数, 使其控制在定理条件下<span class="math display">\[\mathbb{V}_{\xi_k}[g(w_k, \xi_k)]\le\frac{C}{n_k}\le M\zeta^{k-1}\Rightarrow n_k\ge\frac{C}{M}(\frac{1}{\zeta})^{k-1}\]</span>(这里假定所有样本的梯度的方差有一致的上界, 记为<span class="math inline">\(C\)</span>) 就能得到线性收敛的结果. 然而虽有线性收敛的速度, 但每次迭代所需样本也是指数级增长, 因此动态样本大小方法的计算复杂度为[leon bottou TH5.3]:<span class="math display">\[O(1/\epsilon)\]</span>由于样本集是有限的, 在某一步中必然会出现样本不够用的情况, 一种可行的做法是转而要求每步迭代的方向产生了足够多的下降, 而不管上述样本量指数级增长的要求.</p>
<h3 id="梯度聚合">梯度聚合</h3>
<p>这类方法着重于利用先前迭代中计算得到的样本的梯度信息, 其包括SVGR、SAG/SAGA等.</p>
<h4 id="svrg">SVRG</h4>
<p><img src="/pictures/SVRG.png" /></p>
<p>SVGA只适用于<span class="math inline">\(R_n(w)\)</span>. 如上图算法所示, 相比SG, SVGA多了抽取<span class="math inline">\(m\)</span>个样本的内层循环, 再用内层循环产生的子序列<span class="math inline">\(\{\tilde{w}_j\}\)</span>,<span class="math inline">\({j=1,...,m}\)</span>去生成高质量的<span class="math inline">\(w_{k+1}\)</span>, 方式比如说(a),(b),(c). 核心式子为:<span class="math display">\[\tilde{g}_j\leftarrow \nabla f_{i_{j}}(\tilde{w}_j)-(\nabla f_{i_{j}}(w_k)-\nabla R_n(w_k))\]</span>它可以从两个角度去理解:</p>
<ol type="1">
<li><span class="math inline">\((\nabla f_{i_{j}}(w_k)-\nabla R_n(w_k))\)</span>可视为从单个样本<span class="math inline">\(i_j\)</span>到<span class="math inline">\(n\)</span>个样本的修正, 因此<span class="math inline">\(\tilde{g}_j\)</span>可近似视为<span class="math inline">\(\nabla R_n(\tilde{w}_j)\)</span>.</li>
<li>若<span class="math inline">\(m\)</span>取的不是特别大, <span class="math inline">\(\nabla f_{i_{j}}(\tilde{w}_j)\)</span>显然与<span class="math inline">\(\nabla f_{i_{j}}(w_k)\)</span>强相关, 而<span class="math inline">\(\nabla f_{i_{j}}(w_k)\)</span>的期望恰为<span class="math inline">\(\nabla R_n(w_k)\)</span>, 此时上述形如<span class="math display">\[\tilde{g}_j\leftarrow X-(Y-\mathbb{E}[Y])\]</span>此时<span class="math inline">\(\mathbb{E}[\tilde{g}_j]=\mathbb{E}[X]\)</span>, <span class="math inline">\(\mathbb{V}[\tilde{g}_j]=\mathbb{V}[X]+\mathbb{V}[Y]-2Cov(X, Y)\)</span>. 再加上强相关的假设, 确实能取到Variance Reduced(VR)的效果, <a href="https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf" target="_blank" rel="noopener">SVRG原论文</a>在Experiments中说明了这一点.</li>
</ol>
<p>在强凸前提下其收敛速度为线性[SVRG TH1]<span class="math display">\[\mathbb{E}[R_n(w_{k})-R_*]\le\omega\rho^{k-1}\]</span>其中<span class="math inline">\(\rho\in(0,1)\)</span>. SVRG的计算复杂度为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span></p>
<h4 id="sagasag">SAGA/SAG</h4>
<p><img src="/pictures/SAGA.png" /></p>
<p>SAGA只适用于<span class="math inline">\(R_n(w)\)</span>, 显然算法部分第一块为初始化所有样本梯度值并存储, 第二个循环才是算法循环. 其中<span class="math inline">\(\nabla f_j(w_{[j]})\)</span>表示第<span class="math inline">\(j\)</span>个样本最近一次存储着的梯度值. 比如说对于第<span class="math inline">\(1\)</span>个样本, 初始化时<span class="math inline">\(\nabla f_1(w_{[1]})\leftarrow \nabla f_1(w_{1})\)</span>, 剩下涉第<span class="math inline">\(1\)</span>个样本的梯度的更新就只有靠算法循环中的<code>Choose j uniformly in {1,...,n}</code>抽到再用<span class="math inline">\(\nabla f_1(w_k)\)</span>更新. 因此可以看出, SAGA本着<strong>能不计算梯度就不计算</strong>的原则(每步迭代只计算一个样本的梯度, 从这个角度看更接近SG), 而其核心式子为:<span class="math display">\[g_k\leftarrow \nabla f_{j}(w_k)-\nabla f_{j}(w_{[j]})+\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\]</span>它同样可以从两个角度去理解:</p>
<ol type="1">
<li><span class="math inline">\(\nabla f_{j}(w_{[j]})-\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\)</span>视为单样本到<span class="math inline">\(n\)</span>个样本的修正, <span class="math inline">\(g_k\)</span>可近似视为<span class="math inline">\(\nabla R_n(w_k)\)</span>.</li>
<li><span class="math inline">\(\nabla f_{j}(w_{[j]})\)</span>的期望为<span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\)</span>, 但<span class="math inline">\(\nabla f_{j}(w_k)\)</span>与<span class="math inline">\(\nabla f_{j}(w_{[j]})\)</span>未必强相关.</li>
</ol>
<p>SAGA起源自SAG, 它们唯一的不同在于核心式子, SAG的核心式子如下:<span class="math display">\[g_k\leftarrow \frac{1}{n}(\nabla f_{j}(w_k)-\nabla f_{j}(w_{[j]})+\sum_{i=1}^n\nabla f_i(w_{[i]}))\]</span>与SAGA不同的是, 此时<span class="math inline">\(g_k\)</span>不是<span class="math inline">\(\nabla R_n(w_k)\)</span>的无偏估计.在强凸前提下, SAGA与SAG均能线性收敛, 且计算复杂度与SVRG一样均为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span></p>
<h3 id="其它">其它</h3>
<h4 id="sarah">SARAH</h4>
<div data-align="left">
<img src="/pictures/SARAH.png" height="520" width="490" >
</div>
<a href="https://arxiv.org/pdf/1703.00102.pdf" target="_blank" rel="noopener">SARAH</a>只适用于<span class="math inline">\(R_n(w)\)</span>, 它与SVGR一样有内圈循环, 不同的是核心迭代式子中把<span class="math inline">\(\nabla R_n(w_k)\)</span>替换成了上一步的迭代方向. 且此时无偏性不再保持. 在强凸前提下, SARAH线性收敛 且计算复杂度与SVRG一样均为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span>注意到SARAH与SVRG中的<span class="math inline">\(m\)</span>均为超参, SARAH原论文中还提出了以<span class="math inline">\(\Vert v_{t-1}\Vert_2^2\le\gamma \Vert v_0\Vert_2^2\)</span>为内圈循环停止条件的实用版算法SARAH+.
<div data-align="center">
<img src="/pictures/SARAH+.png" height="520" width="490" >
</div>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/SG/" rel="tag"># SG</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/10/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A6%82%E7%8E%87%E4%B8%8D%E7%AD%89%E5%BC%8F/" rel="prev" title="统计学习中的几个概率不等式">
      <i class="fa fa-chevron-left"></i> 统计学习中的几个概率不等式
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习中的优化"><span class="nav-number">1.</span> <span class="nav-text">机器学习中的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一些记号"><span class="nav-number">1.1.</span> <span class="nav-text">一些记号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法"><span class="nav-number">1.2.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机梯度"><span class="nav-number">1.3.</span> <span class="nav-text">随机梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#噪声减小方法"><span class="nav-number">2.</span> <span class="nav-text">噪声减小方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#迭代平均方法"><span class="nav-number">2.1.</span> <span class="nav-text">迭代平均方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#动态样本大小方法"><span class="nav-number">2.2.</span> <span class="nav-text">动态样本大小方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度聚合"><span class="nav-number">2.3.</span> <span class="nav-text">梯度聚合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#svrg"><span class="nav-number">2.3.1.</span> <span class="nav-text">SVRG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sagasag"><span class="nav-number">2.3.2.</span> <span class="nav-text">SAGA&#x2F;SAG</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其它"><span class="nav-number">2.4.</span> <span class="nav-text">其它</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sarah"><span class="nav-number">2.4.1.</span> <span class="nav-text">SARAH</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">msgsxj</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/msgsxj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;msgsxj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:msgsxj@gmail.com" title="E-Mail → mailto:msgsxj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">msgsxj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.6.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
