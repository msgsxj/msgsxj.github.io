<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2017</title>
    <url>/2018/01/24/2017/</url>
    <content><![CDATA[<p>前言:记录2017,为2018立flag <span id="more"></span></p>
<figure>
<img src="/pictures/background.jpg" alt="avatar" /><figcaption aria-hidden="true">avatar</figcaption>
</figure>
<h2 id="回顾2017">回顾2017:</h2>
<ul>
<li>一月,这是我第一次接触机器学习,那时我偏爱R,于是选择了丘祐玮的机器学习与R语言实战,初试刚结束的我没有什么压力,外加还要准备复试和毕业论文,这书前前后后花了我三个月才算读完.</li>
<li>三月,考研复试,分笔试,面试,完了体检,然后继续回江都吉银村镇银行实习.</li>
<li>四月,沉迷中国大学MOOC,Coursera刷网课证书,以及大概这个时候得到拟录取通知.</li>
<li>六月,毕业季和小伙伴们去了芜湖方特.</li>
<li>七月,玩游戏,上网课,玩游戏,上网课,...</li>
<li>八月,月初拿到录取通知书.</li>
<li>九月,开学,参加开学典礼第一次去了仙林,略微吐槽鼓楼小,和元神晓黄在我南鼓楼校区这里进行长达100小时的研究生数学建模(三等奖).</li>
<li>十月,听了不少讲座,看到了不少大佬,在东南大学四牌楼校区看到了人民的名义拍摄场景.</li>
<li>十一月,沉迷学习(特指分析,代数).</li>
</ul>
<h2 id="我达成哪些成就">2017我达成哪些成就:</h2>
<ul>
<li>南大数学系研究生(概率统计专业)</li>
<li>主线:基础代数,现代分析</li>
<li>支线:选修课(随机过程,数值代数)</li>
<li>多读书1(读了丘祐玮的机器学习与R语言实战)</li>
<li>多读书2(粗读了一遍西瓜书)</li>
<li>多读书3(读了深入理解机器学习一书的前1/3)</li>
<li>多读书4(Tensorflow:实战Google深度学习框架)</li>
<li>人生苦短,我用Python(中国大学MOOC上完成5门Python相关慕课)</li>
<li>研究生数学建模(三等奖)</li>
<li>沉迷Coursera(完成10门机器学习,深度学习,数据科学相关课程,<a href="https://www.linkedin.com/in/%E5%9D%9A-%E8%AE%B8-838376146/">领英</a>)</li>
<li>有自己的<a href="msgsxj.cn">博客</a>啦</li>
</ul>
<h2 id="对2018立下的少许flag">对2018立下的少许flag：</h2>
<ul>
<li>2018暑期实习一波</li>
<li>Kaggle某个比赛拿到top10</li>
<li>不挂科,尽量多学</li>
</ul>
]]></content>
      <categories>
        <category>聊聊生活</category>
      </categories>
      <tags>
        <tag>回顾</tag>
      </tags>
  </entry>
  <entry>
    <title>2018</title>
    <url>/2019/01/30/2018/</url>
    <content><![CDATA[<p>前言: 回顾2018, 为2019立flag</p>
<span id="more"></span>
<p><img src="/pictures/2018.jpg" /></p>
<h2 id="section">2018</h2>
<p>相比之下, 2018要忙碌一些, 研一下学期先是听老师讲凸优化, 但从8月份开始就是我们4个的讲机器学习和数值优化的讨论班. 机器学习讨论班用的就是周志华老师的西瓜书, 到期末算是堪堪讲完; 数值优化则是用了numberical optimization, 但没来得及讲完, 还剩下一些留给下学期初.</p>
<p>不得不说讨论班让我收获很多, 以西瓜书为例, 自己第一遍读的时候虽然也是认真仔细的读了, 但讲给人听给出了更高的抠细节的要求, 因此做ppt就花了很多时间, 当然这带来的明显的好处就是对书本的理解加深了许多, 并且latex公式也越打越顺了.</p>
<p>导师对我们说, 研究生阶段就应该多学一些东西, 听得出来他也很欣赏我们南大数学系不要求发论文的做法(只需毕业论文), 我也乐在其中.</p>
<p>coursera上, 3月31日结束了吴恩达的deep learning系列课程; 5月份学习了霍普金斯大学的Exploratory Data Analysis; 10月份及12月份学习了俄罗斯国立高等经济学院的Bayesian Methods for Machine Learning和Practical Reinforcement Learning. 不得不说coursera上几乎没有水课, 可以说我在2018年写的大部分博文的出发点都来自于这些课程, 或是这些课程给出的参考文献, 书籍(一般都是某领域的经典之作).</p>
<p>数据类的比赛方面, 作为萌新的我单枪匹马的参加了腾讯2018广告算法大赛, 没能进复赛; 暑假时参加了天池的阿里巴巴大数据智能云上编程大赛(其实就是在阿里自家的机器学习平台PAI上像搭积木一样做模型的一个比赛), 进了复赛但仍然没进决赛; 至于kaggle则是参加了一个泰坦尼克的新人赛后就没有然后了, 因此2017年末立的flag就这样倒了.</p>
<h2 id="section-1">2019</h2>
<p>还是要给自己立个flag: 在比赛中拿个奖.</p>
]]></content>
      <categories>
        <category>聊聊生活</category>
      </categories>
      <tags>
        <tag>回顾</tag>
      </tags>
  </entry>
  <entry>
    <title>2018腾讯广告算法大赛初赛lgb</title>
    <url>/2018/05/29/2018%E8%85%BE%E8%AE%AF%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B%E5%88%9D%E8%B5%9Blgb/</url>
    <content><![CDATA[<p>前言:最近参加了2018腾讯广告算法大赛,虽然没进复赛,但也花了不少时间,算是为下次参加比赛试试水,最终初赛线上最好成绩是0.745764,特此记录.</p>
<span id="more"></span>
<p>主要参考了<a href="https://github.com/YouChouNoBB/2018-tencent-ad-competition-baseline">bryan开源</a>的lgb模型,代码如下:</p>
<h2 id="code">code</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder,LabelEncoder</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">homedir = os.getcwd()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdata</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;read data&quot;</span>)</span><br><span class="line">    adFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\adFeature.csv&#x27;</span>) <span class="comment">#7KB</span></span><br><span class="line">    test1 = pd.read_csv(homedir + <span class="string">&#x27;\\data\\test2.csv&#x27;</span>)<span class="comment"># 29MB</span></span><br><span class="line">    train = pd.read_csv(homedir + <span class="string">&#x27;\\data\\train.csv&#x27;</span>) <span class="comment"># 140MB</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_11.csv&#x27;</span>):</span><br><span class="line">        userFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_0.csv&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;read one million&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">            userFeature = userFeature.append(pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_&#x27;</span> + <span class="built_in">str</span>(i+<span class="number">1</span>) + <span class="string">&#x27;.csv&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;read one million&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        userFeature_list = []</span><br><span class="line">        userFeature = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(homedir + <span class="string">&#x27;\\data\\userFeature.data&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f: <span class="comment"># 4173MB</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f):</span><br><span class="line">                line = line.strip().split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">                userFeature_dict = &#123;&#125;</span><br><span class="line">                <span class="keyword">for</span> each <span class="keyword">in</span> line:</span><br><span class="line">                    each_list = each.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                    userFeature_dict[each_list[<span class="number">0</span>]] = <span class="string">&#x27; &#x27;</span>.join(each_list[<span class="number">1</span>:])</span><br><span class="line">                userFeature_list.append(userFeature_dict)</span><br><span class="line">                <span class="keyword">if</span> i % <span class="number">100000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">            userFeature_temp = pd.DataFrame(userFeature_list[i*<span class="number">1000000</span>:(i+<span class="number">1</span>)*<span class="number">1000000</span>])</span><br><span class="line">            userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;save one million&quot;</span>)</span><br><span class="line">            <span class="keyword">del</span> userFeature_temp</span><br><span class="line">            gc.collect()</span><br><span class="line">        userFeature_temp = pd.DataFrame(userFeature_list[<span class="number">11000000</span>:])</span><br><span class="line">        userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_11.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;save as csv successfully&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> userFeature_temp</span><br><span class="line">        gc.collect()</span><br><span class="line">    <span class="keyword">return</span> mergedata(adFeature, test1, train, userFeature)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergedata</span>(<span class="params">adFeature, test1, train, userFeature</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;merge data and set na as -1&quot;</span>)</span><br><span class="line">    train.loc[train[<span class="string">&#x27;label&#x27;</span>]==-<span class="number">1</span>, <span class="string">&#x27;label&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    test1[<span class="string">&#x27;label&#x27;</span>] = -<span class="number">1</span></span><br><span class="line">    data = pd.concat([train, test1])</span><br><span class="line">    data = pd.merge(data, adFeature, on=<span class="string">&#x27;aid&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">    data = pd.merge(data, userFeature, on=<span class="string">&#x27;uid&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">    data = data.fillna(<span class="string">&#x27;-1&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_predict</span>(<span class="params">data,index</span>):</span></span><br><span class="line">    one_hot_feature=[<span class="string">&#x27;LBS&#x27;</span>,<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;carrier&#x27;</span>,<span class="string">&#x27;consumptionAbility&#x27;</span>,<span class="string">&#x27;education&#x27;</span>,<span class="string">&#x27;gender&#x27;</span>,<span class="string">&#x27;house&#x27;</span>,<span class="string">&#x27;os&#x27;</span>,<span class="string">&#x27;ct&#x27;</span>,<span class="string">&#x27;marriageStatus&#x27;</span>,<span class="string">&#x27;advertiserId&#x27;</span>,<span class="string">&#x27;campaignId&#x27;</span>, <span class="string">&#x27;creativeId&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;adCategoryId&#x27;</span>, <span class="string">&#x27;productId&#x27;</span>, <span class="string">&#x27;productType&#x27;</span>]</span><br><span class="line">    vector_feature=[<span class="string">&#x27;appIdAction&#x27;</span>,<span class="string">&#x27;appIdInstall&#x27;</span>,<span class="string">&#x27;interest1&#x27;</span>,<span class="string">&#x27;interest2&#x27;</span>,<span class="string">&#x27;interest3&#x27;</span>,<span class="string">&#x27;interest4&#x27;</span>,<span class="string">&#x27;interest5&#x27;</span>,<span class="string">&#x27;kw1&#x27;</span>,<span class="string">&#x27;kw2&#x27;</span>,<span class="string">&#x27;kw3&#x27;</span>,<span class="string">&#x27;topic1&#x27;</span>,<span class="string">&#x27;topic2&#x27;</span>,<span class="string">&#x27;topic3&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> one_hot_feature:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data[feature] = LabelEncoder().fit_transform(data[feature].apply(<span class="built_in">int</span>))</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            data[feature] = LabelEncoder().fit_transform(data[feature])</span><br><span class="line"></span><br><span class="line">    train=data[data.label!=-<span class="number">1</span>]</span><br><span class="line">    train_y=train.pop(<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">    test=data[data.label==-<span class="number">1</span>]</span><br><span class="line">    res=test[[<span class="string">&#x27;aid&#x27;</span>,<span class="string">&#x27;uid&#x27;</span>]]</span><br><span class="line">    test=test.drop(<span class="string">&#x27;label&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line">    enc = OneHotEncoder()</span><br><span class="line">    train_x=train[[<span class="string">&#x27;creativeSize&#x27;</span>]]</span><br><span class="line">    test_x=test[[<span class="string">&#x27;creativeSize&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> one_hot_feature:</span><br><span class="line">        enc.fit(data[feature].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">del</span> data[feature]</span><br><span class="line">        gc.collect()</span><br><span class="line">        train_a = enc.transform(train[feature].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        test_a = enc.transform(test[feature].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        train_x = sparse.hstack((train_x, train_a))</span><br><span class="line">        test_x = sparse.hstack((test_x, test_a))</span><br><span class="line">        <span class="built_in">print</span>(feature+<span class="string">&#x27; finish&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    cv=CountVectorizer()</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> vector_feature:</span><br><span class="line">        cv.fit(data[feature])</span><br><span class="line">        <span class="keyword">del</span> data[feature]</span><br><span class="line">        gc.collect()</span><br><span class="line">        train_a = cv.transform(train[feature])</span><br><span class="line">        test_a = cv.transform(test[feature])</span><br><span class="line">        train_x = sparse.hstack((train_x, train_a))</span><br><span class="line">        test_x = sparse.hstack((test_x, test_a))</span><br><span class="line">        <span class="built_in">print</span>(feature + <span class="string">&#x27; finish&#x27;</span>)</span><br><span class="line">    <span class="keyword">del</span> data</span><br><span class="line">    gc.collect()</span><br><span class="line">    <span class="keyword">return</span> LGB_predict(train_x, train_y, test_x, res, index)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LGB_predict</span>(<span class="params">train_x, train_y, test_x, res, index</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;split train data as train and eval&quot;</span>)</span><br><span class="line">    train_x, evals_x, train_y, evals_y = train_test_split(train_x, train_y,test_size=<span class="number">0.2</span>)</span><br><span class="line">    gc.collect()</span><br><span class="line">    clf = lgb.LGBMClassifier(</span><br><span class="line">        boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">127</span>, reg_alpha=<span class="number">10</span>, reg_lambda=<span class="number">10</span>,</span><br><span class="line">        max_depth=<span class="number">8</span>, n_estimators=<span class="number">10000</span>, objective=<span class="string">&#x27;binary&#x27;</span>, metric= <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">        subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">        learning_rate=<span class="number">0.05</span>, min_child_weight=<span class="number">20</span>, random_state=<span class="number">2018</span>, n_jobs=-<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">    clf.fit(train_x, train_y, eval_set=[(evals_x, evals_y)], eval_metric=<span class="string">&#x27;auc&#x27;</span>,early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predict&quot;</span>)</span><br><span class="line">    res[<span class="string">&#x27;score&#x27;</span>+<span class="built_in">str</span>(index)] = clf.predict_proba(test_x)[:,<span class="number">1</span>]</span><br><span class="line">    res[<span class="string">&#x27;score&#x27;</span>+<span class="built_in">str</span>(index)] = res[<span class="string">&#x27;score&#x27;</span>+<span class="built_in">str</span>(index)].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(<span class="string">&#x27;%.6f&#x27;</span> % x))</span><br><span class="line">    auc_valid = clf.evals_result_[<span class="string">&#x27;valid_0&#x27;</span>][<span class="string">&#x27;auc&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;save valid auc curve&quot;</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">60</span>, <span class="number">40</span>))</span><br><span class="line">    plt.plot(auc_valid)</span><br><span class="line">    plt.title(<span class="built_in">max</span>(clf.evals_result_[<span class="string">&#x27;valid_0&#x27;</span>][<span class="string">&#x27;auc&#x27;</span>]))</span><br><span class="line">    plt.savefig(<span class="string">&#x27;./picture/&#x27;</span>+ <span class="built_in">str</span>(i) + <span class="string">&#x27;.png&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    gc.collect()</span><br><span class="line">    res=res.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> res[<span class="string">&#x27;score&#x27;</span>+<span class="built_in">str</span>(index)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据分片处理，对每片分别训练预测，然后求平均</span></span><br><span class="line">data=getdata()</span><br><span class="line">train=data[data[<span class="string">&#x27;label&#x27;</span>]!=-<span class="number">1</span>]</span><br><span class="line">test=data[data[<span class="string">&#x27;label&#x27;</span>]==-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">del</span> data</span><br><span class="line">gc.collect()</span><br><span class="line">predict=pd.read_csv(<span class="string">&#x27;./data/test2.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分片&quot;</span>)</span><br><span class="line">cnt = <span class="number">1</span></span><br><span class="line">size = math.ceil(<span class="built_in">len</span>(train) / cnt)</span><br><span class="line">result=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(cnt):</span><br><span class="line">    start = size * i</span><br><span class="line">    end = (i + <span class="number">1</span>) * size <span class="keyword">if</span> (i + <span class="number">1</span>) * size &lt; <span class="built_in">len</span>(train) <span class="keyword">else</span> <span class="built_in">len</span>(train)</span><br><span class="line">    <span class="built_in">slice</span> = train[start:end]</span><br><span class="line">    result.append(batch_predict(pd.concat([<span class="built_in">slice</span>,test]),i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">str</span>(i),<span class="string">&#x27;/&#x27;</span>,<span class="built_in">str</span>(cnt))</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line">result=pd.concat(result,axis=<span class="number">1</span>)</span><br><span class="line">result[<span class="string">&#x27;score&#x27;</span>]=np.mean(result,axis=<span class="number">1</span>)</span><br><span class="line">result[<span class="string">&#x27;score&#x27;</span>] = result[<span class="string">&#x27;score&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">float</span>(<span class="string">&#x27;%.6f&#x27;</span> % x))</span><br><span class="line">result=result.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;save as csv&quot;</span>)</span><br><span class="line">result=pd.concat([predict[[<span class="string">&#x27;aid&#x27;</span>,<span class="string">&#x27;uid&#x27;</span>]].reset_index(drop=<span class="literal">True</span>),result[<span class="string">&#x27;score&#x27;</span>]],axis=<span class="number">1</span>)</span><br><span class="line">result[[<span class="string">&#x27;aid&#x27;</span>,<span class="string">&#x27;uid&#x27;</span>,<span class="string">&#x27;score&#x27;</span>]].to_csv(<span class="string">&#x27;./submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="hyperparameters">hyperparameters</h2>
<p>这里给出调参的一点心得,上述最关键lgb模型参数部分为: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = lgb.LGBMClassifier(</span><br><span class="line">    boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, num_leaves=<span class="number">127</span>, reg_alpha=<span class="number">10</span>, reg_lambda=<span class="number">10</span>,</span><br><span class="line">    max_depth=<span class="number">8</span>, n_estimators=<span class="number">10000</span>, objective=<span class="string">&#x27;binary&#x27;</span>, metric= <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    subsample=<span class="number">0.7</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">    learning_rate=<span class="number">0.05</span>, min_child_weight=<span class="number">20</span>, random_state=<span class="number">2018</span>, n_jobs=-<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure> 以超参数正则化项系数为例子:</p>
<ol type="1">
<li>第一次试验l1,l2正则化项系数均为1,线下0.760,线上0.7451</li>
<li>第二次试验l1,l2正则化项系数均为10,线下0.759,线上0.7457</li>
<li>第三次试验l1,l2正则化项系数均为100,线下0.740,线上0.7347</li>
</ol>
<p>显然第三次试验正则化系数加的过多导致严重欠拟合,对上述系数(1,10,100)取倒数,再取log可得(0,-1,-2),从第三次试验到第一次试验为正则化项系数减少方向,也即模型复杂程度增加方向,也即VC维增加方向(不清楚地朋友可以看我的第一篇博文博文统计学习基本定理的1.3.4节),故此时线下正确率可大致视为指数型递减,线上大致可视为二次曲线或某凸函数. 大致画出此时线上线下1-auc(auc为ROC曲线下方的面积大小,也是此次比赛评估模型优劣的参数)的图像,蓝线表示线下,绿线表示线上,由图像猜测第一次试验过拟合,第二次第三次试验欠拟合: <img src="/pictures/p2.png" alt="l1_l2" /></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>2021</title>
    <url>/2022/01/05/2021/</url>
    <content><![CDATA[<p>前言:已经好久没立flag了,记录2021,为2022立flag.</p>
<span id="more"></span>
<p><img src="/pictures/2021.jpeg" /></p>
<h2 id="section">2021</h2>
<ol type="1">
<li>爷爷离开了</li>
<li>工作地点从常熟的创新中心换到了苏州的思必驰</li>
<li>逐渐养成习惯在山姆买东西</li>
<li>在年末遇到了猪猪</li>
</ol>
<h2 id="section-1">2022</h2>
<ol type="1">
<li>想要保护好自己的牙齿</li>
<li>想要好好谈恋爱</li>
</ol>
]]></content>
      <categories>
        <category>聊聊生活</category>
      </categories>
      <tags>
        <tag>回顾</tag>
      </tags>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2018/09/02/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>前言:本文主要是关于EM算法, 这是用来求解隐变量模型的算法, 主要参考了Coursera上国立高等经济大学Advanced Machine Learning系列课程Course3: Bayesian Methods for Machine Learning Week2. <span id="more"></span></p>
<p>首先简单说明EM算法:</p>
<ul>
<li>EM算法是一种迭代算法, 它的每次迭代由两步组成, E步, 求期望(Expectation); M步, 求极大(Maximization).</li>
<li>EM算法是一种针对隐变量模型的算法, 这里的隐变量指的是未观测到值的变量, 比方说人为引入的变量或是数据缺失的变量. 针对隐变量模型的算法有多种, EM是其中兼顾了高效以及精度的方法, 因此比较流行.</li>
<li>记隐变量为<span class="math inline">\(T\)</span>, 数据为<span class="math inline">\(X\)</span>, 未知参数为<span class="math inline">\(\theta\)</span>, 乍看之下, EM算法是对<span class="math inline">\(\log P(X\mid\theta)\)</span>的极大似然估计, 但其每一步迭代又给出了隐变量<span class="math inline">\(T\)</span>的概率分布以及<span class="math inline">\(\theta\)</span>的点估计.</li>
</ul>
<hr />
<h2 id="隐变量模型">隐变量模型</h2>
<p><a href="https://en.wikipedia.org/wiki/Latent_variable_model">隐变量模型</a>, 隐变量模型含有未观测到值的变量, 且该隐变量解释了观测到的变量之间的相关性(<a href="https://en.wikipedia.org/wiki/Local_independence">局部独立性</a>), 简言之引进的隐变量不能没有用.</p>
<p>下面举一个隐变量模型的例子, 例子来自Week2的课件:</p>
<ul>
<li>考虑银行是否发放信用卡的问题, 现有客户收入及负债的数据如下图:
<div data-align="center">
<img src="/pictures/bayes in ml/GMM_EM_origin.png" width="80%" height="50%" />
</div>
通过探索性数据分析发现客户可大致分为三类, 并假定每一类服从高斯分布, 故可以采用高斯混合模型(Gaussian Mixture Model: GMM), 此时模型为:<span class="math display">\[P(x\mid\theta)=\pi_1N(\mu_1,\Sigma_1)+\pi_2N(\mu_2,\Sigma_2)+\pi_3N(\mu_3,\Sigma_3)\]</span>其中<span class="math inline">\(\theta=\{\pi_1, \pi_2\, \pi_3,\mu_1, \mu_2, \mu_3, \Sigma_1, \Sigma_2, \Sigma_3\}\)</span>, 且<span class="math inline">\(\pi_1+\pi_2+\pi_3=1\)</span>. 引入隐变量<span class="math inline">\(t\)</span>, <span class="math inline">\(t=1,2or3\)</span>代表了客户所属人群类别, 显然我们没有隐变量<span class="math inline">\(t\)</span>的观测值. 假定<span class="math display">\[P(t=c\mid\theta)=\pi_c$, $P(x\mid t=c, \theta)=N(\mu_c, \Sigma_c)\]</span>我们就有了:<span class="math display">\[P(x\mid\theta)=\sum_{c=1}^3P(x\mid t=c, \theta)P(t=c\mid\theta)\]</span>目标则是对最大化上式得到参数值<span class="math inline">\(\theta\)</span>. 故对于每一个样本<span class="math inline">\(x_i\)</span>, 计算<span class="math inline">\(P(t=c\mid x_i, \theta)\)</span>从而推测出其所属类别.</li>
</ul>
<hr />
<h2 id="em算法">EM算法</h2>
汇总一下上文叙述的GMM模型:<span class="math display">\[P(x\mid\theta)=\sum_{c=1}^3P(x\mid t=c, \theta)P(t=c\mid\theta)\]</span>其中<span class="math inline">\(\theta=\{\pi_c, \mu_c, \Sigma_c\mid c=\{1,2,3\}\}\)</span>,且<span class="math inline">\(P(x\mid t=c, \theta)=N(\mu_c, \Sigma_c)\)</span>,<span class="math inline">\(P(t=c\mid\theta)=\pi_c\ge 0\)</span>, <span class="math inline">\(\sum_{c=1}^3\pi_c=1\)</span>如果能求出参数<span class="math inline">\(\theta\)</span>的值, 此时模型就确定下来了, 一个自然地想法是极大化对数似然函数:<span class="math display">\[\hat\theta=\arg\max_{\theta}{\log P(X\mid\theta)}\]</span>EM算法没有直接对对数似然函数进行优化求解, 而是通过E步先求对目标函数下界的极大化逼近, 此时下界函数往往是凹函数, 再进行优化求解就简单很多, 下面来看:假设有<span class="math inline">\(m\)</span>个样本, 对每个样本任取的概率分布<span class="math inline">\(q(t_i=c)\)</span>,<span class="math inline">\(i=1,...,m\)</span> <span class="math display">\[\begin{aligned} \log P(X\mid\theta)&amp; =  \sum_{i=1}^{m}\log P(x_i\mid\theta)\\\
&amp; =  \sum_{i=1}^{m}\log \sum_{c=1}^3P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)\\\
&amp; =  \sum_{i=1}^{m}\log \sum_{c=1}^3q(t_i=c)\frac{P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)}{q(t_i=c)}\\\
&amp; \ge  \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log\frac{P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)}{q(t_i=c)}\\\
&amp; \doteq \mathcal{L}(\theta, q) \end{aligned}\]</span> 其中唯一的一个不等号用到了<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen不等式</a>(概率形式), 其余等号都是基本的化简. 值得注意的是这里的<span class="math inline">\(q\)</span>是任取的:
<div data-align="center">
<img src="/pictures/bayes in ml/EM_L.png" width="80%" height="50%" />
</div>
<p>下面做的事情则是考虑在<span class="math inline">\(\theta\)</span>给定的情况下, 如何取<span class="math inline">\(q\)</span>才能让<span class="math inline">\(\mathcal{L}(\theta, q)\)</span>尽可能的靠近<span class="math inline">\(\log P(X\mid\theta)\)</span>. 事实上, 观察两者之差可以发现: <span class="math display">\[\begin{aligned} \log P(X\mid\theta)-\mathcal{L}(\theta, q)&amp; =  \sum_{i=1}^{m}\log P(x_i\mid\theta)-\sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log\frac{P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)}{q(t_i=c)}\\\
&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\left(\log P(x_i\mid\theta)-\log\frac{P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)}{q(t_i=c)}\right)\\\
&amp; = \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log\frac{P(x_i\mid\theta)q(t_i=c)}{P(x_i, t_i=c\mid \theta)}\\\
&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log\frac{q(t_i=c)}{P(t_i=c\mid x_i, \theta)}\\\
&amp; = \mathcal{KL}(q(T)\Vert P(T\mid X, \theta)) \end{aligned}\]</span>恰为某<span class="math inline">\(\mathcal{KL}\)</span>散度, <span class="math inline">\(\mathcal{KL}\)</span>散度定义见<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">维基百科</a>, <span class="math inline">\(\mathcal{KL}\)</span>散度定义为两个分布之间距离的度量, 必然<span class="math inline">\(\ge 0\)</span>, 且只在两个分布相同的情况下取<span class="math inline">\(0\)</span>. 可取<span class="math inline">\(q(T)=P(T\mid X, \theta)\)</span>. 最后对下界函数<span class="math inline">\(\mathcal{L}(\theta, q)\)</span>稍作化简: <span class="math display">\[\begin{aligned} \mathcal{L}(\theta, q)&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log\frac{P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)}{q(t_i=c)}\\\
&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log P(x_i, t_i=c\mid \theta)-\sum_{i=1}^{m} \sum_{c=1}^3q(t_i=c)\log q(t_i=c)\\\
&amp; =  \mathbb{E}_q\log P(X, T\mid\theta)+C \end{aligned}\]</span>这里<span class="math inline">\(C\)</span>与<span class="math inline">\(\theta\)</span>无关, 此时EM算法已经出来了, 第k+1次迭代, 参数为<span class="math inline">\(\theta^k\)</span>:</p>
<ol type="1">
<li>E步求期望:求<span class="math inline">\(\mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)\)</span>, 其中<span class="math inline">\(q^{k+1}(T)=P(T\mid X, \theta^k)\)</span>(有些书会说E步是求隐变量的分布, 但个人觉得称其为求期望的一步会比较恰当).</li>
<li>M步求极大: <span class="math inline">\(\theta^{k+1}=\arg\max_{\theta}\mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)\)</span>.</li>
</ol>
<hr />
<h2 id="em算法收敛性">EM算法收敛性</h2>
<p>易得<span class="math inline">\(\log P(X\mid\theta^{k+1})\ge\mathcal{L}(\theta^{k+1}, q^{k+1})\ge\mathcal{L}(\theta^{k}, q^{k+1})=\log P(X\mid\theta^{k})\)</span>, 其中第一个不等号因为对于任意的<span class="math inline">\(q\)</span>不等号都成立; 第二个不等号由M步易得; 第三个等号由E步易得.</p>
<hr />
<h2 id="gmm模型的em算法">GMM模型的EM算法</h2>
<p>为了加深印象, 将</p>
<ul>
<li><span class="math inline">\(P(x\mid t=c, \theta)=N(\mu_c, \Sigma_c)\)</span></li>
<li><span class="math inline">\(P(t=c\mid\theta)=\pi_c\ge 0\)</span></li>
<li><span class="math inline">\(\sum_{c=1}^3\pi_c=1\)</span></li>
</ul>
<p>条件带入来走一遍EM算法流程, 这里建议读者手动算一遍.第k+1次迭代, 参数为<span class="math inline">\(\theta^k\)</span>, 并计算<span class="math display">\[P(t_i=c\mid x_i, \theta^k)=\frac{P(x_i\mid t_i=c, \theta_k)P(t_i=c\mid \theta_k)}{\sum_{c=1}^3P(x_i\mid t_i=c, \theta_k)P(t_i=c\mid \theta_k)}\]</span></p>
<ol type="1">
<li>E步: 求<span class="math inline">\(\mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)\)</span>, 其中<span class="math inline">\(q^{k+1}(t_i=c)=P(t_i=c\mid x_i, \theta^k)\)</span>:<span class="math display">\[\begin{aligned} \mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3P(t_i=c\mid x_i, \theta^k)\log P(x_i, t_i=c\mid \theta)\\\
&amp; =  \sum_{i=1}^{m} \sum_{c=1}^3P(t_i=c\mid x_i, \theta^k)\log P(x_i\mid t_i=c, \theta)P(t_i=c\mid\theta)\\\
&amp; = \sum_{i=1}^{m} \sum_{c=1}^3P(t_i=c\mid x_i, \theta^k)\left(\log\frac{\pi_c}{\sqrt{(2\pi)^d|\Sigma_c|}}-\frac{1}{2}(x_i-\mu_c)^T\Sigma_c^{-1}(x_i-\mu_c)\right) \end{aligned}\]</span></li>
<li>M步: 令<span class="math inline">\(\frac{\partial\mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)}{\partial \mu_c}=0\)</span>, 可得<span class="math inline">\(\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)(-\Sigma_c^{-1}(x_i-\mu_c))=0\)</span>,即<span class="math display">\[\mu_c=\frac{\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)x_i}{\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)}\]</span>对<span class="math inline">\(\Sigma_c\)</span>求导时遇到了一些困难, 这里做出一些简化假设, 即令<span class="math inline">\(\Sigma_c=\sigma_c^2 I\)</span>, 令<span class="math inline">\(\frac{\partial\mathbb{E}_{q^{k+1}}\log P(X, T\mid\theta)}{\partial \sigma_c}=0\)</span>, 可得<span class="math display">\[\sigma_c^2=\frac{\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)(x_i-\mu_c)^T(x_i-\mu_c)}{\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)}\]</span>至于对<span class="math inline">\(\pi_c\)</span>这样的有一个等式约束的优化可以用拉格朗日乘子法:<span class="math display">\[\pi_c=\frac{\sum_{i=1}^{m}P(t_i=c\mid x_i, \theta^k)}{m}\]</span></li>
</ol>
<hr />
<h2 id="gmm模型的em算法的python实现">GMM模型的EM算法的Python实现</h2>
<p>首先下载<a href="/download/samples.npz">数据集</a>, 放在与py文件同一目录下.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Created on Mon Sep  3 00:19:21 2018</span></span><br><span class="line"><span class="string">@author: msgsxj</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span>  solve</span><br><span class="line"><span class="comment">#!conda install -c conda-forge pyqt=4 -y</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">samples = np.load(<span class="string">&#x27;samples.npz&#x27;</span>)</span><br><span class="line">X = samples[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">pi0 = samples[<span class="string">&#x27;pi0&#x27;</span>]</span><br><span class="line">mu0 = samples[<span class="string">&#x27;mu0&#x27;</span>]</span><br><span class="line">sigma0 = samples[<span class="string">&#x27;sigma0&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(pi0)</span><br><span class="line"><span class="built_in">print</span>(mu0.shape)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=<span class="string">&#x27;grey&#x27;</span>, s=<span class="number">30</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">E_step</span>(<span class="params">X, pi, mu, sigma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs E-step on GMM model</span></span><br><span class="line"><span class="string">    Each input is numpy array:</span></span><br><span class="line"><span class="string">    X: (N x d), data points</span></span><br><span class="line"><span class="string">    pi: (C), mixture component weights</span></span><br><span class="line"><span class="string">    mu: (C x d), mixture component means</span></span><br><span class="line"><span class="string">    sigma: (C x d x d), mixture component covariance matrices</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gamma: (N x C), probabilities of clusters for objects</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    C = pi.shape[<span class="number">0</span>] <span class="comment"># number of clusters</span></span><br><span class="line">    d = mu.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line">    gamma = np.zeros((N, C)) <span class="comment"># distribution q(T)</span></span><br><span class="line">    <span class="comment"># compute det of sigma</span></span><br><span class="line">    sigma_det = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">        sign, logdet = np.linalg.slogdet(sigma[j])</span><br><span class="line">        sigma_det.append(sign * np.exp(logdet))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">            gamma[i, j] = np.power(sigma_det[j], -<span class="number">0.5</span>) * np.exp(-<span class="number">0.5</span> * np.dot(X[i,:] - mu[j], solve(sigma[j], X[i,:] - mu[j]))) * pi[j]</span><br><span class="line">        gamma[i,:] /= <span class="built_in">sum</span>(gamma[i,:])</span><br><span class="line">    <span class="keyword">return</span> gamma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">M_step</span>(<span class="params">X, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs M-step on GMM model</span></span><br><span class="line"><span class="string">    Each input is numpy array:</span></span><br><span class="line"><span class="string">    X: (N x d), data points</span></span><br><span class="line"><span class="string">    gamma: (N x C), distribution q(T)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pi: (C)</span></span><br><span class="line"><span class="string">    mu: (C x d)</span></span><br><span class="line"><span class="string">    sigma: (C x d x d)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    C = gamma.shape[<span class="number">1</span>] <span class="comment"># number of clusters</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line">    mu = np.zeros((C, d))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">        mu[j,:] = np.<span class="built_in">sum</span>(np.transpose(np.multiply(gamma[:,j], np.transpose(X))), axis = <span class="number">0</span>)/np.<span class="built_in">sum</span>(gamma[:,j])</span><br><span class="line">    sigma = np.zeros((C, d, d))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            sigma[j,:,:] += np.multiply(gamma[i,j], np.outer(X[i,:] - mu[j,:], X[i,:] - mu[j,:]))</span><br><span class="line">        sigma[j,:,:] /= np.<span class="built_in">sum</span>(gamma[:,j])</span><br><span class="line">    pi = np.zeros((C,))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">        pi[j] = np.<span class="built_in">sum</span>(gamma[:,j])/N</span><br><span class="line">    <span class="keyword">return</span> pi, mu, sigma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_vlb</span>(<span class="params">X, pi, mu, sigma, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Each input is numpy array:</span></span><br><span class="line"><span class="string">    X: (N x d), data points</span></span><br><span class="line"><span class="string">    gamma: (N x C), distribution q(T)</span></span><br><span class="line"><span class="string">    pi: (C)</span></span><br><span class="line"><span class="string">    mu: (C x d)</span></span><br><span class="line"><span class="string">    sigma: (C x d x d)</span></span><br><span class="line"><span class="string">    Returns value of variational lower bound</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    C = gamma.shape[<span class="number">1</span>] <span class="comment"># number of clusters</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line">    <span class="comment"># compute det of sigma</span></span><br><span class="line">    sigma_det = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">        sign, logdet = np.linalg.slogdet(sigma[j])</span><br><span class="line">        sigma_det.append(sign * np.exp(logdet))</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">            loss += gamma[i ,j] * (np.log(pi[j]) +</span><br><span class="line">                                   np.log(<span class="number">1</span>/np.sqrt(np.power(<span class="number">2</span> * np.pi, d) * sigma_det[j])) -<span class="number">0.5</span> * np.dot(X[i,:] - mu[j], solve(sigma[j], X[i,:] - mu[j])) -</span><br><span class="line">                                   np.log(gamma[i,j]))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_EM</span>(<span class="params">X, C, rtol=<span class="number">1e-3</span>, max_iter=<span class="number">100</span>, restarts=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Starts with random initialization *restarts* times</span></span><br><span class="line"><span class="string">    Runs optimization until saturation with *rtol* reached</span></span><br><span class="line"><span class="string">    or *max_iter* iterations were made.</span></span><br><span class="line"><span class="string">    X: (N, d), data points</span></span><br><span class="line"><span class="string">    C: int, number of clusters</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>] <span class="comment"># number of objects</span></span><br><span class="line">    d = X.shape[<span class="number">1</span>] <span class="comment"># dimension of each object</span></span><br><span class="line">    best_loss = <span class="number">0</span></span><br><span class="line">    best_pi = np.zeros((C,))</span><br><span class="line">    best_mu = np.zeros((C, d))</span><br><span class="line">    best_sigma = np.zeros((C, d, d))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(restarts):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            temp = np.random.random(C)</span><br><span class="line">            best_pi = temp/np.<span class="built_in">sum</span>(temp)</span><br><span class="line">            best_sigma[:,:,:] = np.eye(<span class="number">2</span>)</span><br><span class="line">            best_mu = X[np.random.randint(N, size=<span class="number">3</span>),:]</span><br><span class="line">            gamma = E_step(X, best_pi, best_mu, best_sigma)</span><br><span class="line">            best_pi, best_mu, best_sigma = M_step(X, gamma)</span><br><span class="line">            loss_pre = compute_vlb(X, best_pi, best_mu, best_sigma, gamma)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">                gamma = E_step(X, best_pi, best_mu, best_sigma)</span><br><span class="line">                best_pi, best_mu, best_sigma = M_step(X, gamma)</span><br><span class="line">                best_loss = compute_vlb(X, best_pi, best_mu, best_sigma, gamma)</span><br><span class="line">                <span class="keyword">if</span>(np.<span class="built_in">abs</span>((best_loss - loss_pre)/loss_pre)&lt;rtol):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                loss_pre = best_loss</span><br><span class="line">        <span class="keyword">except</span> np.linalg.LinAlgError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Singular matrix: components collapsed&quot;</span>)</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> best_loss, best_pi, best_mu, best_sigma</span><br><span class="line"></span><br><span class="line">best_loss, best_pi, best_mu, best_sigma = train_EM(X, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">gamma = E_step(X, best_pi, best_mu, best_sigma)</span><br><span class="line">labels = gamma.argmax(<span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">12</span>))</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, s=<span class="number">30</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&quot;GMM_EM.png&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
运行结果图如下:
<div data-align="center">
<img src="/pictures/bayes in ml/GMM_EM.png" width="80%" height="50%" />
</div>
<hr />
<h2 id="em算法的变体">EM算法的变体</h2>
<p>事实上,EM算法的E步其实是采用了变分推断的方法, 变分推断又称<a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">变分贝叶斯方法</a>, 它是用来计算难以计算积分的一类方法(求期望本质上是求积分):</p>
<ul>
<li>在一个给定分布族<span class="math inline">\(Q\)</span>中找到<span class="math inline">\(q(T)\)</span>, 使其逼近<span class="math inline">\(P(T\mid X, \theta)\)</span>, 且一般选用最小化<span class="math inline">\(\mathcal{KL}\)</span>散度<span class="math inline">\(\mathcal{KL}(q(T)\Vert P(T\mid X, \theta))\)</span></li>
<li>计算目标积分时进行近似计算: <span class="math inline">\(\mathbb{E}_{P(T\mid X, \theta)}\log P(X,T\mid\theta)\approx \mathbb{E}_{q(T)}\log P(X,T\mid\theta)\)</span></li>
<li>完整版的EM算法则可以视为此时分布族<span class="math inline">\(Q\)</span>取得足够大了, 因而包含<span class="math inline">\(P(T\mid X, \theta)\)</span>, 也就必然有<span class="math inline">\(q(T) = P(T\mid X, \theta)\)</span>; 而一般分布族<span class="math inline">\(Q\)</span>选取可为正态分布, 且其协方差阵为对角阵这样的简单分布.</li>
<li>此外对变量之间做出额外的独立性假定的方法被称为是平均场近似法, 显然这样的假定能降低计算量, 这种方法、变分推断与EM算法结合可以产生不同的变体, 详见我的另外一篇博文<a href="https://msgsxj.cn/2018/09/05/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/">变分贝叶斯方法</a>.</li>
</ul>
<p>除了变分推断, 还有三类非常著名的方法同样能够用来计算难以计算的积分, 分别是: <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a>, <a href="https://en.wikipedia.org/wiki/Expectation_propagation">期望传播</a>(Expectation Propagation: EP)及<a href="https://en.wikipedia.org/wiki/Laplace%27s_method">拉普拉斯近似方法</a>(Laplace Approximation):</p>
<ul>
<li>MCMC是一类对分布进行抽样的方法: <span class="math inline">\(T_s \sim P(T\mid X, \theta)\)</span>, 其本质是构建稳态分布恰为所需分布<span class="math inline">\(P(T\mid X, \theta)\)</span>的马尔可夫链. 概率论中的极限理论告诉我们, <span class="math inline">\(\frac{1}{M} \sum_{s=1}^M\log P(X,T_s\mid\Theta)\)</span>会慢慢逼近<span class="math inline">\(\mathbb{E}_{P(T\mid X, \theta)}\log P(X,T\mid\Theta)\)</span>, 因此随着抽样数的增大, 其近似值会越来越精确. 这块内容详见我的另一篇博文<a href="https://msgsxj.cn/2018/09/18/MCMC/">MCMC</a>.</li>
<li>期望传播方法可以简单理解为变分贝叶斯中最小化的<span class="math inline">\(\mathcal{KL}\)</span>散度逆过来, 即最小化<span class="math inline">\(\mathcal{KL}(P(T\mid X, \theta)\Vert q(T))\)</span>, 见<a href="https://msgsxj.cn/2018/10/02/%E6%9C%9F%E6%9C%9B%E4%BC%A0%E6%92%AD/">期望传播</a>.</li>
<li>拉普拉斯近似方法本质上是将被积函数视为正态分布, 见<a href="https://msgsxj.cn/2018/09/28/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E6%96%B9%E6%B3%95/">拉普拉斯方法</a>.</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>EM算法</tag>
        <tag>隐变量模型</tag>
        <tag>Jensen不等式</tag>
      </tags>
  </entry>
  <entry>
    <title>Exploratory Data Analysis</title>
    <url>/2018/04/17/Exploratory-Data-Analysis/</url>
    <content><![CDATA[<p>来自coursera上约翰霍普金斯大学Data Science系列课程Course4:Exploratory Data Analysis. <span id="more"></span></p>
<hr />
<h1 id="princlples-of-analytic-graphics">Princlples of Analytic Graphics</h1>
<ol type="1">
<li>Show comparisons compared to what(PM25 in house with aircleaner compared to without aircleaner)</li>
<li>Show casuality, mechanism, explanation show how you believe the world works(show you believe child living in house with lower pm25 is more likely to be healthy)</li>
<li>Show multivariate Data more than 2 variables</li>
<li>Integrate multiple models of evidence don't let the tools drive the analysis(plot depend on your own idea, not the tools)</li>
<li>Describe and document the evidence</li>
<li>Content is king</li>
</ol>
<hr />
<h1 id="take-a-look-at-the-data">Take a Look at the Data</h1>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(datasets)</span><br><span class="line">data(airquality)</span><br></pre></td></tr></table></figure>
<h2 id="one-dimension">One dimension</h2>
<ol type="1">
<li>Summary <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">summary(airquality$Ozone) <span class="comment"># 臭氧</span></span><br></pre></td></tr></table></figure></li>
<li>Boxplots <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">boxplot(airquality$Ozone, col = <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">abline(h = <span class="number">100</span>)</span><br></pre></td></tr></table></figure></li>
<li>Historgrams <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">hist(airquality$Ozone, col = <span class="string">&quot;green&quot;</span>, breaks = <span class="number">100</span>)</span><br><span class="line">abline(v = <span class="number">100</span>, lwd = <span class="number">2</span>)</span><br><span class="line">abline(v = median(airquality$Ozone), col = <span class="string">&quot;magenta&quot;</span>, lwd = <span class="number">4</span>)</span><br><span class="line">rug(airquality$Ozone)</span><br></pre></td></tr></table></figure></li>
<li>Barplot <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">barplot(table(airquality$Month),col = <span class="string">&quot;wheat&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="two-dimensions">Two dimensions</h2>
<ol type="1">
<li>Multiple Boxplots <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">boxplot(Ozone ~ Month, data = airquality, col = <span class="string">&quot;red&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>Multiple Historgrams <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">par(mfrow = <span class="built_in">c</span>(<span class="number">2</span>,<span class="number">1</span>), mar = <span class="built_in">c</span>(<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">hist(subset(airquality, Month == <span class="number">5</span>)$Ozone, col = <span class="string">&quot;green&quot;</span>)</span><br><span class="line">hist(subset(airquality, Month == <span class="number">8</span>)$Ozone, col = <span class="string">&quot;green&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>Scatterplot <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">par(mfrow = <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">with(airquality, plot(Solar.R, Ozone, col = Month))</span><br><span class="line">legend(<span class="string">&quot;topright&quot;</span>, pch = <span class="number">1</span>, col = <span class="built_in">c</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>), legend = <span class="built_in">c</span>(<span class="string">&quot;5月&quot;</span>, <span class="string">&quot;6月&quot;</span>, <span class="string">&quot;7月&quot;</span>, <span class="string">&quot;8月&quot;</span>, <span class="string">&quot;9月&quot;</span>))</span><br><span class="line">abline(h = <span class="number">100</span>, lwd = <span class="number">2</span>, lty = <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<hr />
<h1 id="plotting-systems-in-r">Plotting Systems in R</h1>
<h2 id="base-plotting-system">Base Plotting System</h2>
<ul>
<li>Base:artist's palette model, and usually needs two steps to create a plot</li>
<li>representative:plot()</li>
</ul>
<h3 id="two-packages">Two packages</h3>
<ul>
<li>graphics(including plot, hist, boxplot, etc)</li>
<li>grDevices(including X11, PDF, PostScript, PNG, etc)</li>
</ul>
<h3 id="two-steps-to-create-a-base-plot">Two steps to create a base plot</h3>
<ul>
<li>Initializing a new plot <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(datasets)</span><br><span class="line">with(airquality, plot(Wind, Ozone)) <span class="comment"># Scatterplot</span></span><br></pre></td></tr></table></figure></li>
<li>Annotation an existing plot <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">model &lt;- lm(Ozone ~ Wind, airquality)</span><br><span class="line">abline(model, lwd = <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="base-plotting-functions">Base Plotting Functions</h3>
<h4 id="initialize">Initialize</h4>
<ul>
<li>plot:initialize a new plot</li>
<li>hist:initialize a new hist</li>
<li>boxplot:initialize a new boxplot</li>
</ul>
<h4 id="add">Add</h4>
<ul>
<li>lines:add lines to a plot</li>
<li>abline:add lines to a plot</li>
<li>points:add points to a plot</li>
<li>text:add text labels to a plot using specified x, y coordinates</li>
<li>title:add annotations to x, y axis labels, title, subtitle, outer margin</li>
<li>mtext:add arbitrary text to the margins</li>
<li>axis:add axis labels</li>
</ul>
<h3 id="some-important-base-graphics-parameters">Some Important Base Graphics Parameters</h3>
<ul>
<li>pch:the plotting symbol</li>
<li>lty:the line type</li>
<li>lwd:the line width</li>
<li>col:color</li>
<li>xlab:string for the xlab</li>
<li>ylab:string for the ylab</li>
<li>las:the orientation of the axis</li>
<li>bg:thebackground color</li>
<li>mar:the margin size</li>
<li>oma:the outer margin size</li>
<li>mfrow:number of plots per row, column</li>
<li>mfcol:number of plots per row, column(differ in order)</li>
</ul>
<p>Default parameters:</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">par(<span class="string">&quot;bg&quot;</span>) <span class="comment"># &quot;transparent&quot;</span></span><br><span class="line">par(<span class="string">&quot;mar&quot;</span>) <span class="comment"># 4 4 2 1</span></span><br></pre></td></tr></table></figure>
<h3 id="examples">Examples</h3>
<h4 id="examplelegend">Example:Legend</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">with(airquality, plot(Wind, Ozone, main = <span class="string">&quot;Ozone and Wind in New York&quot;</span>, type = <span class="string">&quot;n&quot;</span>))</span><br><span class="line">with(subset(airquality, Month == <span class="number">5</span>), points(Wind, Ozone, col = <span class="string">&quot;blue&quot;</span>))</span><br><span class="line">with(subset(airquality, Month != <span class="number">5</span>), points(Wind, Ozone, col = <span class="string">&quot;red&quot;</span>))</span><br><span class="line">legend(<span class="string">&quot;topright&quot;</span>, pch = <span class="number">1</span>, col = <span class="built_in">c</span>(<span class="string">&quot;blue&quot;</span>, <span class="string">&quot;red&quot;</span>), legend = <span class="built_in">c</span>(<span class="string">&quot;May&quot;</span>, <span class="string">&quot;Other Months&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/base_ex1.png" /></p>
<h4 id="examplemultiple-base-plots">Example:Multiple Base Plots</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">par(mfrow = <span class="built_in">c</span>(<span class="number">1</span>, <span class="number">3</span>), mar = <span class="built_in">c</span>(<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>), oma = <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">with(airquality, &#123;</span><br><span class="line">    plot(Wind, Ozone, main = <span class="string">&quot;Ozone and Wind&quot;</span>)</span><br><span class="line">    plot(Solar.R, Ozone, main = <span class="string">&quot;Ozone and Solar Radiation&quot;</span>)</span><br><span class="line">    plot(Temp, Ozone, main = <span class="string">&quot;Ozone and Temperature&quot;</span>)</span><br><span class="line">    mtext(<span class="string">&quot;Ozone and Weather in New York&quot;</span>, outer = <span class="literal">TRUE</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/base_ex2.png" /></p>
<h2 id="the-lattice-system">The Lattice System</h2>
<ul>
<li>Lattice:Entire plot specified by one function</li>
<li>useful for plotting high dimensional data(conditioning plots)</li>
<li>different from base plot driectly to the graphics device, lattice plot returns an object of class <strong>trellis</strong> (and will be auto-printed)</li>
<li>representative:xyplot()</li>
</ul>
<h3 id="two-packages-1">Two packages</h3>
<ul>
<li>lattice(including xyplot bwplot, levelplot, etc)</li>
<li>grid(usually indirectedly called through lattice or ggplot2)</li>
</ul>
<h3 id="lattice-functions">Lattice Functions</h3>
<ul>
<li>xplot:create scatterplots</li>
<li>bwplot:box-and-whiskers plots</li>
<li>histogram:histograms</li>
<li>stripplot:like a boxplot but with actual points</li>
<li>dotplot:plot dots on "violin strings"</li>
<li>splom:scatterplot matrix(like pairs in base plotting)</li>
<li>levelplot, contourplot:for plotting "image" data</li>
</ul>
<h3 id="examples-1">Examples</h3>
<h4 id="examplexyplot">Example:xyplot</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(datasets)</span><br><span class="line">library(lattice)</span><br><span class="line">state &lt;- data.frame(state.x77, region = state.region)</span><br><span class="line">xyplot(Life.Exp ~ Income | region, data = state, layout = <span class="built_in">c</span>(<span class="number">4</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/lattice_ex1.png" /></p>
<h4 id="exampleplane-functiuon">Example:plane functiuon</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(lattice)</span><br><span class="line">set.seed(<span class="number">10</span>)</span><br><span class="line">x &lt;- rnorm(<span class="number">100</span>)</span><br><span class="line">f &lt;- <span class="built_in">rep</span>(<span class="number">0</span>:<span class="number">1</span>, each = <span class="number">50</span>)</span><br><span class="line">y &lt;- x + f- f * x + rnorm(<span class="number">100</span>, sd = <span class="number">0.5</span>)</span><br><span class="line">f &lt;- factor(f, labels = <span class="built_in">c</span>(<span class="string">&quot;group1&quot;</span>, <span class="string">&quot;group2&quot;</span>))</span><br><span class="line">xyplot(y ~ x | f, panel = <span class="keyword">function</span>(x, y, ...)&#123;</span><br><span class="line">    panel.xyplot(x, y, ...)</span><br><span class="line">    panel.lmline(x, y, col = <span class="number">2</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/lattice_ex2.png" /></p>
<h2 id="the-ggplot2-system">The ggplot2 System</h2>
<ul>
<li>ggplot2:Mixed elements of Base and Lattice</li>
<li>book<ggplot2>: In brief, thegrammar tells us that a statistical graphic is a mapping from data to aesthetic(美学) attributes(color, shape, size) of geometric objects (points, lines, bars).The plot may also contain statistical transformations of the data and is drawn on a specific corrdinate system.</li>
<li>representative:qplot(), ggplot()</li>
</ul>
<h3 id="basic-components-of-a-ggplot2-plot">Basic Components of a ggplot2 Plot</h3>
<ul>
<li>a data frame</li>
<li>aesthetic mapping:how data are mapped to color, size</li>
<li>geoms:points, lines, shapes</li>
<li>facets:for conditional plots</li>
<li>stats:binning(柱形分析), quantiles, smoothing</li>
<li>scales:for example:sex</li>
<li>corrdinate system</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">g &lt;- ggplot(mpg, aes(displ, hwy))</span><br><span class="line">summary(g)</span><br><span class="line">g + geom_point()</span><br></pre></td></tr></table></figure>
<h3 id="annotation">Annotation</h3>
<ol type="1">
<li>labs and theme</li>
</ol>
<ul>
<li>xlab(), ylab(), ggtitle(), labs()</li>
<li>theme(legend.position = "none")</li>
<li>theme_gray()</li>
<li>theme_bw()</li>
</ul>
<p><figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">g &lt;- ggplot(mpg, aes(displ, hwy))</span><br><span class="line">g + geom_point(color = <span class="string">&quot;steelblue&quot;</span>, size = <span class="number">3.14</span>, alpha = <span class="number">0.5</span>) + labs(x = <span class="built_in">expression</span>(PM[<span class="number">25</span>]))<span class="comment"># alpha表示透明度</span></span><br><span class="line">g + geom_point(aes(color = drv), size = <span class="number">3.14</span>, alpha = <span class="number">0.5</span>) + ggtitle(<span class="string">&quot;title&quot;</span>) + theme(plot.title = element_text(hjust = <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure> 2. ylim and coord_cartesian - ylim(-3, 3)# 把y限定在(-3, 3) - coord_cartesian(ylim = c(-3, 3))# 显示(-3, 3)的范围</p>
<p><figure class="highlight r"><table><tr><td class="code"><pre><span class="line">testdata &lt;- data.frame(x = <span class="number">1</span>:<span class="number">100</span>, y = rnorm(<span class="number">100</span>))</span><br><span class="line">testdata[<span class="number">50</span>,<span class="number">2</span>] &lt;- <span class="number">100</span></span><br><span class="line">g &lt;- ggplot(testdata, aes(x = x,y = y))</span><br><span class="line">g + geom_line()</span><br><span class="line">g + geom_line() + ylim(-<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># 把y限定在(-3, 3)</span></span><br><span class="line">g + geom_line() + coord_cartesian(ylim = <span class="built_in">c</span>(-<span class="number">3</span>, <span class="number">3</span>)) <span class="comment"># 显示(-3, 3)的范围</span></span><br></pre></td></tr></table></figure> 3. cut <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">testdata &lt;- data.frame(x = <span class="number">1</span>:<span class="number">100</span>, y = rnorm(<span class="number">100</span>))</span><br><span class="line">testdata[<span class="number">50</span>,<span class="number">2</span>] &lt;- <span class="number">100</span></span><br><span class="line">cutpoints &lt;- quantile(testdata$y, seq(<span class="number">0</span>, <span class="number">1</span>, <span class="built_in">length</span> = <span class="number">4</span>), na.rm = <span class="built_in">T</span>)</span><br><span class="line">testdata$y_new &lt;- cut(testdata$y, cutpoints)</span><br></pre></td></tr></table></figure></p>
<h3 id="examples-2">Examples</h3>
<h4 id="examplegeom">Example:geom</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">qplot(displ, hwy, data = mpg, geom = <span class="built_in">c</span>(<span class="string">&quot;point&quot;</span>, <span class="string">&quot;smooth&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/ggplot2_ex1.png" /></p>
<h4 id="examplefill">Example:fill</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">qplot(hwy, data = mpg, fill = drv)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/ggplot2_ex2.png" /></p>
<h4 id="examplefacets">Example:facets</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">qplot(displ, hwy, data = mpg, facets = .~drv)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/ggplot2_ex3.png" /> <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">qplot(displ, hwy, data = mpg, facets = drv~.)</span><br></pre></td></tr></table></figure> <img src="/pictures/R/ggplot2_ex4.png" /></p>
<h4 id="exampleboxplot">Example:boxplot</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">qplot(drv, hwy, data = mpg, geom = <span class="string">&quot;boxplot&quot;</span>, color = manufacturer)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/ggplot2_ex5.png" /></p>
<h4 id="exampleggplot">Example:ggplot</h4>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">data(mpg)</span><br><span class="line">g + geom_point() + facet_grid(drv~cyl, margins=<span class="literal">TRUE</span>) + geom_smooth(method=<span class="string">&quot;lm&quot;</span>, size=<span class="number">1</span>, se=<span class="literal">FALSE</span>, color=<span class="string">&quot;pink&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/ggplot2_ex6.png" /></p>
<hr />
<h1 id="color">Color</h1>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">p1 &lt;- colorRampPalette(<span class="built_in">c</span>(<span class="string">&quot;red&quot;</span>,<span class="string">&quot;yellow&quot;</span>))</span><br><span class="line">showMe(p1(<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/color_1.png" /> <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">p2 &lt;- colorRampPalette(<span class="built_in">c</span>(<span class="string">&quot;orange&quot;</span>,<span class="string">&quot;yellow&quot;</span>,<span class="string">&quot;green&quot;</span>))</span><br><span class="line">showMe(p2(<span class="number">100</span>))</span><br></pre></td></tr></table></figure> <img src="/pictures/R/color_2.png" /> <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">cols &lt;- brewer.pal(<span class="number">3</span>, <span class="string">&quot;BuGn&quot;</span>)</span><br><span class="line">showMe(cols)</span><br><span class="line">pal &lt;- colorRampPalette(cols)</span><br><span class="line">showMe(pal(<span class="number">20</span>))</span><br></pre></td></tr></table></figure> <img src="/pictures/R/color_3.png" /></p>
]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting and Cleaning Data</title>
    <url>/2017/12/06/Getting-and-Cleaning-Data/</url>
    <content><![CDATA[<p>前言:本文主要参考了来自coursera上约翰霍普金斯大学Data Science系列课程Course3:Getting and Cleaning Data.</p>
<span id="more"></span>
<hr />
<h1 id="tips">Tips</h1>
<ul>
<li>记录你的每一步操作</li>
<li>变量名应当取得比较详尽,不要缩写</li>
<li>善用??</li>
</ul>
<hr />
<h1 id="data-collection">Data collection</h1>
<h2 id="set-path">set path</h2>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="create-directories">create directories</h2>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!file.exists(<span class="string">&quot;data&quot;</span>))&#123;</span><br><span class="line">	dir.create(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="get-data">get data</h2>
<h3 id="from-csv">from csv</h3>
<ul>
<li>utils包中函数:</li>
<li>download.file():如果是https开头,mac需要指定method = "curl",win不需要;如果是http开头则都不需要</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">fileurl &lt;- <span class="string">&quot;https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accseeType=DOWNLOAD&quot;</span></span><br><span class="line">download.file(fileurl, destfile = <span class="string">&quot;./data/cameras.csv&quot;</span>, method = <span class="string">&quot;curl&quot;</span>) <span class="comment"># mac需要指定curl,win不需要</span></span><br><span class="line">list.files(<span class="string">&quot;./data&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>utils包中函数:</li>
<li>read.table()</li>
<li>read.csv()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">cameraData &lt;- read.table(<span class="string">&quot;./data/cameras.csv&quot;</span>, sep = <span class="string">&quot;,&quot;</span>, header = <span class="literal">TRUE</span>)</span><br><span class="line">cameraData &lt;- read.csv(<span class="string">&quot;./data/cameras.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>重复读取1000次,两种方式读取csv所花时间对比</li>
<li>read.table():1.862808 secs</li>
<li>read.csv():1.861519 secs</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">t1 &lt;- Sys.time()</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">1000</span>)&#123;</span><br><span class="line">  cameraData &lt;- read.table(<span class="string">&quot;./data/cameras.csv&quot;</span>, sep = <span class="string">&quot;,&quot;</span>, header = <span class="literal">TRUE</span>)</span><br><span class="line">&#125;</span><br><span class="line">Sys.time() - t1 <span class="comment"># 1.862808 secs</span></span><br><span class="line"></span><br><span class="line">t2 &lt;- Sys.time()</span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">1000</span>)&#123;</span><br><span class="line">  cameraData &lt;- read.csv(<span class="string">&quot;./data/cameras.csv&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line">Sys.time() - t2 <span class="comment"># 1.861519 secs</span></span><br></pre></td></tr></table></figure>
<h3 id="from-xlsx">from xlsx</h3>
<ul>
<li>首先用excel手动将cameras.csv转存为cameras.xlsx</li>
<li><a href="https://github.com/tidyverse/readxl">readxl</a>包中函数:</li>
<li>read_excel()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">install.packages(<span class="string">&quot;tidyverse&quot;</span>)</span><br><span class="line">install.packages(<span class="string">&quot;readxl&quot;</span>)</span><br><span class="line">library(readxl)</span><br><span class="line">cameraData &lt;- read_excel(<span class="string">&quot;./data/cameras.xlsx&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="from-xml">from xml</h3>
<ul>
<li>XML包中函数:</li>
<li>xmlTreeParse()</li>
<li>xmlRoot()</li>
<li>xmlName()</li>
<li>xmlSApply()</li>
<li>xpathSApply()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">install.packages(<span class="string">&quot;XML&quot;</span>)</span><br><span class="line">library(XML)</span><br><span class="line">fileurl_1 &lt;- <span class="string">&quot;http://www.w3school.com.cn/example/xmle/note.xml&quot;</span></span><br><span class="line">doc_1 &lt;- xmlTreeParse(fileurl_1, useInternal = <span class="literal">TRUE</span>)</span><br><span class="line">doc_1</span><br><span class="line">rootNode &lt;- xmlRoot(doc_1)</span><br><span class="line">rootNode</span><br><span class="line">xmlName(rootNode)</span><br><span class="line">rootNode[[<span class="number">1</span>]]</span><br><span class="line">rootNode[[<span class="number">1</span>]][[<span class="number">1</span>]]</span><br><span class="line">xmlSApply(rootNode, xmlValue) <span class="comment">#获得所有文本</span></span><br><span class="line">xpathSApply(rootNode, <span class="string">&quot;//to&quot;</span>, xmlValue) <span class="comment">#所有标签为to的文本</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_XML_1.png" /></p>
<ul>
<li>另一个例子</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">fileurl_2 &lt;- <span class="string">&quot;http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens&quot;</span></span><br><span class="line">doc_2 &lt;- htmlTreeParse(fileurl_2, useInternal = <span class="literal">TRUE</span>)</span><br><span class="line">time &lt;- xpathSApply(doc_2, <span class="string">&quot;//div[@class=&#x27;game-meta&#x27;]&quot;</span>, xmlValue)</span><br><span class="line">time</span><br><span class="line">team &lt;- xpathSApply(doc_2, <span class="string">&quot;//div[@class=&#x27;game-info&#x27;]&quot;</span>, xmlValue)</span><br><span class="line">team</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_XML_2.png" /></p>
<h3 id="from-json">from JSON</h3>
<ul>
<li>json结构有点类似xml,应用广泛,也是通过API获得的数据的最常见类型</li>
<li>jsonlite包中函数:</li>
<li>fromJSON()</li>
<li>toJSON()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(jsonlite)</span><br><span class="line">jsonData &lt;- fromJSON(<span class="string">&quot;https://api.github.com/users/jtleek/repos&quot;</span>)</span><br><span class="line"><span class="built_in">names</span>(jsonData)</span><br><span class="line"><span class="built_in">names</span>(jsonData$owner)</span><br><span class="line">jsonData$owner$login</span><br><span class="line">myjson &lt;- toJSON(iris, pretty=<span class="literal">TRUE</span>)</span><br><span class="line">iris2 &lt;- fromJSON(myjson)</span><br><span class="line">head(iris2)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_JSON.png" /></p>
<h3 id="from-mysql">from MySQL</h3>
<ul>
<li>DBI包中函数:</li>
<li>dbConnect()</li>
<li>dbGetQuery()</li>
<li>RMySQL包中函数:</li>
<li>MySQL()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(DBI)</span><br><span class="line">install.packages(<span class="string">&quot;RMySQL&quot;</span>)</span><br><span class="line">library(RMySQL)</span><br><span class="line">ucscDb &lt;- dbConnect(MySQL(), user = <span class="string">&quot;genome&quot;</span>, host = <span class="string">&quot;genome-mysql.cse.ucsc.edu&quot;</span>)</span><br><span class="line">result &lt;- dbGetQuery(ucscDb, <span class="string">&quot;show databases;&quot;</span>);dbDisconnect(ucscDb); <span class="comment">#断开连接后会返回TRUE</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一个数据库的例子:hg19</li>
<li>DBI包中函数:</li>
<li>dbListTables()</li>
<li>dbReadTable()</li>
<li>dbSendQuery()</li>
<li>fetch()</li>
<li>dbClearResult()</li>
<li>dbDisconnect()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">hg19 &lt;- dbConnect(MySQL(), user = <span class="string">&quot;genome&quot;</span>, db = <span class="string">&quot;hg19&quot;</span>, host = <span class="string">&quot;genome-mysql.cse.ucsc.edu&quot;</span>)</span><br><span class="line">allTables &lt;- dbListTables(hg19)</span><br><span class="line"><span class="built_in">length</span>(allTables)</span><br><span class="line">allTables[<span class="number">1</span>:<span class="number">5</span>]</span><br><span class="line">dbListFields(hg19, <span class="string">&quot;affyU133Plus2&quot;</span>)</span><br><span class="line">dbGetQuery(hg19, <span class="string">&quot;select count(*) from affyU133Plus2&quot;</span>) <span class="comment">#计算表中所有记录 58463</span></span><br><span class="line">affyData &lt;- dbReadTable(hg19, <span class="string">&quot;affyU133Plus2&quot;</span>)</span><br><span class="line">affyData[<span class="number">1</span>,]</span><br><span class="line">query &lt;- dbSendQuery(hg19, <span class="string">&quot;select * from affyU133Plus2 where misMatches between 1 and 3&quot;</span>)</span><br><span class="line">affyMis &lt;- fetch(query)</span><br><span class="line">quantile(affyMis$misMatches)</span><br><span class="line">affyMisSmall &lt;- fetch(query, n = <span class="number">10</span>)</span><br><span class="line">dbClearResult(query) <span class="comment">#清空查询后会返回TRUE</span></span><br><span class="line"><span class="built_in">dim</span>(affyMisSmall)</span><br><span class="line">dbDisconnect(hg19)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_mysql.png" /></p>
<h3 id="from-hdf5">from HDF5</h3>
<ul>
<li>官网:https://support.hdfgroup.org/HDF5/</li>
<li>Hierarchical Data Format(层次型数据结构)</li>
<li>rhdf5包中函数:</li>
<li>h5createFile()</li>
<li>h5createGroup()</li>
<li>h5write()</li>
<li>h5ls()</li>
<li>h5read()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">source(<span class="string">&quot;http://bioconductor.org/biocLite.R&quot;</span>)</span><br><span class="line">biocLite(<span class="string">&quot;rhdf5&quot;</span>)</span><br><span class="line">library(rhdf5)</span><br><span class="line">created = h5createFile(<span class="string">&quot;./data/example.h5&quot;</span>)</span><br><span class="line">created</span><br><span class="line">created = h5createGroup(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo&quot;</span>)</span><br><span class="line">created = h5createGroup(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;baa&quot;</span>)</span><br><span class="line">created = h5createGroup(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/foobaa&quot;</span>)</span><br><span class="line">A = matrix(<span class="number">1</span>:<span class="number">10</span>, nr = <span class="number">2</span>, nc = <span class="number">5</span>)</span><br><span class="line">h5write(A, <span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/A&quot;</span>)</span><br><span class="line">B = array(seq(<span class="number">0.1</span>,<span class="number">2.0</span>,by=<span class="number">0.1</span>) ,<span class="built_in">dim</span> = <span class="built_in">c</span>(<span class="number">5</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">attr</span>(B, <span class="string">&quot;scale&quot;</span>) &lt;- <span class="string">&quot;liter&quot;</span></span><br><span class="line">h5write(B, <span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/foobaa/B&quot;</span>)</span><br><span class="line">h5ls(<span class="string">&quot;./data/example.h5&quot;</span>)</span><br><span class="line">df = data.frame(<span class="number">1L</span>:<span class="number">5L</span>, seq(<span class="number">0</span>,<span class="number">1</span>,length.out=<span class="number">5</span>), <span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>,<span class="string">&quot;d&quot;</span>,<span class="string">&quot;e&quot;</span>), stringAsFactors = <span class="literal">FALSE</span>)</span><br><span class="line">h5write(df, <span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;df&quot;</span>) <span class="comment"># df直接写进最高群组</span></span><br><span class="line">h5ls(<span class="string">&quot;./data/example.h5&quot;</span>)</span><br><span class="line">readA = h5read(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/A&quot;</span>)</span><br><span class="line">readB = h5read(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/foobaa/B&quot;</span>)</span><br><span class="line">readdf = h5read(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;df&quot;</span>)</span><br><span class="line">readA</span><br><span class="line">h5write(<span class="built_in">c</span>(<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>), <span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/A&quot;</span>, index = <span class="built_in">list</span>(<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>))</span><br><span class="line">h5read(<span class="string">&quot;./data/example.h5&quot;</span>, <span class="string">&quot;foo/A&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_HDF5.png" /></p>
<h3 id="from-the-web">from The Web</h3>
<ul>
<li>base包中函数:</li>
<li>url()</li>
<li>readLines()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">con = url(<span class="string">&quot;http://www.msgsxj.cn/about/&quot;</span>)</span><br><span class="line">htmlCode = readLines(con)</span><br><span class="line">close(con)</span><br><span class="line">htmlCode[<span class="number">378</span>:<span class="number">380</span>]</span><br></pre></td></tr></table></figure>
<figure>
<img src="/pictures/R/R_me.png" alt="me" /><figcaption aria-hidden="true">me</figcaption>
</figure>
<ul>
<li>XML包中函数:</li>
<li>htmlTreeParse()</li>
<li>htmlParse()</li>
<li>xpathSApply()</li>
<li>httr包中函数:</li>
<li>GET()</li>
<li>content()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(XML)</span><br><span class="line">url &lt;- <span class="string">&quot;http://xueshu.baidu.com/scholarID/CN-BP75S7TJ&quot;</span> <span class="comment"># 周志华</span></span><br><span class="line">html &lt;- htmlTreeParse(url, useInternalNodes = <span class="built_in">T</span>)</span><br><span class="line">xpathSApply(html, <span class="string">&quot;//div[@class=&#x27;res_info&#x27;]&quot;</span>, xmlValue)</span><br><span class="line"><span class="comment"># httr包的等效实现</span></span><br><span class="line">library(httr)</span><br><span class="line">html2 &lt;- GET(url)</span><br><span class="line">content2 &lt;- content(html2, as = <span class="string">&quot;text&quot;</span>)</span><br><span class="line">parsedHtml &lt;- htmlParse(content2, asText = <span class="literal">TRUE</span>)</span><br><span class="line">xpathSApply(parsedHtml, <span class="string">&quot;//div[@class=&#x27;res_info&#x27;]&quot;</span>, xmlValue)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_zhouzhihua.png" /></p>
<ul>
<li>httr包中函数:</li>
<li>GET():websites with passwords</li>
<li>handle()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">pg1 = GET(<span class="string">&quot;https://httpbin.org/basic-auth/user/passwd&quot;</span>)</span><br><span class="line">pg1</span><br><span class="line"><span class="built_in">names</span>(pg1)</span><br><span class="line">pg2 = GET(<span class="string">&quot;https://httpbin.org/basic-auth/user/passwd&quot;</span>, authenticate(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;passwd&quot;</span>))</span><br><span class="line">pg2</span><br><span class="line"><span class="built_in">names</span>(pg2)</span><br><span class="line">baidu = handle(<span class="string">&quot;https://baidu.com&quot;</span>)</span><br><span class="line">pg3 = GET(handle = baidu, path = <span class="string">&quot;/&quot;</span>)</span><br><span class="line">pg3</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_httr.png" /></p>
<h3 id="from-api">from API</h3>
<ul>
<li><a href="https://github.com/r-lib/httr/blob/master/demo/oauth2-github.r"></a></li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(httr)</span><br><span class="line"><span class="comment"># 1. Find OAuth settings for github:</span></span><br><span class="line"><span class="comment">#    http://developer.github.com/v3/oauth/</span></span><br><span class="line">oauth_endpoints(<span class="string">&quot;github&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. To make your own application, register at</span></span><br><span class="line"><span class="comment">#    https://github.com/settings/developers. Use any URL for the homepage URL</span></span><br><span class="line"><span class="comment">#    (http://github.com is fine) and  http://localhost:1410 as the callback url</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    Replace your key and secret below.</span></span><br><span class="line">myapp &lt;- oauth_app(<span class="string">&quot;github&quot;</span>,</span><br><span class="line">  key = <span class="string">&quot;56b637a5baffac62cad9&quot;</span>,</span><br><span class="line">  secret = <span class="string">&quot;8e107541ae1791259e9987d544ca568633da2ebf&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Get OAuth credentials</span></span><br><span class="line">github_token &lt;- oauth2.0_token(oauth_endpoints(<span class="string">&quot;github&quot;</span>), myapp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Use API</span></span><br><span class="line">gtoken &lt;- config(token = github_token)</span><br><span class="line">req &lt;- GET(<span class="string">&quot;https://api.github.com/rate_limit&quot;</span>, gtoken)</span><br><span class="line">stop_for_status(req)</span><br><span class="line">content(req)</span><br><span class="line"><span class="comment"># OR:</span></span><br><span class="line">req &lt;- with_config(gtoken, GET(<span class="string">&quot;https://api.github.com/rate_limit&quot;</span>))</span><br><span class="line">stop_for_status(req)</span><br><span class="line">content(req)</span><br></pre></td></tr></table></figure>
<h3 id="from-other-sources">from other sources</h3>
<ul>
<li>下列函数或包自行查询文档:</li>
<li>file()</li>
<li>url()</li>
<li>gzfile()</li>
<li>bzfile()</li>
<li>read.arff()(Weka)</li>
<li>read.dta()(Stata)</li>
<li>read.mtp()(Minitab)</li>
<li>read.octave()(Octave)</li>
<li>read.spss()(SPSS)</li>
<li>read.xport()(SAS)</li>
<li>read_fwf(.for)</li>
<li>RPostresSQL包(provide a DBI-compliant(兼容数据库接口)的数据库连接)</li>
<li>RODBC包(为PostgreQL,MySQL,Microsoft Accsee SQLite提供接口)</li>
<li>RMongo包,rmongodb包(MongoDB)</li>
<li>jpeg(),readbitmap(),png(),EBImage包(读取图像数据)</li>
<li>rdgal包,rgeos包,raster包(读取GIS(Geographic Information System)数据)</li>
<li>tuneR包,seewave包(读取MP3)</li>
</ul>
<h2 id="some-data-resources">Some Data Resources</h2>
<ul>
<li>United Bations:http://data.un.org/</li>
<li>U.S.:http://www.data.gov/</li>
<li>United Kingdom:http://data.gov.uk/</li>
<li>France http://www.data.gouv.fr/</li>
<li>Ghana:http://data.gov.gh/</li>
<li>Australia:http://data.gov.au/</li>
<li>Grenamy:http://www.govdata.de/</li>
<li>Hong Kong:http://www.gov.hk/en/theme/psi/datasets/</li>
<li>Janpan:http://www.data.go.jp/</li>
<li>Many more:http://www.data.gov/opendatasites</li>
<li>http://www.gapminder.org/</li>
<li>http://www.asdfree.com/</li>
<li>http://www.kaggle.com/</li>
<li>Hilary Mason:http://bitly.com/bundles/hmason/1</li>
<li>Jeff Hammerbacher:http://www.quora.com/Jeff-Hammerbacher/Introduction-to-Data-Science-Data-Sets</li>
</ul>
<hr />
<h1 id="making-data-tidy">Making data tidy</h1>
<h2 id="subsetting-and-sorting">Subsetting and Sorting</h2>
<h3 id="subsetting">Subsetting</h3>
<ul>
<li>X[]</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">set.seed(<span class="number">13435</span>)</span><br><span class="line">X &lt;- data.frame(<span class="string">&quot;var1&quot;</span>=sample(<span class="number">1</span>:<span class="number">5</span>),<span class="string">&quot;var2&quot;</span>=sample(<span class="number">6</span>:<span class="number">10</span>),<span class="string">&quot;var3&quot;</span>=sample(<span class="number">11</span>:<span class="number">15</span>))</span><br><span class="line">X &lt;- X[sample(<span class="number">1</span>:<span class="number">5</span>),]</span><br><span class="line">X$var2[<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">3</span>)]=<span class="literal">NA</span></span><br><span class="line">X</span><br><span class="line">X[,<span class="number">1</span>] <span class="comment">#提取第一列数据</span></span><br><span class="line">X[,<span class="string">&quot;var1&quot;</span>] <span class="comment">#提取var1数据</span></span><br><span class="line">X[<span class="number">1</span>:<span class="number">2</span>,<span class="string">&quot;var2&quot;</span>]</span><br><span class="line">X[(X$var1 &lt;= <span class="number">3</span> &amp; X$var3 &gt; <span class="number">11</span>),]</span><br><span class="line">X[(X$var1 &lt;= <span class="number">3</span> | X$var3 &gt; <span class="number">15</span>),]</span><br><span class="line">X[which(X$var2 &gt; <span class="number">8</span>),]</span><br><span class="line">X[(X$var2 &gt; <span class="number">8</span>),] <span class="comment">#会返回NA</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="/pictures/R/R_X%5B%5D.png" alt="X[]" /><figcaption aria-hidden="true">X[]</figcaption>
</figure>
<h3 id="sorting">Sorting</h3>
<ul>
<li>base包中函数:</li>
<li>sort()</li>
<li>order()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">set.seed(<span class="number">13435</span>)</span><br><span class="line">X &lt;- data.frame(<span class="string">&quot;var1&quot;</span>=sample(<span class="number">1</span>:<span class="number">5</span>),<span class="string">&quot;var2&quot;</span>=sample(<span class="number">6</span>:<span class="number">10</span>),<span class="string">&quot;var3&quot;</span>=sample(<span class="number">11</span>:<span class="number">15</span>))</span><br><span class="line">X &lt;- X[sample(<span class="number">1</span>:<span class="number">5</span>),]</span><br><span class="line">X$var2[<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">3</span>)]=<span class="literal">NA</span></span><br><span class="line">sort(X$var1)</span><br><span class="line">sort(X$var1, decreasing = <span class="literal">TRUE</span>)</span><br><span class="line">sort(X$var2, na.last = <span class="literal">TRUE</span>)</span><br><span class="line">X[order(X$var1),]</span><br><span class="line">X[order(X$var1,X$var3),]</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_sort.png" /></p>
<ul>
<li>plyr包中函数:</li>
<li>arrange()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(plyr)</span><br><span class="line">set.seed(<span class="number">13435</span>)</span><br><span class="line">X &lt;- data.frame(<span class="string">&quot;var1&quot;</span>=sample(<span class="number">1</span>:<span class="number">5</span>),<span class="string">&quot;var2&quot;</span>=sample(<span class="number">6</span>:<span class="number">10</span>),<span class="string">&quot;var3&quot;</span>=sample(<span class="number">11</span>:<span class="number">15</span>))</span><br><span class="line">X &lt;- X[sample(<span class="number">1</span>:<span class="number">5</span>),]</span><br><span class="line">X$var2[<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">3</span>)]=<span class="literal">NA</span></span><br><span class="line">arrange(X, var1)</span><br><span class="line">arrange(X, desc(var1))</span><br><span class="line">X$var4 &lt;- rnorm(<span class="number">5</span>)</span><br><span class="line">X</span><br><span class="line">Y &lt;- cbind(X, rnorm(<span class="number">5</span>))</span><br><span class="line">Y</span><br></pre></td></tr></table></figure>
<p>![(/pictures/R/R_pylr.png)</p>
<h2 id="summarizing-data">Summarizing Data</h2>
<ul>
<li>下载数据</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">fileurl &lt;- <span class="string">&quot;https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accseeType=DOWNLOAD&quot;</span></span><br><span class="line">download.file(fileurl, destfile = <span class="string">&quot;./data/restaurants.csv&quot;</span>, method = <span class="string">&quot;curl&quot;</span>)</span><br><span class="line">restData &lt;- read.csv(<span class="string">&quot;./data/restaurants.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>utils包中函数</li>
<li>head()</li>
<li>tail()</li>
<li>str()</li>
<li>base包中函数:</li>
<li>summary()</li>
<li>table()</li>
<li>stats包中函数:</li>
<li>quantile()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">head(restData, n = <span class="number">1</span>)</span><br><span class="line">tail(restData, n = <span class="number">1</span>)</span><br><span class="line">summary(restData)</span><br><span class="line">str(restData) <span class="comment"># 比summary好用</span></span><br><span class="line">quantile(restData$councilDistrict, na.rm = <span class="literal">TRUE</span>)</span><br><span class="line">quantile(restData$councilDistrict, probs = <span class="built_in">c</span>(<span class="number">0.5</span>,<span class="number">0.75</span>,<span class="number">0.9</span>))</span><br><span class="line">table(restData$zipCode, useNA =<span class="string">&quot;ifany&quot;</span>) <span class="comment"># useNA =&quot;ifany&quot;为统计NA个数</span></span><br><span class="line">table(restData$councilDistrict, restData$zipCode)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_str.png" /></p>
<ul>
<li>base包中函数:</li>
<li>is.na()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>(<span class="built_in">is.na</span>(restData$councilDistrict))</span><br><span class="line"><span class="built_in">any</span>(<span class="built_in">is.na</span>(restData$councilDistrict))</span><br><span class="line"><span class="built_in">all</span>(restData$zipCode &gt; <span class="number">0</span>)</span><br><span class="line">colSums(<span class="built_in">is.na</span>(restData))</span><br><span class="line"><span class="built_in">all</span>(colSums(<span class="built_in">is.na</span>(restData)) == <span class="number">0</span>)</span><br><span class="line">table(restData$zipCode %in% <span class="built_in">c</span>(<span class="string">&quot;21212&quot;</span>))</span><br><span class="line">table(restData$zipCode %in% <span class="built_in">c</span>(<span class="string">&quot;21212&quot;</span>,<span class="string">&quot;21213&quot;</span>))</span><br><span class="line">restData[(restData$zipCode %in% <span class="built_in">c</span>(<span class="string">&quot;21212&quot;</span>,<span class="string">&quot;21213&quot;</span>)),]</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_is.na.png" /></p>
<ul>
<li>base包中函数:</li>
<li>as.data.frame()</li>
<li>stats中函数:</li>
<li>xtabs()</li>
<li>ftable()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">data(UCBAdmissions)</span><br><span class="line">DF = as.data.frame(UCBAdmissions)</span><br><span class="line">summary(DF)</span><br><span class="line">xt &lt;- xtabs(Freq ~ Gender + Admit, data = DF)</span><br><span class="line">xt</span><br><span class="line">warpbreaks$replicate &lt;- <span class="built_in">rep</span>(<span class="number">1</span>:<span class="number">9</span>, len = <span class="number">54</span>)</span><br><span class="line">xt &lt;- xtabs(breaks ~., data = warpbreaks)</span><br><span class="line">ftable(xt)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_xtabs.png" /></p>
<ul>
<li>utils包中函数:</li>
<li>object.size()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">fakeData = rnorm(<span class="number">1e5</span>)</span><br><span class="line">object.size(fakeData)</span><br><span class="line">print(object.size(fakeData), units = <span class="string">&quot;Mb&quot;</span>) <span class="comment"># 0.8 Mb</span></span><br></pre></td></tr></table></figure>
<h2 id="creating-new-variables">Creating new Variables</h2>
<ul>
<li>下载数据</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">fileurl &lt;- <span class="string">&quot;https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accseeType=DOWNLOAD&quot;</span></span><br><span class="line">download.file(fileurl, destfile = <span class="string">&quot;./data/restaurants.csv&quot;</span>, method = <span class="string">&quot;curl&quot;</span>)</span><br><span class="line">restData &lt;- read.csv(<span class="string">&quot;./data/restaurants.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="create-sequences">Create sequences</h3>
<ul>
<li>base包中函数:</li>
<li>seq()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">seq(<span class="number">1</span>, <span class="number">10</span>, by = <span class="number">2</span>) <span class="comment"># 1 3 5 7 9</span></span><br><span class="line">seq(<span class="number">1</span>, <span class="number">10</span>, <span class="built_in">length</span> = <span class="number">3</span>) <span class="comment"># 1.0  5.5 10.0</span></span><br><span class="line"><span class="built_in">c</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">25</span>, <span class="number">100</span>) <span class="comment"># 1 3 8 25 100</span></span><br><span class="line">seq(along = x) <span class="comment"># 1 2 3 4 5</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_sequence.png" /></p>
<h3 id="create-binary-variables">Create binary variables</h3>
<ul>
<li>base包中函数:</li>
<li>ifelse()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">restData$zipWrong &lt;- ifelse(restData$zipCode &lt; <span class="number">0</span>, <span class="built_in">T</span>, <span class="built_in">F</span>)</span><br><span class="line"><span class="built_in">class</span>(restData$zipWrong)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_logical.png" /></p>
<h3 id="create-categorical-variables">Create categorical variables</h3>
<ul>
<li>base包中函数:</li>
<li>cut()</li>
<li>Hmisc包中函数:</li>
<li>cut2()</li>
<li>plyr包中函数:</li>
<li>mutate()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">install.packages(<span class="string">&quot;Hmisc&quot;</span>)</span><br><span class="line">restData$zipGroups &lt;- cut(restData$zipCode, breaks = quantile(restData$zipCode))</span><br><span class="line">table(restData$zipGroups)</span><br><span class="line"><span class="built_in">class</span>(restData$zipGroups)</span><br><span class="line">library(Hmisc)</span><br><span class="line">restData$zipGroups &lt;- cut2(restData$zipCode, g = <span class="number">4</span>)</span><br><span class="line">table(restData$zipGroups)</span><br><span class="line"><span class="built_in">class</span>(restData$zipGroups)</span><br><span class="line">library(plyr)</span><br><span class="line">restData2 &lt;- mutate(restData, zipGroups = cut2(zipCode, g = <span class="number">4</span>)) <span class="comment"># 新变量加入</span></span><br><span class="line">table(restData2$zipGroups)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_category.png" /></p>
<ul>
<li>base包中函数:</li>
<li>factor()</li>
<li>stats包中函数:</li>
<li>relevel()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">restData$zcf &lt;- factor(restData$zipCode)</span><br><span class="line">restData$zcf[<span class="number">1</span>:<span class="number">10</span>]</span><br><span class="line"><span class="built_in">class</span>(restData$zcf)</span><br><span class="line">yesno &lt;- sample(<span class="built_in">c</span>(<span class="string">&quot;yes&quot;</span>,<span class="string">&quot;no&quot;</span>), size = <span class="number">10</span>, replace = <span class="literal">TRUE</span>)</span><br><span class="line"><span class="built_in">class</span>(yesno)</span><br><span class="line">yesnofac &lt;- factor(yesno, levels = <span class="built_in">c</span>(<span class="string">&quot;yes&quot;</span>,<span class="string">&quot;no&quot;</span>))</span><br><span class="line"><span class="built_in">class</span>(yesnofac)</span><br><span class="line">relevel(yesnofac, ref = <span class="string">&quot;yes&quot;</span>) <span class="comment"># yes 设置为比较低的水平</span></span><br><span class="line"><span class="built_in">as.numeric</span>(yesnofac)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_factor.png" /></p>
<h3 id="common-transforms">Common transforms</h3>
<ul>
<li>abs()</li>
<li>sqrt()</li>
<li>ceiling()</li>
<li>floor()</li>
<li>round(x, digits = n) # 小数点后有效数字</li>
<li>signif(x, digits = n) # 一共有效数字</li>
<li>cos()</li>
<li>log()</li>
<li>log2()</li>
<li>log10()</li>
<li>exp()</li>
</ul>
<h2 id="reshaping-data">Reshaping Data</h2>
<ul>
<li>reshape2包中函数:</li>
<li>melt()</li>
<li>dcast()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(reshape2)</span><br><span class="line"><span class="built_in">dim</span>(mtcars)</span><br><span class="line">mtcars$carname &lt;- rownames(mtcars)</span><br><span class="line">carMelt &lt;- melt(mtcars, id = <span class="built_in">c</span>(<span class="string">&quot;carname&quot;</span>,<span class="string">&quot;gear&quot;</span>,<span class="string">&quot;cyl&quot;</span>), measure.vars = <span class="built_in">c</span>(<span class="string">&quot;mpg&quot;</span>,<span class="string">&quot;hp&quot;</span>)) <span class="comment"># 样本量翻倍</span></span><br><span class="line"><span class="built_in">dim</span>(carMelt)</span><br><span class="line">head(carMelt)</span><br><span class="line">tail(carMelt)</span><br><span class="line"><span class="built_in">class</span>(carMelt$variable)</span><br><span class="line">cylData &lt;- dcast(carMelt, cyl ~ variable, mean)</span><br><span class="line">cylData <span class="comment"># 对melt函数中设定的variable按cyl求平均</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_melt.png" /></p>
<ul>
<li>base包中函数:</li>
<li>tapply()</li>
<li>split()</li>
<li>lappy()</li>
<li>unlist()</li>
<li>sapply()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">tapply(InsectSprays$count, InsectSprays$spray, <span class="built_in">sum</span>) <span class="comment"># 按杀虫剂种类求和</span></span><br><span class="line">spIns &lt;- split(InsectSprays$count, InsectSprays$spray);spIns</span><br><span class="line">sprCount &lt;- lapply(spIns, <span class="built_in">sum</span>);sprCount <span class="comment"># 按杀虫剂种类求和</span></span><br><span class="line">unlist(sprCount)</span><br><span class="line">sapply(spIns, <span class="built_in">sum</span>) <span class="comment"># sapply为lapply简化版</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_tapply.png" /></p>
<ul>
<li>plyr包中函数:</li>
<li>ddply()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(plyr)</span><br><span class="line">ddply(InsectSprays, .(spray), summarise, <span class="built_in">sum</span>=<span class="built_in">sum</span>(count))</span><br><span class="line">spraySums &lt;- ddply(InsectSprays, .(spray), summarise, <span class="built_in">sum</span>=ave(count))</span><br><span class="line"><span class="built_in">dim</span>(spraySums)</span><br><span class="line">head(spraySums)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_ddply.png" /></p>
<h2 id="editing-text-variable">Editing Text Variable</h2>
<ul>
<li>下载数据</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">setwd(<span class="string">&quot;C:/Users/msgsxj/Desktop/coursera/Getting and Cleaning Data&quot;</span>)</span><br><span class="line">fileurl &lt;- <span class="string">&quot;https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accseeType=DOWNLOAD&quot;</span></span><br><span class="line">download.file(fileurl, destfile = <span class="string">&quot;./data/camera2.csv&quot;</span>, method = <span class="string">&quot;curl&quot;</span>)</span><br><span class="line">camera2Data &lt;- read.csv(<span class="string">&quot;./data/camera2.csv&quot;</span>)</span><br><span class="line"><span class="built_in">names</span>(camera2Data)</span><br></pre></td></tr></table></figure>
<ul>
<li>base包中函数:</li>
<li>tolower()</li>
<li>strsplit()</li>
<li>sapply()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="built_in">names</span>(camera2Data) &lt;- tolower(<span class="built_in">names</span>(camera2Data)) <span class="comment"># 变量名全部变小写</span></span><br><span class="line"><span class="built_in">names</span>(camera2Data)</span><br><span class="line">splitNames &lt;- strsplit(<span class="built_in">names</span>(camera2Data),<span class="string">&quot;\\.&quot;</span>) <span class="comment"># split 需转义</span></span><br><span class="line">splitNames[[<span class="number">6</span>]]</span><br><span class="line">splitNames[[<span class="number">6</span>]][<span class="number">1</span>]</span><br><span class="line">firstElement &lt;- <span class="keyword">function</span>(x)&#123;x[<span class="number">1</span>]&#125;</span><br><span class="line"><span class="built_in">names</span>(camera2Data) &lt;- sapply(splitNames,firstElement)  <span class="comment"># 想只保留splitNames[[6]]的第一部分</span></span><br><span class="line"><span class="built_in">names</span>(camera2Data)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_sapply.png" /></p>
<ul>
<li>上述srtsplit()+sapply()过程的相似处理:用""取代"."</li>
<li>sub()</li>
<li>gsub() 替换所有</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">camera2Data &lt;- read.csv(<span class="string">&quot;./data/camera2.csv&quot;</span>) <span class="comment">#重新载入</span></span><br><span class="line"><span class="built_in">names</span>(camera2Data)</span><br><span class="line">sub(<span class="string">&quot;\\.&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="built_in">names</span>(camera2Data),)  <span class="comment">#需转义</span></span><br><span class="line">mytest &lt;- <span class="string">&quot;this_is_a_test&quot;</span></span><br><span class="line">sub(<span class="string">&quot;_&quot;</span>, <span class="string">&quot;&quot;</span>, mytest) <span class="comment">#不需要转义</span></span><br><span class="line">gsub(<span class="string">&quot;_&quot;</span>, <span class="string">&quot;&quot;</span>, mytest)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_sub.png" /></p>
<ul>
<li>base包中函数:</li>
<li>grep()</li>
<li>grepl():返回一列logical</li>
<li>更多正则表达式规则见<a href="https://en.wikipedia.org/wiki/Regular_expression">维基百科</a></li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">grep(<span class="string">&quot;Alameda&quot;</span>, camera2Data$intersection) <span class="comment"># 4 5 36</span></span><br><span class="line">table(grepl(<span class="string">&quot;Alameda&quot;</span>, camera2Data$intersection))</span><br><span class="line">camera2Data_noAlameda &lt;- camera2Data[!grepl(<span class="string">&quot;Alameda&quot;</span>, camera2Data$intersection),]</span><br><span class="line">grep(<span class="string">&quot;Alameda&quot;</span>, camera2Data$intersection, value = <span class="built_in">T</span>) <span class="comment"># 直接返回值</span></span><br><span class="line"><span class="built_in">length</span>(grep(<span class="string">&quot;JeffStreet&quot;</span>, camera2Data$intersection)) <span class="comment"># 检查是否出现</span></span><br><span class="line"><span class="built_in">length</span>(grep(<span class="string">&#x27;^[12]/2/2007&#x27;</span>, <span class="string">&#x27;22/2/2007&#x27;</span>, value = <span class="built_in">T</span>))</span><br><span class="line"><span class="built_in">length</span>(grep(<span class="string">&#x27;^[12]/2/2007&#x27;</span>, <span class="string">&#x27;2/2/2007&#x27;</span>, value = <span class="built_in">T</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_grep.png" /></p>
<ul>
<li>base包中函数:</li>
<li>nchar()</li>
<li>substr()</li>
<li>paste()</li>
<li>paste0()</li>
<li>stringr包中函数:</li>
<li>str_trim()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(stringr)</span><br><span class="line">nchar(<span class="string">&quot;Jeffery Leek&quot;</span>)</span><br><span class="line">substr(<span class="string">&quot;Jeffery Leek&quot;</span>,<span class="number">1</span>,<span class="number">7</span>)</span><br><span class="line">paste(<span class="string">&quot;Jeffery&quot;</span>,<span class="string">&quot;Leek&quot;</span>) <span class="comment">#自动加上了空格</span></span><br><span class="line">paste0(<span class="string">&quot;Jeffery&quot;</span>,<span class="string">&quot;Leek&quot;</span>) <span class="comment">#不加空格</span></span><br><span class="line">str_trim(<span class="string">&quot;Jeff      &quot;</span>) <span class="comment">#去掉空格</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_stringr.png" /></p>
<h2 id="date">Date</h2>
<ul>
<li>base包中函数:</li>
<li>date()</li>
<li>Sys.Date()</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">d1 &lt;- date()</span><br><span class="line"><span class="built_in">class</span>(d1)</span><br><span class="line">d2 &lt;- Sys.Date()</span><br><span class="line"><span class="built_in">class</span>(d2)</span><br><span class="line">format(d2, <span class="string">&quot;%a %b %d&quot;</span>)</span><br><span class="line">julian(d2)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_date.png" /></p>
<ul>
<li>lubridate包中函数:</li>
<li>ymd()</li>
<li>ymd_hms()</li>
<li>dmy():返回日期</li>
<li>wday():返回星期</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(lubridate)</span><br><span class="line">ymd(<span class="string">&quot;20140108&quot;</span>)</span><br><span class="line">mdy(<span class="string">&quot;08/04/2013&quot;</span>)</span><br><span class="line">ymd_hms(<span class="string">&quot;2011-08-03 10:15:03&quot;</span>)</span><br><span class="line">ymd_hms(<span class="string">&quot;2011-08-03 10:15:03&quot;</span>, tz = <span class="string">&quot;Pacific/Auckland&quot;</span>)</span><br><span class="line">x = dmy(<span class="built_in">c</span>(<span class="string">&quot;1jan2013&quot;</span>,<span class="string">&quot;2jan2013&quot;</span>,<span class="string">&quot;31mar2013&quot;</span>,<span class="string">&quot;30jul2013&quot;</span>))</span><br><span class="line">x[<span class="number">1</span>]</span><br><span class="line">wday(x[<span class="number">1</span>])</span><br><span class="line">wday(x[<span class="number">1</span>],label=<span class="built_in">T</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_lubridate.png" /></p>
<hr />
<h1 id="一些强大的技巧">一些强大的技巧</h1>
<h2 id="section">%&gt;%</h2>
<ul>
<li>%&gt;%:管道函数</li>
<li>下面两条代码等价</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">anscombe_tidy &lt;- anscombe %&gt;% mutate(observation = <span class="built_in">seq_len</span>(n()))</span><br><span class="line">anscombe_tidy &lt;- mutate(anscombe,observation = <span class="built_in">seq_len</span>(n()))</span><br></pre></td></tr></table></figure>
<h2 id="parse_number">parse_number()</h2>
<ul>
<li>readr包中函数:</li>
<li>parse_number():获得数字</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(readr)</span><br><span class="line">parse_number(<span class="string">&quot;$1000&quot;</span>) <span class="comment"># 1000</span></span><br><span class="line">parse_number(<span class="string">&quot;1,234,567.78&quot;</span>) <span class="comment"># 1234568</span></span><br><span class="line">parse_number(<span class="string">&quot;class5&quot;</span>) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_parse_number.png" /></p>
<h2 id="print10">print(10)</h2>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">k &lt;- &#123;<span class="number">10</span>; <span class="number">5</span>; <span class="number">5</span>; <span class="number">88</span>; <span class="number">7</span>&#125;  <span class="comment"># 取最后一个赋值</span></span><br><span class="line">k</span><br><span class="line">k &lt;- &#123;print(<span class="number">10</span>); <span class="number">5</span>&#125;</span><br><span class="line">k</span><br><span class="line">k &lt;- &#123;print(<span class="number">10</span>)&#125;</span><br><span class="line">k</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_print%2810%29.png" /></p>
<h2 id="data.table-package">data.table package</h2>
<ul>
<li>data.table继承于data.frame</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(data.table)</span><br><span class="line">DT &lt;- data.table(x = rnorm(<span class="number">9</span>), r = <span class="built_in">rep</span>(<span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>),each=<span class="number">3</span>), z = rnorm(<span class="number">9</span>))</span><br><span class="line">head(DT, <span class="number">3</span>)</span><br><span class="line">DT[<span class="built_in">c</span>(<span class="number">2</span>,<span class="number">3</span>)] <span class="comment"># 取2,3行</span></span><br><span class="line">DT[<span class="number">1</span>:<span class="number">2</span>,<span class="built_in">c</span>(<span class="number">2</span>,<span class="number">3</span>)] <span class="comment"># 取2,3列</span></span><br><span class="line">DT[,<span class="built_in">list</span>(mean(x),<span class="built_in">sum</span>(z))] <span class="comment"># 返回x的均值和z的和</span></span><br><span class="line">DT[,table(r)] <span class="comment"># 返回r的分布</span></span><br><span class="line">DT[,w:=&#123;tmp &lt;- (x+z); log2(tmp+<span class="number">5</span>)&#125;] <span class="comment">#增加一列w</span></span><br><span class="line">DT[,a:=x&gt;<span class="number">0</span>] <span class="comment"># 增加一列a</span></span><br><span class="line">DT[,b:=mean(x+w),by=a] <span class="comment"># 增加一列b</span></span><br><span class="line">DT &lt;- data.table(x = sample(<span class="built_in">letters</span>[<span class="number">1</span>:<span class="number">3</span>], <span class="number">1E5</span>, <span class="literal">TRUE</span>))</span><br><span class="line">DT[, .N, by=x]</span><br><span class="line">DT &lt;- data.table(x=<span class="built_in">rep</span>(<span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>),each=<span class="number">100</span>),y=rnorm(<span class="number">300</span>))</span><br><span class="line">setkey(DT, x)</span><br><span class="line">head(DT[<span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">DT1 &lt;- data.table(x=<span class="built_in">c</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;dt1&#x27;</span>),y=<span class="number">1</span>:<span class="number">4</span>)</span><br><span class="line">DT2 &lt;- data.table(x=<span class="built_in">c</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;dt2&#x27;</span>),z=<span class="number">5</span>:<span class="number">7</span>)</span><br><span class="line">setkey(DT1,x);setkey(DT2,x) <span class="comment"># key不要求唯一</span></span><br><span class="line">DT1</span><br><span class="line">DT2</span><br><span class="line">merge(DT1,DT2) <span class="comment">#合并</span></span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_data.table.png" /></p>
<ul>
<li>fread():读取数据特别快</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(data.table)</span><br><span class="line">big_df &lt;- data.frame(x=rnorm(<span class="number">1E7</span>),y=rnorm(<span class="number">1E7</span>))</span><br><span class="line">file &lt;- tempfile()</span><br><span class="line">write.table(big_df,file=file,row.names=<span class="literal">FALSE</span>,col.names=<span class="literal">TRUE</span>,sep=<span class="string">&quot;\t&quot;</span>,<span class="built_in">quote</span>=<span class="literal">FALSE</span>)</span><br><span class="line">system.time(fread(file))</span><br><span class="line">system.time(read.table(file,header= <span class="literal">TRUE</span>,sep=<span class="string">&quot;\t&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_fread.png" /></p>
<h2 id="lapplyaspplyapplytapplymapply">lapply,aspply,apply,tapply,mapply</h2>
<ul>
<li>lappy:loop over a list and evaluate a function on each element</li>
<li>sapply:same as lappy but try to simplify the result</li>
<li>apply:apply a function over the margins of any array</li>
<li>tapply:apply a function over subsets of a vector</li>
<li>mapply:multivariate version of lapply</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">x &lt;- 1:<span class="number">4</span></span><br><span class="line">lapply(x, runif)</span><br><span class="line">x &lt;- 1:<span class="number">4</span></span><br><span class="line">sapply(x, runif)</span><br><span class="line">x &lt;- matrix(rnorm(<span class="number">200</span>), <span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">apply(x, <span class="number">2</span>, mean)</span><br><span class="line">x &lt;- <span class="built_in">c</span>(rnorm(<span class="number">10</span>), runif(<span class="number">10</span>), rnorm(<span class="number">10</span>,<span class="number">1</span>))</span><br><span class="line">f &lt;- gl(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">tapply(x, f, mean)</span><br><span class="line">mapply(<span class="built_in">rep</span>, <span class="number">1</span>:<span class="number">4</span>, <span class="number">1</span>:<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_apply.png" /></p>
<h2 id="now">now()</h2>
<ul>
<li>now():可查看指定城市时间</li>
<li><a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">可查城市</a></li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">now(<span class="string">&quot;America/New_York&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/pictures/R/R_now.png" /></p>
]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>get data</tag>
        <tag>clean data</tag>
      </tags>
  </entry>
  <entry>
    <title>Latent Dirichlet Allocation</title>
    <url>/2018/10/16/Latent-Dirichlet-Allocation/</url>
    <content><![CDATA[<p>前言: 本文主要是关于Latent Dirichlet Allocation模型, 这是一类能够从文档集合中挖掘出抽象主题的模型, 主要参考了Coursera上国立高等经济大学Advanced Machine Learning系列课程Course3: Bayesian Methods for Machine Learning Week3. <span id="more"></span></p>
<hr />
<h2 id="隐狄利克雷分配模型">隐狄利克雷分配模型</h2>
<p>以推荐系统为例, 假如我读过东野圭吾的新参者, 我希望推荐系统能把白夜行也推荐给我. 在自然语言处理(Natural Language Processing: NLP)中, 主题模型(Topic Model)是一类能够从文档集合中挖掘出抽象主题的模型, 其中用的最多的大概就是隐狄利克雷分配模型(Latent Dirichlet Allocation: LDA). 比方说给定50个主题数量, 其想法是对每一篇文档给出一个关于50个主题数量的分布, 例如新参者的主题是70%的推理类, 10%的叉叉类等, 因此如果能得到每篇文档的主题分布, 就能给出书之间的相似度.</p>
<p>下面我们来看什么是LDA模型, 并给出一些记号. 假设有文档<span class="math inline">\(D\)</span>篇, 其中第<span class="math inline">\(d\)</span>篇文档有<span class="math inline">\(N_d\)</span>个单词, 给定主题数量<span class="math inline">\(T\)</span>, 记第<span class="math inline">\(d\)</span>篇文档的主题为<span class="math inline">\(\theta_d\)</span>, 显然<span class="math inline">\(\theta_d\)</span>为一个<span class="math inline">\(T\)</span>维向量, <span class="math inline">\(z_{dn}\)</span>是第<span class="math inline">\(d\)</span>篇文档的第<span class="math inline">\(n\)</span>个单词的主题分布, <span class="math inline">\(w_{dn}\)</span>是第<span class="math inline">\(d\)</span>篇文档的第<span class="math inline">\(n\)</span>个单词本身(一般给长度为<span class="math inline">\(\left|w_{set}\right|\)</span>定词汇表<span class="math inline">\(w_{set}\)</span>后用一个长度为<span class="math inline">\(\left|w_{set}\right|\)</span>的向量去表示).</p>
<p>给定主题数量<span class="math inline">\(T=50\)</span>, 目前我们有了三个关键要素:</p>
<ul>
<li>每篇文档的主题分布:<span class="math inline">\(\theta\)</span></li>
<li>每篇文档每个单词的主题分布:<span class="math inline">\(z\)</span></li>
<li>每篇文档每个单词:<span class="math inline">\(w\)</span></li>
</ul>
<p>遵循一些直观的假象, 我们分三步尝试去构造模型:</p>
<ul>
<li>首先要有每篇文档的主题<span class="math inline">\(\theta_d\)</span>, 其中<span class="math inline">\(d\in\{1,2,...,D\}\)</span>.</li>
<li>简单假设每个单词的主题<span class="math inline">\(z_{dn}\)</span>与所在文档的主题<span class="math inline">\(\theta_d\)</span>一致, 其中<span class="math inline">\(n\in\{1,2,...,N_d\}\)</span>.</li>
<li>每个单词确定了主题之后就能按照一定的概率分布从主题中选择单词出来. 比方说给定词汇表<span class="math inline">\(w_{set}\)</span>, 其包含<span class="math inline">\(\left|w_{set}\right|\)</span>个单词, 假设第<span class="math inline">\(t\)</span>主题有一个单词表上的概率分布<span class="math inline">\(\varphi_{\cdot t}\)</span>, 显然<span class="math inline">\(\varphi_{\cdot t}\)</span>为<span class="math inline">\(\left|w_{set}\right|\)</span>维列向量, 那么<span class="math inline">\(\varphi\)</span>为<span class="math inline">\(\left|w_{set}\right|\times T\)</span>的未知参数矩阵.</li>
</ul>
<p>将上述三要素视为随机变量, 此时可以写出三者的联合分布函数: <span class="math display">\[P(W, Z, \Theta)=\prod_{d=1}^{D}P(\theta_d)\prod_{n=1}^{N_d}P(z_{dn}\mid\theta_d)P(w_{dn}\mid z_{dn})\]</span>显然文档主题<span class="math inline">\(\theta_d\)</span>和单词主题<span class="math inline">\(z_{dn}\)</span>是无法观测得到的, 因此作为LDA模型中的L(Lantent), D(Dirichlet)则是指随机变量<span class="math inline">\(\theta_d\)</span>满足Dirichlet分布:</p>
<ul>
<li>假定文章的来源一致, 即<span class="math inline">\(\theta_d\)</span>独立同分布: 服从<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet分布</a><span class="math inline">\(\theta_d\sim Dir(\alpha)=\frac{1}{B(\alpha)}\prod_{t=1}^{T}\theta_{dt}^{\alpha_t-1}\)</span>, 其中<span class="math inline">\(\alpha\)</span>为<span class="math inline">\(T\)</span>维列向量; <span class="math inline">\(B(\alpha)\)</span>为多元Beta函数, 可表示为<span class="math inline">\(B(\alpha)=\frac{\prod_{i=1}^{T}\Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^{T}\alpha_i)}\)</span>, <span class="math inline">\(\Gamma\)</span>为<a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma函数</a>, 详见维基百科.</li>
<li>值得注意的是, <span class="math inline">\(\alpha\)</span>增大能导致每篇文档的话题数量变多(关于这点可以参考维基百科上的一张<a href="https://upload.wikimedia.org/wikipedia/commons/5/54/LogDirichletDensity-alpha_0.3_to_alpha_2.0.gif">动图</a>), 所以另一种思路是把<span class="math inline">\(\alpha\)</span>视为超参去调.</li>
<li>假定每个单词的主题<span class="math inline">\(z_{dn}\)</span>与所在文档的主题<span class="math inline">\(\theta_d\)</span>一致, 即<span class="math inline">\(P(z_{dn}\mid\theta_d)=\theta_{dz_{dn}}\)</span>, 写得直观些, 即<span class="math inline">\(P(z_{dn}=t\mid\theta_d)=\theta_{dt}\)</span>, <span class="math inline">\(\theta_{dt}\)</span>表示<span class="math inline">\(\theta_d\)</span>的第<span class="math inline">\(t\)</span>个分量.</li>
<li><span class="math inline">\(T\)</span>个主题与词汇表<span class="math inline">\(w_{set}\)</span>的分布记为<span class="math inline">\(\phi\)</span>, 这里的<span class="math inline">\(\phi\)</span>为<span class="math inline">\(\left|w_{set}\right|\times T\)</span>的矩阵. 即<span class="math inline">\(P(w_{dn}\mid z_{dn})=\varphi_{z_{dn}w_{dn}}\)</span>, 写得直观些, <span class="math inline">\(v\)</span>为不超过<span class="math inline">\(\left|w_{set}\right|\)</span>的正整数, <span class="math inline">\(P(w_{dn}=v\mid z_{dn})=\varphi_{v z_{dn}}\)</span>.</li>
</ul>
<p>若用大小字母表示小写字母的总和, 参数仍简记为小写字母, 我们现有五个关键要素: <span class="math inline">\(W\)</span>为数据集; <span class="math inline">\(Z, \Theta\)</span>为隐变量; <span class="math inline">\(\varphi, \alpha\)</span>为未知参数.</p>
板表示法(plate notation)是一种表示在图形模型中重复的变量的方法, 下面采用板表示法的贝叶斯网络表达上述模型:
<div data-align="center">
<img src="/pictures/bayes in ml/LDA_plate_model_1.png" width="60%"/>
</div>
<p>这里的虚线框右下角的字母表示框中的部分应该重复画字母次, 所以板表示法的好处就很明显:</p>
<ul>
<li>简化图模型,更美观.</li>
<li>同时模型表达式变得一模了然.</li>
</ul>
<p>最后把该LDA模型用较为简练的语言再写一遍(可以对照图去写): <span class="math display">\[P(W, Z, \Theta\mid \varphi, \alpha)=\prod_{d=1}^{D}P(\theta_d)\prod_{n=1}^{N_d}P(z_{dn}\mid\theta_d)P(w_{dn}\mid z_{dn})\]</span>其中:</p>
<ul>
<li><span class="math inline">\(\theta_d\sim Dir(\alpha)=\frac{1}{B(\alpha)}\prod_{t=1}^{T}\theta_{dt}^{\alpha_t-1}\)</span>, 其中<span class="math inline">\(d\in\{1,2,...,D\}\)</span>, <span class="math inline">\(T\)</span>给定.</li>
<li><span class="math inline">\(P(z_{dn}\mid\theta_d)=\theta_{dz_{dn}}\)</span>, 其中<span class="math inline">\(n\in\{1,2,...,N_d\}\)</span>.</li>
<li><span class="math inline">\(P(w_{dn}\mid z_{dn})=\varphi_{w_{dn}z_{dn}}\)</span>, 给定词汇表<span class="math inline">\(w_{set}\)</span>, <span class="math inline">\(w_{dn}\in\{1,2,...,\left|w_{set}\right|\}\)</span>.</li>
</ul>
<p>大小字母表示小写字母的总和, 参数仍简记为小写字母, 总结如下:</p>
<ul>
<li>未知:<span class="math inline">\(Z, \Theta\)</span>为隐变量; <span class="math inline">\(\varphi, \alpha\)</span>未知参数.</li>
<li>已知:<span class="math inline">\(W\)</span>为数据集, <span class="math inline">\(D\)</span>为文档数量, <span class="math inline">\(N_d\)</span>为每篇文档单词数.</li>
<li>超参:主题数<span class="math inline">\(T\)</span>, 词汇表<span class="math inline">\(w_{set}\)</span>(可以取为这些文档出现过的单词组成的集合).</li>
</ul>
<p>这个模型因为属于隐变量模型, 故可用<a href="https://msgsxj.cn/2018/09/02/EM%E7%AE%97%E6%B3%95/">EM算法</a>求解: 第k+1次迭代, 当前参数为<span class="math inline">\(\varphi^k, \alpha^k\)</span>:</p>
<ul>
<li>E步求期望:求<span class="math inline">\(\mathbb{E}_{q^{k+1}}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span>, 其中<span class="math inline">\(q^{k+1}(Z, \Theta)=P(Z, \Theta\mid W, \varphi^k, \alpha^k)\)</span>.</li>
<li>M步求max: <span class="math inline">\((\varphi^{k+1}, \alpha^{k+1})=\arg\max_{(\varphi, \alpha)}\mathbb{E}_{q^{k+1}}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span>.</li>
</ul>
<p>若后验<span class="math inline">\(P(Z, \Theta\mid W, \varphi^k, \alpha^k)\)</span>难以处理, 则可以用一些近似方法取求得期望, 如<a href="https://msgsxj.cn/2018/09/05/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/">变分贝叶斯</a>, <a href="https://msgsxj.cn/2018/09/28/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E6%96%B9%E6%B3%95/">Laplace近似方法</a>, <a href="https://msgsxj.cn/2018/10/02/%E6%9C%9F%E6%9C%9B%E4%BC%A0%E6%92%AD/">期望传播方法</a>, <a href="https://msgsxj.cn/2018/09/18/MCMC/">MCMC方法</a>等</p>
<hr />
<h2 id="lda模型的拓展">LDA模型的拓展</h2>
<h3 id="phi"><span class="math inline">\(\Phi\)</span></h3>
若把未知参数<span class="math inline">\(\varphi\)</span>视为随机变量, 重新记为<span class="math inline">\(\Phi\)</span>, 假定<span class="math inline">\(\Phi_{i\cdot}\sim Dir(\beta_i)\)</span>, 即假定第<span class="math inline">\(i\)</span>个单词的主题分布满足参数为<span class="math inline">\(\beta_i\)</span>的狄利克雷分布(回忆开头狄利克雷分布的特性), 这里假定<span class="math inline">\(i\)</span>的取值集合<span class="math inline">\(w_{set}\)</span>的大小为<span class="math inline">\(S\)</span>, 上述修改后的模型用板表示法表示如下:
<div data-align="center">
<img src="/pictures/bayes in ml/LDA_plate_model_2.png" width="60%"/>
</div>
<p><span class="math display">\[P(W, Z, \Theta, \Phi\mid  \alpha, \beta)=\prod_{d=1}^{D}P(\theta_d)\prod_{n=1}^{N_d}\prod_{i=1}^{S}P(z_{dn}=i\mid\theta_d)P(w_{dn}\mid z_{dn}=i, \Phi_{i\cdot})P(\Phi_{i\cdot})\]</span>与上文的模型相比只是把参数<span class="math inline">\(\varphi\)</span>视为随机变量<span class="math inline">\(\Phi\)</span>, 且<span class="math inline">\(\Phi_{i\cdot}\sim Dir(\beta_i)\)</span>, 其余不变. 总结如下:</p>
<ul>
<li>未知:<span class="math inline">\(Z, \Theta, \Phi\)</span>为隐变量; $ , $为未知参数.</li>
<li>已知:<span class="math inline">\(W\)</span>为数据集, <span class="math inline">\(D\)</span>为文档数量, <span class="math inline">\(N_d\)</span>为每篇文档单词数.</li>
<li>超参:主题数<span class="math inline">\(T\)</span>, 词汇表<span class="math inline">\(w_{set}\)</span>(可以取为这些文档出现过的单词组成的集合).</li>
</ul>
<h3 id="logit-normal-distribution">Logit-normal distribution</h3>
<p><span class="math inline">\(\theta_d\sim Dir(\alpha)\)</span>的一个良好替代是假定<span class="math inline">\(\theta_d\sim \mathcal{P}(N(\mu, \Sigma))\)</span>, 即满足<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution">Logit-normal分布</a>. 逻辑正态分布: 若<span class="math inline">\(Y\sim N(\mu, \Sigma)\)</span>, 则称<span class="math inline">\(X=\mathcal{P}(Y)\)</span>为逻辑正态(Logit-normal)分布, 其中<span class="math inline">\(\mathcal{P}()\)</span>为逻辑函数. 下面来看这两个分布的异同:</p>
<ul>
<li>与狄利克雷分布相比, 逻辑正态分布能捕捉<span class="math inline">\(\theta_d\)</span>之间的相关性, 因而更受欢迎.</li>
<li>虽然任何参数<span class="math inline">\(\mu, \Sigma, \alpha\)</span>的选择都不能让两个的密度函数相等, 但通过最小化<span class="math inline">\(\mathcal{KL}\)</span>散度的方式来让逻辑正态分布去逼近狄利克雷分布的方式, 即最小化<span class="math display">\[\mathcal{KL}(p, q)=\int p(x\mid \alpha)\log\frac{p(x\mid\alpha)}{q(x\mid\mu, \Sigma)}dx\]</span>可以得到关于<span class="math inline">\(\alpha\)</span>的参数<span class="math inline">\(\mu,\Sigma\)</span>在<span class="math inline">\(\mathcal{KL}\)</span>散度意义下的最优选择, 详见<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution">维基百科</a>, 这里要说明的是, 当<span class="math inline">\(\alpha\)</span>的每个分量都趋于无穷, <span class="math inline">\(p(x\mid \alpha)\to q(x\mid\mu, \Sigma)\)</span>, 也就是说<span class="math inline">\(\alpha\)</span>很大时两者可以变得非常靠近.</li>
</ul>
<h3 id="dynamic-topic-models">Dynamic Topic Models</h3>
<p>此外还有动态主题模型, 见<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.2783&amp;rep=rep1&amp;type=pdf">这篇文章</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>隐变量模型</tag>
        <tag>LDA模型</tag>
        <tag>Dirichlet分布</tag>
      </tags>
  </entry>
  <entry>
    <title>MCMC</title>
    <url>/2018/09/18/MCMC/</url>
    <content><![CDATA[<p>前言:本文主要是关于MCMC, 这是一类对概率进行采样的方法, 主要参考了Coursera上国立高等经济大学Advanced Machine Learning系列课程Course3: Bayesian Methods for Machine Learning Week4. <span id="more"></span></p>
<hr />
<p>本文讲一讲Markov chain Monte Carlo(MCMC), MCMC是一类对概率<span class="math inline">\(q(x)\)</span>进行采样的方法, 其本质是构建稳态分布恰为所需分布<span class="math inline">\(q(x)\)</span>的马尔可夫链, 这里主要介绍两种MCMC方法: Gibbs采样和Metropolis-Hastings算法.</p>
<p>首先要明确的是我们期望抽样的目标分布通常是高维分布, 而对高维分布抽样的方法都基于一维分布的抽样, 因此接下来会介绍一些一维抽样方法. 接着我们会分别回顾MCMC的前半和后半, 即马尔可夫链和Monte Carlo蒙特卡洛的相关知识, 最后介绍Gibbs采样和Metropolis-Hastings算法, 并结合两者的优点给出一种能够进行并行计算的抽样方法.</p>
<hr />
<h2 id="准备知识">准备知识</h2>
<h3 id="一维分布的抽样">一维分布的抽样</h3>
<p>一维离散分布的抽样:</p>
<ul>
<li>假设一维离散分布的密度函数为<span class="math display">\[q(x)=q(x_1,...,x_k)=\prod_{i=1}^kp_i^{x_i}, \sum_{i=1}^{k}p_k=1\]</span>其中<span class="math inline">\(x_i\in\{0,1\}, \sum_{i=1}^{k}x_k=1\)</span>. 只需取<span class="math inline">\(y\sim U(0,1)\)</span>, 令<span class="math inline">\(x=e_k\)</span>当<span class="math inline">\(y\in\left[\sum_{i=1}^{k-1}p_i, \sum_{i=1}^{k}p_i\right)\)</span></li>
</ul>
<p>一维连续分布的抽样:</p>
<ul>
<li>假设一维连续分布的密度函数为<span class="math inline">\(q(x)\)</span>, 其对应的分布函数为<span class="math inline">\(F(y)=P(x\le y)=\int_{-\infty}^{y}q(x)dx\)</span>. 若<span class="math inline">\(F^{-1}(y)\)</span>易求, 只需取<span class="math inline">\(y\sim U(0,1)\)</span>, 令<span class="math inline">\(x=F^{-1}(y)\)</span>, 可以得到<span class="math inline">\(x\sim q(x)\)</span>. 证明如下<span class="math display">\[P(x\le a)=P(F^{-1}(y)\le a)=P(y\le F(a))=F(a)\]</span>对a求导可得<span class="math inline">\(P(x=a)=q(a)\)</span></li>
<li>一般<span class="math inline">\(F^{-1}(y)\)</span>难求, 这里介绍一种拒绝采样方法:
<ol type="1">
<li>取<span class="math inline">\(y\sim N(a,b)=r(x)\)</span>, 找到常数<span class="math inline">\(M&gt;0\)</span>, 满足<span class="math inline">\(q(x)\le Mr(x)\)</span></li>
<li>取<span class="math inline">\(z\sim U(0,Mq(y))\)</span>, 令<span class="math inline">\(x=y\)</span>当<span class="math inline">\(z\le q(y)\)</span>时, 否则拒绝<span class="math inline">\(y\)</span></li>
</ol></li>
<li>其他的采样方法如Adaptive rejection sampling、Importance sampling及Sampling-importance-resampling可参考Bishop的PRML的11.1</li>
</ul>
<h3 id="马氏链">马氏链</h3>
<p>马尔可夫过程, 又称马氏过程, 且当其时间状态离散时, 马尔可夫过程又称为马尔可夫链, 又称马氏链.</p>
<ul>
<li>马氏链由结构<span class="math inline">\(G\)</span>和转移概率矩阵<span class="math inline">\(M\)</span>构成. 结构<span class="math inline">\(G\)</span>是一个有向图, 每个节点对应一个状态;转移概率矩阵<span class="math inline">\(M\)</span>定义在节点上, <span class="math inline">\(M_{ij}\)</span>表示第<span class="math inline">\(i\)</span>个状态转移到第<span class="math inline">\(j\)</span>个状态的概率. 我们说由转移概率矩阵<span class="math inline">\(M\)</span>可以构造一个马氏链, 若<span class="math inline">\(M\)</span>满足:
<ol type="1">
<li><span class="math inline">\(\sum_{j=1}M_{ij}=1, \forall i\)</span>, 即<span class="math inline">\(M\)</span>的行和为1</li>
<li>K-C方程(自然满足)</li>
</ol></li>
<li>称<span class="math inline">\(\pi\)</span>为稳态分布, 如果满足如下平稳条件<span class="math inline">\(\pi_j=\sum_i\pi_i M_{ij}\)</span>. 可以说, 一个马氏链从任意状态出发最终都能到达稳态分布, 若能构造稳态分布即为期望分布的马氏链, 我们的目的也就达成了.</li>
</ul>
<p><strong>p.s.</strong> 这里假定本文提到的马氏链均为时间离散且状态空间可数的时齐马氏链. 状态空间可数很好理解, 对于每一台机器来说都有一个机器精度的存在, 所以实数上取值集合实际上是可数的; 这里的时齐可以简单理解为不管从哪个时间点出发, 其转移概率矩阵都是一样的. KC方程说的是对任意的<span class="math inline">\(s\le r\le t\)</span>, 从时间<span class="math inline">\(s\)</span>转移到时间<span class="math inline">\(t\)</span>的转移概率矩阵等于<span class="math inline">\(s\)</span>到<span class="math inline">\(r\)</span>再到<span class="math inline">\(t\)</span>的三维转移张量对<span class="math inline">\(r\)</span>时刻的所有状态求和, 乍一看这是很自然的事情, 而也正因为时齐及状态空间可数, K-C方程才能得以自然满足.</p>
<h3 id="蒙特卡洛">蒙特卡洛</h3>
<p>蒙特卡洛是通过重复随机抽样来获得数值结果的一种计算方法, 假定目标为获得<span class="math inline">\(\mathbb{E}_{q(x)}f(x)\)</span>的估计值, 蒙特卡洛做的就是:</p>
<ol type="1">
<li><span class="math inline">\(x_{s}\sim q(x)\)</span></li>
<li><span class="math inline">\(\mathbb{E}_{q(x)}f(x)\approx \frac{1}{M}\sum_{s=1}^Mf(x_s)\)</span></li>
</ol>
<p>下面举个例子来说明MCMC的用处, 回忆EM算法中的求期望的E步, 可见博文<a href="">EM算法</a></p>
<ol type="1">
<li>在<span class="math inline">\(Q\)</span>中找到<span class="math inline">\(q(T)\approx P(T\mid X, \theta)\)</span></li>
<li><span class="math inline">\(\mathbb{E}_{P(T\mid X, \theta)}\log P(X,T\mid\theta)\approx \mathbb{E}_{q(T)}\log P(X,T\mid\theta)\)</span></li>
</ol>
<p>两者同样是对某期望的近似, 一个自然的想法是把MCMC替换掉EM算法中的E步, 即</p>
<ol type="1">
<li><span class="math inline">\(T_s\sim P(T\mid X, \theta)\)</span></li>
<li><span class="math inline">\(\mathbb{E}_{P(T\mid X, \theta)}\log P(X,T\mid\theta)\approx \sum_{s=1}^M\log P(X,T_s\mid\theta)\)</span></li>
</ol>
<p>我们要做的就是从<span class="math inline">\(P(T\mid X, \theta)\)</span>中抽样, 然后得到期望的估计值. 下面以EM算法为例介绍两种MCMC方法:Gibbs采样和Metropolis-Hastings算法</p>
<hr />
<h2 id="gibbs采样">Gibbs采样</h2>
<p>Gibbs采样的想法是化多维抽样为一维抽样, 假定采样目标分布为<span class="math inline">\(P(T\mid X, \theta)\)</span>, 假定<span class="math inline">\(T=\{T_1,...,T_d\}\)</span>, 第<span class="math inline">\(k\)</span>次采样得到的数据为<span class="math inline">\(T^k=\{t_1^k,...,t_d^k\}\)</span>, 第<span class="math inline">\(k+1\)</span>次采样策略如下<span class="math display">\[\begin{aligned}t_1^{k+1}&amp;\sim P(T_1\mid T_2=t_2^k, T_3=t_3^k,..., T_d=t_d^k, X, \theta) \\\
t_2^{k+1}&amp;\sim P(T_2\mid T_1=t_1^{k+1}, T_3=t_3^k,..., T_d=t_d^k, X, \theta) \\\
&amp;...\\\
t_d^{k+1}&amp;\sim P(T_d\mid T_1=t_1^{k+1}, T_2=t_2^{k+1},..., T_{d-1}=t_{d-1}^{k+1}, X, \theta)\end{aligned}\]</span>首先验证上述策略定义的矩阵<span class="math inline">\(M\)</span>满足转移概率矩阵的条件, 即验证转移阵的行和均为1:<span class="math display">\[\begin{aligned}\forall  i, \sum_j M_{ij}&amp;=\sum_jP(T_1=j_1\mid T_2=i_2, T_3=i_3,..., T_d=i_d, X, \theta) \\\
&amp;\times P(T_2=j_2\mid T_1=j_1, T_3=i_3,..., T_d=i_d, X, \theta) \\\
&amp;...\\\
&amp;\times P(T_d=j_d\mid T_1=j_1, T_2=j_2,..., T_{d-1}=j_{d-1}, X, \theta)\\\
&amp;=\sum_{j_{-d}}P(T_1=j_1\mid T_2=i_2, T_3=i_3,..., T_d=i_d, X, \theta) \\\
&amp;\times P(T_2=j_2\mid T_1=j_1, T_3=i_3,..., T_d=i_d, X, \theta) \\\
&amp;...\\\
&amp;\times \sum_{j_{d}}P(T_d=j_d\mid T_1=j_1, T_2=j_2,..., T_{d-1}=j_{d-1}, X, \theta)\end{aligned}\]</span>容易看到<span class="math inline">\(\sum_{j_{d}}P(T_d=j_d\mid T_1=j_1, T_2=j_2,..., T_{d-1}=j_{d-1}, X, \theta)=1\)</span>, 因而以此类推能得到结论, 所以可以说由上述策略定义的转移阵<span class="math inline">\(M\)</span>能构造出一个概率空间及其上的马氏链.</p>
<p>下面证明我们的目标分布恰为该马氏链的稳态分布, 即验证平稳条件: <span class="math display">\[P(T=j\mid X, \theta)=\sum_i P(T=i\mid X, \theta)M_{ij}\]</span>式子太长这里就不再写出来了, 有兴趣的朋友可以尝试去验证一下:<span class="math display">\[P(T_1=j_1, T_2=j_2,..., T_d=j_d\mid X, \theta)=\sum_i P(T_1=i_1, T_2=i_2, ..., T_d=i_d\mid X, \theta) M_{ij}\]</span>你需要做的就是仿照上面验证行和为1, 将<span class="math inline">\(M_{ij}\)</span>带入验证 ,很容易发现, 右边带有<span class="math inline">\(T_1=i_1\)</span>的只有一项, 因此可以先对<span class="math inline">\(i\)</span>的第一个分量<span class="math inline">\(i_1\)</span>求和, 剩下的以此类推, 不在赘述.</p>
<p>最后由马氏链的性质就可以推出, 在转移足够长的次数<span class="math inline">\(K\)</span>后, 其后的数据<span class="math inline">\(T_K, T_{K+1},...\)</span>恰来自我们目标分布<span class="math inline">\(P(T\mid X, \theta)\)</span></p>
<hr />
<h2 id="metropolis-hastings算法">Metropolis-Hastings算法</h2>
<p>Metropolis-Hastings算法的想法是先让抽样按照随机选取的<span class="math inline">\(Q\)</span>非常随意的进行, 然后再根据稳态分布应该满足的条件去制定一个拒绝策略<span class="math inline">\(A\)</span>. 假定第<span class="math inline">\(k\)</span>次采样数据为<span class="math inline">\(x_k\)</span>, 其第<span class="math inline">\(k+1\)</span>次采样思路如下:</p>
<ul>
<li><span class="math inline">\(x&#39;\sim Q(x^k\to x)\)</span></li>
<li>以<span class="math inline">\(A(x^k\to x&#39;)\)</span>的概率接受<span class="math inline">\(x&#39;\)</span>, 令<span class="math inline">\(x^{k+1}=x&#39;\)</span>;否则令<span class="math inline">\(x^{k+1}=x^k\)</span></li>
</ul>
<p>写成数学表达式即为:<span class="math display">\[\begin{aligned}M_{ij}&amp;= Q_{ij}A_{ij}, j\neq i  \\\
M_{ii}&amp;= Q_{ii}A_{ii}+\sum_{j\neq i}Q_{ij}(1-A_{ij})\end{aligned}\]</span>下面根据<span class="math inline">\(M\)</span>应该满足的性质来构造拒绝策略<span class="math inline">\(A\)</span>, 首先应当满足<span class="math inline">\(\sum_j M_{ij}=1\)</span> <span class="math display">\[\begin{aligned}\sum_j M_{ij}&amp;= \sum_{j\neq i}Q_{ij}A_{ij}+Q_{ii}A_{ii}+\sum_{j\neq i}Q_{ij}(1-A_{ij})\\\
&amp;= \sum_{j\neq i}Q_{ij}+Q_{ii}A_{ii}\end{aligned}\]</span>若<span class="math inline">\(A_{ii}=1\)</span>, 那么上式<span class="math inline">\(=1\)</span>, 即M为转移概率阵. <span class="math inline">\(\pi_j M_{ji}=\pi_i M_{ij}\)</span>被称为是细平稳条件, 因为两边对<span class="math inline">\(i\)</span>求和能推得平稳条件<span class="math display">\[\sum_i\pi_i M_{ij}=\sum_i\pi_j M_{ji}=\pi_j\]</span></p>
<p>若直接假定细平稳条件成立, 当<span class="math inline">\(i\neq j\)</span>时即为<span class="math display">\[\pi_j Q_{ji}A_{ji}=\pi_i Q_{ij}A_{ij}\]</span>变形得<span class="math inline">\(\frac{A_{ij}}{A_{ji}}=\frac{\pi_j Q_{ji}}{\pi_i Q_{ij}}, i\neq j\)</span>. 最后令<span class="math inline">\(A_{ij}=\min\{1, \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}}\}, i\neq j\)</span>即可. 事实上, <span class="math inline">\(A_{ii}=1\)</span>恰能统一进上式故<span class="math display">\[A_{ij}=\min\left\{1, \frac{\pi_j Q_{ji}}{\pi_i Q_{ij}}\right\}\]</span></p>
<p>举个例子, <span class="math inline">\(x&#39;\sim Q(x\to x&#39;)=N(x,1)\)</span>, 此时拒绝策略为<span class="math display">\[A(x\to x&#39;)=\min\left\{1, \frac{\pi(x&#39;) }{\pi(x)}\right\}\]</span></p>
<hr />
<h2 id="gibbsmh">Gibbs+MH</h2>
<p>若Metropolis-Hastings算法的Q直接选用稍作修改版的Gibbs采样策略, 这样的修改使得并行计算能得以进行<span class="math display">\[\begin{aligned}t_1^{k+1}&amp;\sim P(T_1\mid T_2=t_2^k, T_3=t_3^k,..., T_d=t_d^k, X, \theta) \\\
t_2^{k+1}&amp;\sim P(T_2\mid T_1=t_1^{k}, T_3=t_3^k,..., T_d=t_d^k, X, \theta) \\\
&amp;......\\\
t_d^{k+1}&amp;\sim P(T_d\mid T_1=t_1^{k}, T_2=t_2^{k},..., T_{d-1}=t_{d-1}^{k}, X, \theta)\end{aligned}\]</span>此时并不能保证此种策略能收敛到目标分布<span class="math inline">\(P(T\mid X, \theta)\)</span>, 但加上拒绝策略就能保证<span class="math display">\[A(T\to T&#39;)=\min\left\{1, \frac{P(T&#39;\mid X, \theta) Q(T&#39;\to T)}{P(T\mid X, \theta) Q(T\to T&#39;)}\right\}\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>MCMC</tag>
        <tag>Gibbs</tag>
        <tag>Metropolis–Hastings算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Python食用记录</title>
    <url>/2018/04/19/Python%E9%A3%9F%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>前言:记录使用Python中一些有意思的事情. <span id="more"></span></p>
<h2 id="python读取.data数据占用的内存">Python读取.data数据占用的内存</h2>
<h3 id="记录">2018/4/19记录</h3>
<p>昨天是腾讯广告算法大赛第一天,本咸鱼本着试水的心态下载了数据集,数据大小如下: <img src="/pictures/Python/%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B0%8F.png" /> 其中最大的文件userFeature.data有4075MB,用Sublime Text3打开,发现其占用了7914MB大小的内存,这和2倍大小的内存原则完全一致: <img src="/pictures/Python/%E6%8E%A7%E5%88%B6%E9%9D%A2%E6%9D%BF.png" /> 尝试执行以下Python代码读取该.data文件,代码参考了<a href="https://github.com/YouChouNoBB/2018-tencent-ad-competition-baseline/blob/master/baseline.py">这个baseline</a>: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdata</span>():</span></span><br><span class="line">    time1 = time.time()</span><br><span class="line">    homedir = os.getcwd()</span><br><span class="line">    adFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\adFeature.csv&#x27;</span>) <span class="comment">#7KB</span></span><br><span class="line">    test1 = pd.read_csv(homedir + <span class="string">&#x27;\\data\\test1.csv&#x27;</span>)<span class="comment"># 29MB</span></span><br><span class="line">    train = pd.read_csv(homedir + <span class="string">&#x27;\\data\\train.csv&#x27;</span>) <span class="comment"># 140MB</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_11.csv&#x27;</span>):</span><br><span class="line">        userFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_0.csv&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;read one million&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">            userFeature = userFeature.append(pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_&#x27;</span> + <span class="built_in">str</span>(i+<span class="number">1</span>) + <span class="string">&#x27;.csv&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;read one million&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        userFeature_list = []</span><br><span class="line">        userFeature = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(homedir + <span class="string">&#x27;\\data\\userFeature.data&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f: <span class="comment"># 4173MB</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f):</span><br><span class="line">                line = line.strip().split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">                userFeature_dict = &#123;&#125;</span><br><span class="line">                <span class="keyword">for</span> each <span class="keyword">in</span> line:</span><br><span class="line">                    each_list = each.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                    userFeature_dict[each_list[<span class="number">0</span>]] = <span class="string">&#x27; &#x27;</span>.join(each_list[<span class="number">1</span>:])</span><br><span class="line">                userFeature_list.append(userFeature_dict)</span><br><span class="line">                <span class="keyword">if</span> i % <span class="number">100000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">            userFeature_temp = pd.DataFrame(userFeature_list[i*<span class="number">1000000</span>:(i+<span class="number">1</span>)*<span class="number">1000000</span>])</span><br><span class="line">            userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;save one million&quot;</span>)</span><br><span class="line">            <span class="keyword">del</span> userFeature_temp</span><br><span class="line">            gc.collect()</span><br><span class="line">        userFeature_temp = pd.DataFrame(userFeature_list[<span class="number">11000000</span>:])</span><br><span class="line">        userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_11.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;save as csv successfully&quot;</span>)</span><br><span class="line">        <span class="keyword">del</span> userFeature_temp</span><br><span class="line">        gc.collect()</span><br><span class="line">    time2 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;get data cost&quot;</span>,time2 - time1,<span class="string">&quot;s&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> adFeature, test1, train, userFeature</span><br><span class="line"></span><br><span class="line">adFeature, test1, train, userFeature = getdata()</span><br></pre></td></tr></table></figure> 第一次执行上述代码,即逐行读取.data数据,每100w条数据转存DataFrame性数据,执行过程中到440w左右的数据量会导致内存占用率达到95%,12G左右的量时就不会上涨,猜测是内存保护机制,并且注意到固态盘C剩余空间慢慢减少. 执行到with open()函数结束,将userFeature_list转为DataFrame型数据之前,此时内存占用12308MB约为12G: <img src="/pictures/Python/%E6%8E%A7%E5%88%B6%E9%9D%A2%E6%9D%BF2.png" /> 外加征用了固态盘C的13.3G的容量,(征用前剩26.2G,征用后剩12.9G): <img src="/pictures/Python/C_1.png" /> <img src="/pictures/Python/C_2.png" /> 所以4G大小的数据用Python导入一共会产生25.3G的内存占用.(Why? :) ) 执行以下代码会返回此时局部变量占用空间大小:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">locals</span>().keys()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">locals</span>().keys():</span><br><span class="line">    <span class="built_in">print</span>(x, getsizeof(<span class="built_in">locals</span>()[x]))</span><br></pre></td></tr></table></figure>
<p>发现占用最大的就是userFeature_list,大概10.9G的样子.(So why 25.3G? :) )此时若是把想把userFeature_list转为DataFrame型数据则会抛出MemoryError,故只能分批次转DataFrame并保存为csv,最后在执行一遍会把csv文件读入:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    userFeature_temp = pd.DataFrame(userFeature_list[i*<span class="number">1000000</span>:(i+<span class="number">1</span>)*<span class="number">1000000</span>])</span><br><span class="line">    userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_&#x27;</span> + <span class="built_in">str</span>(i) + <span class="string">&#x27;.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;save one million&quot;</span>)</span><br><span class="line">    <span class="keyword">del</span> userFeature_temp</span><br><span class="line">    gc.collect()</span><br><span class="line">userFeature_temp = pd.DataFrame(userFeature_list[<span class="number">11000000</span>:])</span><br><span class="line">userFeature_temp.to_csv(homedir + <span class="string">&#x27;\\data\\userFeature_csv\\userFeature_11.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;save as csv successfully&quot;</span>)</span><br><span class="line"><span class="keyword">del</span> userFeature_temp</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure>
<p>第一次执行结果图如下: <img src="/pictures/Python/firsttime.png" /> 第二次执行因为已经有了csv文件,所以直接读入csv就好,执行结果图如下: <img src="/pictures/Python/second_time.png" /></p>
<h3 id="记录-1">2018/5/25记录</h3>
<p>此时初赛已经结束了,最好成绩为0.745764,没能进复赛. 初赛阶段都结束时才发现一种流式逐行读取数据,填充缺失数据为-1,逐行保存为csv文件的方法,特此补充:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdata</span>():</span></span><br><span class="line">    time1 = time.time()</span><br><span class="line">    homedir = os.getcwd()</span><br><span class="line">    adFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\adFeature.csv&#x27;</span>) <span class="comment">#7KB</span></span><br><span class="line">    test1 = pd.read_csv(homedir + <span class="string">&#x27;\\data\\test1.csv&#x27;</span>)<span class="comment"># 29MB</span></span><br><span class="line">    train = pd.read_csv(homedir + <span class="string">&#x27;\\data\\train.csv&#x27;</span>) <span class="comment"># 140MB</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(homedir + <span class="string">&#x27;\\data\\userFeature.csv&#x27;</span>):</span><br><span class="line">        userFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature.csv&#x27;</span>, sep=<span class="string">&#x27;delimiter&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        userFeature = []</span><br><span class="line">        f_csv = <span class="built_in">open</span>(homedir + <span class="string">&#x27;\\data\\userFeature.csv&#x27;</span>, <span class="string">&quot;w&quot;</span>)</span><br><span class="line">        name = <span class="string">&#x27;uid,age,gender,marriageStatus,education,consumptionAbility,LBS,interest1,interest2,interest5,kw1,kw2,topic1,topic2,ct,os,carrier&#x27;</span></span><br><span class="line">        f_csv.write(name)</span><br><span class="line">        names = name.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(homedir + <span class="string">&#x27;\\data\\userFeature.data&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f_data: <span class="comment"># 4173MB</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f_data):</span><br><span class="line">                line = line.strip().split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">                userFeature_dict = <span class="built_in">dict</span>.fromkeys(names)</span><br><span class="line">                <span class="keyword">for</span> each <span class="keyword">in</span> line:</span><br><span class="line">                    each_list = each.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                    userFeature_dict[each_list[<span class="number">0</span>]] = <span class="string">&#x27; &#x27;</span>.join(each_list[<span class="number">1</span>:])</span><br><span class="line">                value = []</span><br><span class="line">                <span class="keyword">for</span> (k, v) <span class="keyword">in</span> userFeature_dict.items():</span><br><span class="line">                    <span class="keyword">if</span> v == <span class="literal">None</span>:</span><br><span class="line">                        value.append(<span class="string">&#x27;-1&#x27;</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        value.append(v)</span><br><span class="line">                value = <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;,&#x27;</span>.join(value)</span><br><span class="line">                <span class="keyword">if</span> i % <span class="number">100000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(i)</span><br><span class="line">                f_csv.write(value)</span><br><span class="line">        f_csv.close()</span><br><span class="line">        userFeature = pd.read_csv(homedir + <span class="string">&#x27;\\data\\userFeature.csv&#x27;</span>, sep=<span class="string">&#x27;delimiter&#x27;</span>)</span><br><span class="line">    time2 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;get data cost&quot;</span>,time2 - time1,<span class="string">&quot;s&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> adFeature, test1, train, userFeature</span><br><span class="line"></span><br><span class="line">adFeature, test1, train, userFeature = getdata()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>SciPy中的经典优化算法</title>
    <url>/2019/02/20/SciPy%E4%B8%AD%E7%9A%84%E7%BB%8F%E5%85%B8%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>前言: 本文的初衷是试图用尽量简明形象的语言说一说<code>scipy.optimize.minimize</code>函数涉及到的经典优化算法的特点, 方便速查与快速回忆; 严格理论的部分可见<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize">官方文档</a>Notes部分的内容, 其对每一种方法都给出了参考论文, 而笔者关于这些算法的理解大多来自于Jorge Nocedal的Numerical Optimization. <span id="more"></span></p>
<p>首先来看<code>scipy.optimize</code>这个包的<a href="https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html">官方文档</a>, 右侧的目录告诉我们这个包大致分为6块内容:</p>
<ul>
<li>多元函数无约束优化</li>
<li>多元函数约束优化</li>
<li>最小二乘优化</li>
<li>一元函数优化</li>
<li>自定义优化器</li>
<li>寻根问题</li>
</ul>
<h2 id="minimize函数">minimize函数</h2>
<p>可以看到<code>scipy.optimize.minimize</code>函数对应于第一第二块内容, 具体<code>minimize</code>的<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize">文档</a>, 可以看到<code>method</code>参数中罗列着以下14种方法:</p>
<ul>
<li>Nelder-Mead</li>
<li>Powell</li>
<li>CG</li>
<li>BFGS</li>
<li>Newton-CG</li>
<li>L-BFGS-B</li>
<li>TNC</li>
<li>COBYLA</li>
<li>SLSQP</li>
<li>trust-constr</li>
<li>dogleg</li>
<li>trust-ncg</li>
<li>trust-exact</li>
<li>trust-krylov</li>
</ul>
<p>优化是对目标函数的进行迭代求最小值的过程, 若优化过程只需求得目标函数的在迭代点处函数值, 那么这类方法称为是<strong>无导数优化</strong>; 若需要求得目标函数的一阶信息(梯度向量值), 那么这类方法称为是<strong>一阶方法</strong>; 若需要求得二阶信息(Hessian阵), 那么这类方法称为是<strong>二阶方法</strong>. 显然三类方法的计算量的越来越大.</p>
<p>上面罗列的算法中<strong>无导数优化</strong>方法有: Nelder-Mead, Powell, COBYLA; <strong>一阶方法</strong>有: CG, BFGS, L-BFGS-B, TNC, SLSQP; <strong>二阶方法</strong>有: Newton-CG, trust-constr, dogleg, trust-ncg, trust-exact, trust-krylov; 特别的, 二阶方法中的trust-constr, trust-ncg, trust-krylov若能提供Hessian右乘任意向量p的结果(即关于迭代点x和任意向量p的函数), 这样的函数一般来说计算量会小于直接计算Hessian阵, 因此实际上计算量会介于一阶方法与二阶方法之间, 这里笔者称之为<strong>一阶半方法</strong>, 这类方法适合大规模问题.</p>
<p>下面按优化问题的不同分无约束优化, 有界约束优化与约束优化分别来看:</p>
<h2 id="无约束优化">无约束优化</h2>
<hr />
<h3 id="nelder-mead方法">Nelder-Mead方法</h3>
<p>Nelder-Mead方法是<strong>无导数优化方法</strong>, 其保持了对<span class="math inline">\(\mathbb{R}^n\)</span>中的<span class="math inline">\(n+1\)</span>个点的函数值追踪, 且这些点的凸包构成了一个<strong>非退化的单纯形</strong>. 比方说目标函数为<span class="math inline">\(\mathbb{R}^2\)</span>中的函数, Nelder-Mead方法就保持了对<span class="math inline">\(3\)</span>个点的函数值追踪, 非退化指的是这三者不能共线.</p>
<p>下面以<a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">维基百科</a>上的关于Rosenbrock函数的动图为例, 记手中的<span class="math inline">\(n+1\)</span>个点为<span class="math inline">\(\{x_1,...,x_{n+1}\}\)</span>, 并将它们按函数值大小排序<span class="math inline">\(f(x_1)\le\cdots\le f(x_{n+1})\)</span>, <span class="math inline">\(n\)</span>个最好的点的形心为<span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i\)</span>此时<span class="math inline">\(\bar{x}\)</span>与最差的点<span class="math inline">\(x_{n+1}\)</span>之间的连线即为: <span class="math display">\[\bar{x}(t)=\bar{x}+t(x_{n+1}-\bar{x})\]</span>Nelder-Mead方法出于<strong>尽量远离函数值最高的点的想法</strong>, 去观察<span class="math inline">\(\bar{x}(-1)\)</span>的值</p>
<ul>
<li>若<span class="math inline">\(f(x_1)\le f(\bar{x}(-1))&lt;f(x_n)\)</span>, 则用<span class="math inline">\(\bar{x}(-1)\)</span>取代<span class="math inline">\(x_{n+1}\)</span>;</li>
<li>若<span class="math inline">\(f(\bar{x}(-1))&lt;f(x_1)\)</span>, 继续观察<span class="math inline">\(\bar{x}(-2)\)</span>是否会带来更进一步下降, 择优选择取代<span class="math inline">\(x_{n+1}\)</span>;</li>
<li>若<span class="math inline">\(f(x_n)\le f(\bar{x}(-1))&lt;f(x_{n+1})\)</span>, 先后观察<span class="math inline">\(\bar{x}(-\frac{1}{2})\)</span>与<span class="math inline">\(\bar{x}(\frac{1}{2})\)</span>是否会带来更进一步下降, 注意到先后的顺序也体现了尽量远离<span class="math inline">\(x_{n+1}\)</span>;</li>
<li>最后若上述条件均不满足, 则整体向函数值最小的点<span class="math inline">\(x_1\)</span>挪一半.</li>
</ul>
上述算法可见Numerical Optimization一书9.5节, 对应到下图中的第一步迭代, 不难看出第一次迭代点的选择为<span class="math inline">\(\bar{x}(\frac{1}{2})\)</span>.
<div data-align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/e/e4/Nelder-Mead_Rosenbrock.gif" width="420">
</div>
<hr />
<h3 id="powell方法">Powell方法</h3>
<p><a href="https://en.wikipedia.org/wiki/Powell%27s_method">Powell方法</a>是共轭方向法的一种变体, 先来看什么是共轭方向法, <strong>共轭方向法</strong>是<strong>给定一共轭方向的集合</strong><span class="math inline">\(\{p_1....,p_n\}\)</span>, 对集合中的方向依次进一维巡查的方法, 因此显然是<strong>无导数优化方法</strong>.</p>
<p>以最简单的二次函数<span class="math inline">\(f(x)=\frac{1}{2}x^TAx-b^Tx\)</span>为例, 其中<span class="math inline">\(A\)</span>为<span class="math inline">\(n\)</span>阶对称正定方阵, 此时共轭的概念有了具体的要求: <span class="math inline">\(p_i^TAp_j=0\)</span>. 若<span class="math inline">\(A\)</span>恰为对角阵, 如左图所示, 共轭方向集合可取为坐标轴方向<span class="math inline">\(\{e_1, e_2\}\)</span>; 若<span class="math inline">\(A\)</span>不为对角阵, 如右图所示, 此时<span class="math inline">\(\{e_1, e_2\}\)</span>不能作为一组共轭方向, 图中的<strong>红线</strong>和<strong>蓝线</strong>则是给出了两个不同的共轭方向集合.</p>
<p>值得注意的是, 此时<strong>至多</strong><span class="math inline">\(n\)</span>步共轭方向法会收敛到最小值点<span class="math inline">\(x^*\)</span>, 证明可见Numerical Optimization一书定理5.1</p>
<p><img src="/pictures/scipy_optimization/1.png" /></p>
<p>回到Powell方法, 仍然以上述二次函数为例, 其与标准共轭方向法唯一的不同在于<strong>迭代地得到共轭方向的集合</strong>, 且共轭方向的集合的初始化可简单取为<span class="math inline">\(\{e_1, e_2\}\)</span>. 以右图为例,</p>
<ul>
<li>第一步从<span class="math inline">\(x_0\)</span>出发沿着<span class="math inline">\(e_1\)</span>做一维巡查得到<span class="math inline">\(x_1\)</span>;</li>
<li>第二步先分别对<span class="math inline">\(\{e_2,e_1\}\)</span>做一维巡查得到<span class="math inline">\(x_2,x_3\)</span>, 由此得到真巡查方向<span class="math inline">\(x_3-x_1\)</span>, 从<span class="math inline">\(x_1\)</span>出发沿着<span class="math inline">\(x_3-x_1\)</span>做一维巡查就能直接到达<span class="math inline">\(x^*\)</span>(不难验证<span class="math inline">\(x_3-x_1\)</span>与<span class="math inline">\(e_1\)</span>关于<span class="math inline">\(A\)</span>共轭), 且此时共轭方向的集合更新为<span class="math inline">\(\{e_1, x_3-x_1\}\)</span>.</li>
</ul>
<p>容易看到Powell方法不涉及一阶信息因此为<strong>无导数优化方法</strong>. 上述括号内的性质称为是<strong>并行子空间性质</strong>: 记函数<span class="math inline">\(f(x)\)</span>在两条平行线<span class="math inline">\(l_1(\alpha)=x_0+\alpha e_1\)</span>, <span class="math inline">\(l_2(\alpha)=x_2+\alpha e_1\)</span>上的最小值点分别为<span class="math inline">\(x_1, x_3\)</span>, 那么<span class="math inline">\(x_3-x_1\)</span>与<span class="math inline">\(e_1\)</span>关于<span class="math inline">\(A\)</span>共轭. 该性质也容易推广到<span class="math inline">\(n\)</span>维情形.</p>
<p>上述性质可参考Numerical Optimization一书9.4节, 最后要注意的是对于一般的函数只需将精确的一维巡查改为<strong>非精确搜索</strong>.</p>
<hr />
<h3 id="cg方法">CG方法</h3>
<p>CG为Conjugate Gradient的缩写, 即<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">共轭梯度法</a>, 这是利用一组共轭方向求解线性方程组<span class="math inline">\(Ax=b\)</span>的算法, 其中<span class="math inline">\(A\)</span>为<span class="math inline">\(n\)</span>阶对称正定方阵, 而共轭方向一般通过构造一个关于<span class="math inline">\(A\)</span>的Krylov子空间来得到, 且为一阶半方法, 详见Numerical Optimization一书算法5.1. 不难发现线性方程组的求解等价于最小化二次函数<span class="math inline">\(f(x)=\frac{1}{2}x^TAx-b^Tx\)</span>. 事实上, CG方法也可用于一般的优化问题, 其与上述Powell方法类似, <strong>共轭方向由迭代更新得到</strong>, 唯一的不同在于CG方法用了<strong>一阶甚至是二阶信息</strong>来更新共轭方向(<strong>补充一句, 对于一般的非二次函数并没有共轭一说, 这里可以理解为沿用共轭一词</strong>). 以FR(Fletcher-Reeves)方法为例: <span class="math display">\[\beta_{k+1}^{FR}=\nabla f_{k+1}^T\nabla f_{k+1}/\nabla f_{k}^T\nabla f_{k}, p_{k+1}=-\nabla f_{k+1}+\beta_{k+1}^{FR}p_k\]</span>其有许多变体, 具体到<code>SciPy</code>中的这里CG方法指的是PR(Polak-Ribiere)方法:<span class="math display">\[\beta_{k+1}^{PR}=\nabla f_{k+1}^T(\nabla f_{k+1}-\nabla f_{k})/\nabla f_{k}^T\nabla f_{k}, p_{k+1}=-\nabla f_{k+1}+\beta_{k+1}^{FR}p_k\]</span>此外还有PR+方法, HS方法, FR-PR方法等可见Numerical Optimization一书5.2节. 且这里除了HS方法是<strong>二阶方法</strong>, 其余涉及的均为<strong>一阶方法</strong>.</p>
<p>上述<strong>初值<span class="math inline">\(p_0\)</span>可直接取为负梯度方向</strong>, 另外值得注意的是, 这里关于<span class="math inline">\(p_k\)</span>的一维巡查方法一般需要满足<a href="https://en.wikipedia.org/wiki/Wolfe_conditions#Strong_Wolfe_condition_on_curvature">强Wolfe条件</a>, 且<span class="math inline">\(c_2&lt;\frac{1}{2}\)</span>.</p>
<hr />
<h3 id="bfgs方法">BFGS方法</h3>
<p><a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS</a>作为一种的<strong>拟牛顿方法</strong>, 其不直接计算Hessian阵, 而是<strong>用梯度信息近似计算Hessian阵的逆</strong>并迭代更新, 因此是<strong>一阶方法</strong>, 其更新公式如下:<span class="math display">\[H_{k+1}=(I-\rho_ks_ky_k^T)H_k(I-\rho_ky_ks_k^T)+\rho_ks_ks_k^T\]</span>其中<span class="math inline">\(s_k=x_{k+1}-x_k, y_k=\nabla f_{k+1}-\nabla f_k, \rho_k=1/y_k^Ts_k&gt;0\)</span>. 由于<span class="math inline">\(H_{k+1}\)</span>直接为Hessian阵的<strong>逆</strong>的近似, 因此搜索方向为<span class="math inline">\(p_{k+1}=-H_{k+1}\nabla f_{k+1}\)</span>.</p>
<p>DFP方法与BFGS方法唯一的不同在于<strong>迭代更新Hessian阵本身</strong>而不是逆, 但由于仍然需要求逆操作, 因此计算量会比BFGS大上一圈. DFP方法的迭代公式可参考Numerical Optimization一书6.1节, 另一种的常见拟牛顿方法SR1可参考Numerical Optimization一书6.2节.</p>
<p>最后值得注意的是, BFGS常用<a href="https://en.wikipedia.org/wiki/Wolfe_conditions">Wolfe条件</a>做为巡查条件.</p>
<hr />
<h3 id="newton-cg方法">Newton-CG方法</h3>
<p>Newton-CG方法是一种<a href="https://en.m.wikipedia.org/wiki/Truncated_Newton_method">截断牛顿方法</a>, 而截断牛顿方法则是近似去解牛顿方程来得到巡查方向的思路, 解牛顿方程即为了获得牛顿方向, 而截断意味着近似解方程要在截断的迭代次数内, 因而是近似.</p>
<p>选用CG方法来近似求解牛顿方程(<span class="math inline">\(\nabla^2f_k p_k+\nabla f_k=0\)</span>), 我们就得到了Newton-CG方法, 由于要求得到<span class="math inline">\(\nabla^2 f_k\)</span>, 因此Newton-CG方法为<strong>二阶方法</strong>.</p>
<p>关于算法的更多细节可见Numerical Optimization一书算法7.1. 另外通过观察算法不难发现, 若能提供Hseeian-vector products函数, 那么Newton-CG方法此时就是<strong>一阶半方法</strong>.</p>
<p>Newton-CG方法的主要计算量在CG迭代部分, preconditioning可加速CG迭代过程, 见Numerical Optimization一书page118.</p>
<hr />
<h3 id="dogleg法">Dogleg法</h3>
<p>狗腿法是一种<a href="https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods">信赖域方法</a>, 简单介绍信赖域方法, 与线搜索方法先确定方向后确定步长不同的是, 信赖域方法在<strong>确定方向前首先给定一个信赖域半径</strong><span class="math inline">\(\Delta\)</span>, 要求搜索方向在该信赖域内.</p>
<p>涉及信赖域方法就不得不谈信赖域子问题: <span class="math display">\[\min_{p}m_k(p)=f_k+\nabla f_k^Tp_k+\frac{1}{2}p^TB_kp\ \ \operatorname{ s.t. }\Vert p\Vert\le\Delta\]</span>且一般的, 这里的<span class="math inline">\(B_k\)</span><strong>仅仅要求对称, 一致有界</strong>. 关于这点可见Numerical Optimization一书page68; 但具体到这里的狗腿法仍然需要加上<strong>正定性</strong>的要求.</p>
<p>回过头来看狗腿法, 狗腿法实际上是<strong>对信赖域子问题最优解曲线的近似</strong>, 从图中可以看到, 第一段是沿着负梯度方向到达柯西点, 这里的柯西点指的是<strong>沿着负梯度方向极小化信赖域子问题</strong>得到的极小值点; 接着像狗腿那样拐弯, 直奔<strong>信赖域子问题的最优解</strong>而去, 因此信赖域半径的作用就是决定了上述过程能跑多远.</p>
<div data-align="center">
<img src="/pictures/scipy_optimization/2.png" width="420">
</div>
<hr />
<h3 id="trust-ncg方法">Trust-ncg方法</h3>
<p>信赖域Newton-CG方法与Newton-CG方法的区别在于不是直接用CG方法解牛顿方程(<span class="math inline">\(\nabla^2f_k p_k+\nabla f_k=0\)</span>), 而是<strong>解信赖域子问题</strong> <span class="math display">\[\min_{p}m_k(p)=f_k+\nabla f_k^Tp_k+\frac{1}{2}p^TB_kp\ \ \operatorname{ s.t. }\Vert p\Vert\le\Delta\]</span> 这里<span class="math inline">\(B_k=\nabla^2 f_k\)</span>. 另外步长可<strong>直接设为1</strong>而不需要做一维巡查.</p>
<p>与Newton-CG方法类似, 显然Trust-ncg法本身是<strong>二阶方法</strong>, 但若能提供Hseeian-vector products函数, Trust-ncg方法也是<strong>一阶半方法</strong>. 算法细节可见Numerical Optimization一书算法7.2.</p>
<p>preconditioning可加速CG迭代过程, 见Numerical Optimization一书page118.</p>
<hr />
<h3 id="trust-krylov方法">Trust-krylov方法</h3>
<p>其实应该叫Trust-Lanczos方法, <a href="https://en.wikipedia.org/wiki/Lanczos_algorithm">Lanczos方法</a>可以用来三对角化矩阵, 同时构造出了一个Krylov子空间. 上述过程的应用之一就是用来解线性方程<span class="math inline">\(Ax=b\)</span>, 因此也可解牛顿方程, 参见矩阵计算第三版9.3节.</p>
<p>与Trust-ncg方法相比, Trust-krylov方法只是将CG方法换成了<strong>Lanczos方法</strong>, 且本身仍为<strong>二阶方法</strong>, 同样若能提供Hseeian-vector products函数, Trust-krylov方法也是<strong>一阶半方法</strong>.</p>
<hr />
<h3 id="trust-exact方法">Trust-exact方法</h3>
<p>exact指的是信赖域子问题 <span class="math display">\[\min_{p}m_k(p)=f_k+\nabla f_k^Tp_k+\frac{1}{2}p^TB_kp\ \ \operatorname{ s.t. }\Vert p\Vert\le\Delta\]</span>中的<span class="math inline">\(B_k=\nabla^2 f_k\)</span>再进行直接求解. 形式上与Trust-ncg方法、Trust-krylov方法非常类似, 但计算量显然要大一些, 因此只适合小规模问题.</p>
<p>显然Trust-exact方法为<strong>二阶方法</strong>.</p>
<hr />
<h2 id="有界约束优化">有界约束优化</h2>
<h3 id="l-bfgs-b方法">L-BFGS-B方法</h3>
<p>To be continued...</p>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SciPy</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>Trade-Off in Machine Learning</title>
    <url>/2022/05/07/Trade-Off%20in%20Machine%20Learning/</url>
    <content><![CDATA[<p>本文总结了笔者在机器学习中碰到的各种Trade-Off</p>
<span id="more"></span>
<h2 id="模型复杂度的trade-off">模型复杂度的Trade-Off</h2>
<p><img src="/pictures/capacity.png" /></p>
<ul>
<li>在训练集给定的场景(预定给定一个较大的训练数据集应该是最常见的), 模型的复杂度(capacity)要适中选择. (<strong>p.s.</strong>具体到二分类问题, 最常见模型的复杂度为拉德马赫复杂度<a href="https://msgsxj.cn/2018/11/09/%E6%8B%89%E5%BE%B7%E9%A9%AC%E8%B5%AB%E5%A4%8D%E6%9D%82%E5%BA%A6/">Rademacher complexity</a>, 虽然已经有一些依赖于训练集&amp;复杂度的数据相关界去约束住泛化误差(Generalization error), 但仍然缺少更一般的、更容易计算的模型复杂度。)</li>
<li>过大的模型的复杂度, 容易使得训练好的模型在训练集上过拟合, 对应high bias &amp; low Variance的情况.</li>
<li>过大的模型的复杂度, 容易使得训练好的模型在训练集上欠拟合, 对应low bias &amp; high Variance的情况.</li>
</ul>
<h2 id="验证集大小的trade-off">验证集大小的Trade-Off</h2>
<ul>
<li>为了衡量泛化误差, 最常见的做法是取一个与训练集同分布、相对较小的数据比例(从以前的20%到现在的1%) 作为验证集, 来粗略的评估泛化误差, 从而根据泛化误差的最小值来给出结束训练的建议.</li>
<li>过大的验证集带来可用训练集的缩小与额外计算量的增大.</li>
<li>过小的验证集容易使得验证集损失的波动, 从而降低了结束训练建议的可靠度.</li>
</ul>
<h2 id="损失函数的trade-off">损失函数的Trade-Off</h2>
<ul>
<li>损失函数的往往在人们用的最多的那几个loss中选取, 但未必是那个ground truth.(<strong>p.s.</strong> 以语音领域的信号处理为例, 信号处理会服务于下游的唤醒识别, 因此信号处理的AI模型的真正目标往往是唤醒识别率)</li>
<li>损失函数的选取越靠近ground truth, 往往越难以去执行最优化算法. (<strong>p.s.</strong> 直接选择唤醒识别率作为损失函数必然是难以优化)</li>
<li>损失函数的选取越远离ground truth, 训练之后模型越不容易达到我们想要的模型效果. (<strong>p.s.</strong> 选择语音领域更为流行的sisnr、mse损失, 会使得最优化算法容易执行, 但唤醒识别的最好的模型需要通过手动测唤醒识别挑选得到)</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Trade-Off</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu16.04下便捷安装tensorflow</title>
    <url>/2018/12/23/Ubuntu1604%E4%B8%8B%E4%BE%BF%E6%8D%B7%E5%AE%89%E8%A3%85tensorflow/</url>
    <content><![CDATA[<p>前言: 这篇博文记载了Ubuntu16.04下便捷安装tensorflow的方法. <span id="more"></span> 摸索了几天, 发现一种Ubuntu下便捷安装tensorflow的方法, 特此分享. 过程大致分为2步:</p>
<ol type="1">
<li>安装显卡驱动</li>
<li>安装anaconda, 并conda一个新环境, 装入tensorflow</li>
</ol>
<h3 id="安装显卡驱动">安装显卡驱动</h3>
ubuntu下最方便的安装显卡驱动的方式如下图所示, 系统设置-软件和更新-附加驱动-nvidia410-应用更改
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/1.png" width="80%" />
</div>
安装完成后需要重启一次, 不出意外打开终端输入<code>nvidia-smi</code>就能看到下图:
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/2.png" width="80%" />
</div>
<p>可以看到<code>cuda 10.0</code>(安装驱动会自动安装cuda?), 1066的显卡型号, 320MiB的显存占用, 43度的显卡温度, 9W的功耗等信息.</p>
<h3 id="安装tensorflow">安装tensorflow</h3>
首先是安装anaconda, 这个请移步<a href="https://www.anaconda.com/">官网</a>, 安装最新版本的即可; 接着<code>conda</code>一个新环境, 且包含最新版<code>tensorflow-gpu</code>, 这里笔者命名为<code>tf_gpu</code>
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/3.png" width="80%" />
</div>
正常安装需要耗费一些时间, 这里笔者已经装过了, 进入这个环境后, 可以看到其默认安装了 <code>python 3.6.7</code>以及<code>tensorflow-gpu 1.12.0</code>
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/4.png" width="80%" />
</div>
<h3 id="测试">测试</h3>
进入python测试一下, 成功.
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/5.png" width="80%" />
</div>
<div data-align="center">
<img src="/pictures/Ubuntu/tensorflow/6.png" width="80%" />
</div>
]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>tensorflow</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu食用记录</title>
    <url>/2018/04/18/Ubuntu%E9%A3%9F%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>前言:上中国大学MOOC上面的一门慕课(人工智能实践：Tensorflow笔记)时装了Ubuntu16.04,按照课程视屏方法顺利安装,特此开文记录使用中碰到的一些有意思的事. <span id="more"></span></p>
<hr />
<p>Q:如何安装Ubuntu?</p>
<p>A:参见中国大学MOOC北大课程tensorflow笔记的第一周视频.</p>
<hr />
<p>Q:如何进入Ubuntu?</p>
<p>A:开机狂按Esc,然后F9,选择进入Ubuntu.</p>
<hr />
<p>Q:Ubuntu默认的python模块有哪些(已安装tensorflow)?</p>
<p>A： <img src="/pictures/Ubuntu/piplist.png" alt="avatar" /></p>
<hr />
<p>Q:Firefox浏览器无法播放中国大学MOOC上的视频怎么办?</p>
<p>A:为Friefox安装flash插件(Flash Block)然并.</p>
<hr />
<p>Q:如何为Ubuntu安装chrome?</p>
<p>A:参见:https://blog.csdn.net/qq551551/article/details/78885704</p>
<hr />
<p>Q:如何在Ubuntu启动root用户?</p>
<p>A: <img src="/pictures/Ubuntu/root.png" alt="avatar" /></p>
<hr />
<p>Q:如何在Ubuntu翻墙?</p>
<p>A:参见:https://blog.csdn.net/superbfly/article/details/54950451</p>
<p>SS启动方式为在文档下打开<code>terminal</code>,输入: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sslocal -c SS.JSON</span><br></pre></td></tr></table></figure> ps:SS.JSON为已配置好信息的文件</p>
<hr />
<p>Q:怎么解决“The TensorFlow library wasn't compiled to use SSE4.1 instructions”?</p>
<p>A: <img src="/pictures/Ubuntu/tf_cpp_min_log_level_2.png" alt="avatar" /></p>
<hr />
<p>Q:如何将tab键设定为4个空格?</p>
<p>A:任意目录打开 <code>terminal</code>,输入 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi ~/.vimrc</span><br></pre></td></tr></table></figure> 最后键入以下内容: <img src="/pictures/Ubuntu/tab.png" alt="tab=4" /></p>
<hr />
<p>Q:Adam,GD,Momentum三种优化算法实际表现?</p>
<p>A:一个简单的比较如下,在这个比较中Momentum是收敛最快的,其次是Adam,最慢是GD: 1. Adam <img src="/pictures/Ubuntu/Adam.png" alt="Adam" /> 2. GD <img src="/pictures/Ubuntu/BGD.png" alt="GD" /> 3. Momentum <img src="/pictures/Ubuntu/Momentum.png" alt="Momentum" /></p>
<hr />
<p>Q:ubuntu下安装matplotlib模块？</p>
<p>A: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-matplotlib</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>VC bound</title>
    <url>/2018/06/14/VC-bound/</url>
    <content><![CDATA[<p>前言:最近读Léon Bottou的18年2月的文章<a href="https://arxiv.org/pdf/1606.04838.pdf">Optimization Methods for Large-Scale Machine Learning</a>时,读到式(2.7)觉得怪怪的,于是特此将笔者看到过的有关VC bound的证明整理一下,分直观证明和理论证明去叙述:</p>
<ul>
<li>直观证明来自台湾大学林轩田的机器学习基石课程.</li>
<li>理论证明来自Shai Shalve-Shwartz的UNDERSTANDING MACHINE LEARNIN[UML]一书.</li>
</ul>
<p><code>p.s.</code> 读本文需要读者至少知道VC维的含义, VC界的含义.</p>
<span id="more"></span>
<hr />
<h2 id="motivation">motivation</h2>
<p>Léon Bottou今年2月发的文章中式(2.7): <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \mathcal{O}\left(\sqrt{\frac{1}{2m}\log\left(\frac{2}{\delta}\right)+\frac{d_{\mathcal{H}}}{m}\log\left(\frac{m}{d_{\mathcal{H}}}\right)}\right)
\]</span>其中<span class="math inline">\(L_{S}(h)\)</span>为训练集<span class="math inline">\(S\)</span>上的误差,<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>为泛化误差,<span class="math inline">\(m\)</span>为训练集<span class="math inline">\(S\)</span>的大小,<span class="math inline">\(\delta\)</span>为置信度(即以至少<span class="math inline">\(1-\delta\)</span>的概率上式成立),<span class="math inline">\(d_{\mathcal{H}}\)</span>为假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维,这个不等式的意思是:</p>
<ol type="1">
<li>范化误差与训练误差的差异的最大值会随着假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维<span class="math inline">\(d_{\mathcal{H}}\)</span>的增大而减小</li>
<li>且要想保持VC bound不变,<span class="math inline">\(d_{\mathcal{H}}\)</span>增大的同时样本量<span class="math inline">\(m\)</span>必须与之同步增大</li>
</ol>
<p>初看到这个结论我是懵逼的, 因为它不同于笔者之前看到的几个VC界:</p>
<hr />
<h2 id="vc-bound-1">VC bound 1</h2>
<p>台大林轩田老师在机器学习基石课程中给出了一个VC bound(来自Lecrure7 page21), : <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\frac{4\left(2m\right)^{d_{\mathcal{H}}}}{\delta}}\]</span>整理得:<span class="math display">\[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\frac{4}{\delta}+\frac{8d_{\mathcal{H}}}{m}\log\left(2m\right)}
\]</span>上式均是在以不少于<span class="math inline">\(1-\delta\)</span>概率成立的意义下. 细看这下会发现与Léon Bottou给出的式子有些许不同,下面来看其给出的直观证明,分5步走:</p>
<ol type="1">
<li><span class="math inline">\(L_{\mathcal{D}}\)</span>没法处理, 设法用<span class="math inline">\(L_{S&#39;}\)</span>取代<span class="math inline">\(L_{\mathcal{D}}\)</span>: <span class="math display">\[
\frac{1}{2}\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right] \le \mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{S&#39;}(h)\right|&gt;\frac{\varepsilon}{2}\right]
\]</span>这一步是难点, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-21852-6.pdf">这篇文章</a>page42的引理2给出了证明, 笔者在这里给出自己的直观理解: 显然有<span class="math inline">\(\mathbb{E}_{S\sim \mathcal{D}}L_{S}(h)=L_{\mathcal{D}}(h)\)</span>, 简单将<span class="math inline">\(L_{S}(h)\)</span>视为正态分布, 其期望则是<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>, 若存在一个<span class="math inline">\(h\)</span>, <span class="math display">\[\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\]</span>而<span class="math inline">\(L_{S&#39;}(h)\)</span>大概率会落在离<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>比较近的区域, 也就是大概率会有:<span class="math display">\[\left|L_{S}(h)-L_{S&#39;}(h)\right|&gt;\frac{\varepsilon}{2}\]</span></li>
<li>期望找到一个多项式增长的函数<span class="math inline">\(\tau_{\mathcal{H}}(m)\)</span>(事实上,<span class="math inline">\(\tau_{\mathcal{H}}(m)=\max_{|C|=m}\left|\mathcal{H}_{C}\right|\)</span>,其中<span class="math inline">\(\mathcal{H}_{C}\)</span>为<span class="math inline">\(\mathcal{H}\)</span>限制在<span class="math inline">\(C\)</span>上)满足 <span class="math display">\[
\begin{aligned}\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right] &amp; \le 2\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{S&#39;}}(h)\right|&gt;\frac{\varepsilon}{2}\right]\\\
&amp; = 2\mathbb{P}\left[\exists h \in\mathcal{H} s.t.\left|L_{S}(h)-L_{\mathcal{S&#39;}}(h)\right|&gt;\frac{\varepsilon}{2}\right]\\\
&amp;\le 2\tau_{\mathcal{H}}(m)\mathbb{P}\left[{\rm fixed} \ h \ s.t.\left|L_{S}(h)-L_{\mathcal{S&#39;}}(h)\right|&gt;\frac{\varepsilon}{2}\right]
\end{aligned}
\]</span></li>
<li>考虑<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">霍夫丁不等式</a><span class="math inline">\(\mathbb{P}\left\{\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right\}\le2\exp\left(-2\varepsilon^{2}m\right)\)</span>: <span class="math display">\[
\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right] \le 4\tau_{\mathcal{H}}(m)\exp\left(-2\left(\frac{\varepsilon}{2}\right)^{2}m\right)
\]</span>注意到此时<span class="math inline">\(\mathcal{D}\)</span>替换成了<span class="math inline">\(S&#39;\)</span>, 而<span class="math inline">\(S\)</span>,<span class="math inline">\(S&#39;\)</span>均有<span class="math inline">\(m\)</span>个样本故将<span class="math inline">\(\tau_{\mathcal{H}}(m)\)</span>修正为<span class="math inline">\(\tau_{\mathcal{H}}(2m)\)</span>, 另外 <span class="math display">\[
\left|L_{S}(h)-L_{\mathcal{S&#39;}}(h)\right|&gt;\frac{\varepsilon}{2}\Leftrightarrow\left|L_{S}(h)-\left(L_{\mathcal{S&#39;}}(h)+L_{S}(h)\right)/2\right|&gt;\frac{\varepsilon}{4}
\]</span>故<span class="math inline">\(\frac{\varepsilon}{2}\)</span>修正为<span class="math inline">\(\frac{\varepsilon}{4}\)</span> ,得到: <span class="math display">\[
\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right] \le 4\tau_{\mathcal{H}}(2m)\exp\left(-2\left(\frac{\varepsilon}{4}\right)^{2}m\right)
\]</span>这个式子也能在上面那篇<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-21852-6.pdf">文章</a>page42的定理2中找到.</li>
<li>归纳法可证:<span class="math inline">\(\tau_{\mathcal{H}}(m)\le\sum_{i=0}^{d_{\mathcal{H}}}\binom{m}{i}\)</span>,林老师在这里粗糙的放大成<span class="math inline">\(\sum_{i=0}^{d_{\mathcal{H}}}\binom{m}{i}&lt;m^{d_{\mathcal{H}}}\)</span>,整理得到: <span class="math display">\[
\mathbb{P}\left\{\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right\}\le4 (2m)^{d_{\mathcal{H}}}\exp\left(-\frac{\varepsilon^{2}}{8}m\right)
\]</span></li>
<li>令<span class="math inline">\(\delta=4 (2m)^{d_{\mathcal{H}}}\exp\left(-\frac{\varepsilon^{2}}{8}m\right)\)</span>, 即<span class="math inline">\(\varepsilon=\sqrt{\frac{8}{m}\log\frac{4\left(2m\right)^{d_{\mathcal{H}}}}{\delta}}\)</span>, 有: <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\frac{4\left(2m\right)^{d_{\mathcal{H}}}}{\delta}}
\]</span></li>
</ol>
<p>思路叙述完毕,细看之下,4中不等式放的太宽了,事实上,当<span class="math inline">\(m&gt;d_{\mathcal{H}}+1\)</span>有如下不等式成立:<span class="math inline">\(\sum_{i=0}^{d_{\mathcal{H}}}\binom{m}{i}\le \left(\frac{em}{d_{\mathcal{H}}}\right)^{d_{\mathcal{H}}}\)</span>(证明可见[UML]一书附录A.5),此时VC bound变成: <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\frac{4(\frac{2em}{d_{\mathcal{H}}})^{d_{\mathcal{H}}}}{\delta}}
\]</span>整理得: <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\left(\frac{4}{\delta}\right)+\frac{8d_{\mathcal{H}}}{m}\log\left(\frac{2em}{d_\mathcal{H}}\right)}
\]</span> <code>p.s.</code> 显然有<span class="math inline">\(\mathbb{P}\left[\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right]\le\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|&gt;\varepsilon\right]\)</span>, 就能轻易得到西瓜书定理12.2及定理12.3的结论.</p>
<hr />
<h2 id="vc-bound-23">VC bound 2&amp;3</h2>
<p>Shai Shalve-Shwartz的[UML]一书中定理6.11也给出了两个VC bound: <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le\frac{2+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\delta\sqrt{2m}}
\]</span>同样上式是以<span class="math inline">\(1-\delta\)</span>的概率成立. <code>p.s.</code>若把 <span class="math display">\[
\tau_{\mathcal{H}}(m)\le\sum_{i=0}^{d_{\mathcal{H}}}\binom{m}{i}\le\left(\frac{em}{d_{\mathcal{H}}}\right)^{d_{\mathcal{H}}}
\]</span> 带入上式, 也能得到形式上类似于上述两个VC bound的估计(第二项会比较相似), 但会明显劣于上述两个界,因为随着<span class="math inline">\(\delta\)</span>接近于0, <span class="math inline">\(\frac{1}{\delta}\)</span>的增长会明显快于<span class="math inline">\(\log(\frac{1}{\delta})\)</span>. 庆幸的是, [UML]的28章中给出一个相对更紧的界, 这与上述两个VC bound也一致起来了(系数不一致), 记为VC bound 3, 证明见28章: <span class="math display">\[
\begin{aligned}
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right| &amp; \leq \sqrt{\frac{8 d_{\mathcal{H}} \log (e m / d_{\mathcal{H}})}{m}}+\sqrt{\frac{2 \log (4 / \delta)}{m}} \\
&amp; \leq \sqrt{2} \sqrt{\frac{8 d_{\mathcal{H}} \log (e m / d_{\mathcal{H}})+2 \log (4 / \delta)}{m}}
\end{aligned}
\]</span> VC bound 2的证明分8步走:</p>
<ol type="1">
<li>引理1: <span class="math display">\[
\mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\right]\le\frac{2+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\sqrt{2m}}
\]</span>现先假定引理1成立,那么由随机变量<span class="math inline">\(\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\)</span>显然是非负的以及马尔可夫不等式<span class="math inline">\(\mathbb{P}[X\ge \delta]\le\frac{\mathbb{E}[X]}{\delta}\)</span>可直接推得结论成立: <span class="math display">\[
\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\ge\frac{2+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\delta\sqrt{2m}}\right] \le \frac{\mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[ \sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\right]}{\frac{2+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\delta\sqrt{2m}}}\le \delta
\]</span>即 <span class="math display">\[
\mathbb{P}\left[\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le\frac{2+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\delta\sqrt{2m}}\right] \ge 1-\delta
\]</span></li>
<li>问题转化为证引理1,考虑引理1的左半部分: <span class="math display">\[
\begin{aligned} \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\right] &amp; =  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|\mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}L_{S&#39;}(h)-L_{S}(h)\right|\right] \\\
&amp; \le  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; \le  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}\sup_{h\in\mathcal{H}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; =  \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; =  \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right] \end{aligned}
\]</span> 其中第一个等号是因为<span class="math inline">\(L_{\mathcal{D}}(h)=\mathbb{E}_{S&#39;\sim\mathcal{D}^{m}}L_{S&#39;}(h)\)</span>,第二个不等号为绝对值放到积分里面,第三个不等号为期望的上界小于等于上界的期望,第四个等号为两个期望合并,第五个等号为<span class="math inline">\(L_{S}(h)\)</span>写开来,且这里的<span class="math inline">\(l(h,x_{i})\)</span>为<span class="math inline">\(h\)</span>下的0-1损失</li>
<li>引入随机变量<span class="math inline">\(\sigma\in\{\pm 1\}^{m}\)</span>,2.中最右端等价于: <span class="math display">\[
\begin{aligned} \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right] &amp; =  \mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right]\\\
&amp; = \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}} \mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right] \end{aligned}
\]</span>其中第一个等号为<span class="math inline">\(\pm1\)</span>对绝对值无影响,第二个等号为交换期望</li>
<li>考虑3.最右端,固定<span class="math inline">\(S,S&#39;\)</span>,令<span class="math inline">\(C=S\cup S&#39;\)</span>,那么此时<span class="math inline">\(\mathcal{H}\)</span>仅会涉及<span class="math inline">\(C\)</span>上的假设,即可以把<span class="math inline">\(\mathcal{H}\)</span>限制在<span class="math inline">\(C\)</span>上: <span class="math display">\[
\mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right]= \mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\left[  \max_{h\in\mathcal{H}_{C}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right]
\]</span></li>
<li>固定<span class="math inline">\(h\in\mathcal{H}_{C}\)</span>,并记<span class="math inline">\(\theta_{h} \triangleq \frac{1}{m}\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\)</span>,则有 <span class="math display">\[
\begin{aligned}\mathbb{E}\theta_{h} &amp; =\mathbb{E}\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right) \\\
&amp; = \frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\\\
&amp; = 0
\end{aligned}
\]</span> 且<span class="math inline">\(\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\)</span>是在<span class="math inline">\([-1,1]\)</span>(确切说是<span class="math inline">\(\{-1,0,1\}\)</span>)上取值的独立随机变量,因此满足<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">霍夫丁不等式</a>的条件,得<span class="math inline">\(\forall \rho&gt;0,\)</span> <span class="math display">\[
\mathbb{P}\left[\left|\theta_{h}\right|&gt;\rho\right]\le2\exp(-2m\rho^{2})
\]</span>即 <span class="math display">\[
\mathbb{P}\left[\frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|&gt;\rho\right]\le2\exp(-2m\rho^{2})
\]</span></li>
<li>注意到此时<span class="math inline">\(\mathcal{H}_{C}\)</span>仅包含有限个假设,由联合界(简言之,<span class="math inline">\(\mathbb{P}(A\cup B)\le\mathbb{P}(A)+\mathbb{P}(B)\)</span>)可得: <span class="math display">\[
\mathbb{P}\left[\max_{h\in\mathcal{H}_{C}}\left|\theta_{h}\right|&gt;\rho\right]\le2\left|\mathcal{H}_{C}\right|\exp(-2m\rho^{2})
\]</span>即 <span class="math display">\[
\mathbb{P}\left[\max_{h\in\mathcal{H}_{C}}\frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|&gt;\rho\right]\le2\left|\mathcal{H}_{C}\right|\exp(-2m\rho^{2})
\]</span></li>
<li>第7步用到一个引理2:</li>
</ol>
<blockquote>
<p><span class="math inline">\(X\)</span>为一随机变量,<span class="math inline">\(x\in\mathbb{R}\)</span>为一标量,假定存在<span class="math inline">\(a&gt;0\)</span>和<span class="math inline">\(b&gt;e\)</span>,使得对<span class="math inline">\(\forall t&gt;0\)</span>,有<span class="math inline">\(\mathbb{P}\left[\left|X-x\right|&gt;t\right]\le2be^{-t^2/a^2}\)</span>,那么 <span class="math display">\[
\mathbb{E}\left[\left|X-x\right|\right]\le a(2+\sqrt{\log{b}})
\]</span></p>
</blockquote>
<p>引理2的证明可见[UML]一书附录A.4,将<span class="math inline">\(X=\max_{h\in\mathcal{H}_{C}}\left|\theta_{h}\right|\)</span>,<span class="math inline">\(x=0\)</span>,<span class="math inline">\(b=\left|\mathcal{H}_{C}\right|\)</span>,<span class="math inline">\(a=1/\sqrt{2m}\)</span>带入得: <span class="math display">\[
\mathbb{E}\left[\max_{h\in\mathcal{H}_{C}}\left|\theta_{h}\right|\right]\le\frac{2+\sqrt{\log\left(\left|\mathcal{H}_{C}\right|\right)}}{\sqrt{2m}}
\]</span>即有4.的最右端 <span class="math display">\[
\mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\left[  \max_{h\in\mathcal{H}_{C}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right]\le\frac{2+\sqrt{\log\left(\left|\mathcal{H}_{C}\right|\right)}}{\sqrt{2m}}
\]</span>再由定义<span class="math inline">\(\tau_{\mathcal{H}}(m)=\max_{|C|=m}\left|\mathcal{H}_{C}\right|\)</span>,以及<span class="math inline">\(C\)</span>最多包含<span class="math inline">\(2m\)</span>个样本得3.的最右端: <span class="math display">\[
\mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}} \mathop{\mathbb{E}}_{\sigma\sim U_{\pm}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\sigma_{i}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right] \le\frac{2+\sqrt{\log\left(\tau_{\mathcal{H}}(2m)\right)}}{\sqrt{2m}}
\]</span>再由2.即 <span class="math display">\[
\mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\right]\le\frac{2+\sqrt{\log\left(\tau_{\mathcal{H}}(2m)\right)}}{\sqrt{2m}}
\]</span>即引理1得证</p>
<p>仔细审视之下,7.中的引理2的证明似乎有些问题,下面直接看修正后的引理2的证明: 引理2:</p>
<blockquote>
<p><span class="math inline">\(X\)</span>为一随机变量,<span class="math inline">\(x\in\mathbb{R}\)</span>为一标量,假定存在<span class="math inline">\(a&gt;0\)</span>和<span class="math inline">\(b&gt;e^{4}\)</span>,使得对<span class="math inline">\(\forall t&gt;0\)</span>,有<span class="math display">\[\mathbb{P}\left[\left|X-x\right|&gt;t\right]\le2be^{-t^2/a^2}\]</span>那么 <span class="math display">\[
\mathbb{E}\left[\left|X-x\right|\right]\le a(2e^{2\sqrt{\log(b)}-1}+\sqrt{\log{b}})
\]</span></p>
</blockquote>
<p>证明:令<span class="math inline">\(t_{i}=a\left(i+\sqrt{\log(b)}\right),i=0,1,...\)</span>,有: <span class="math display">\[
\begin{align}\mathbb{E}\left[\left|X-x\right|\right]&amp; \le a\sqrt{\log(b)}\mathbb{P}\left[\left|X-x\right|\le a\sqrt{\log(b)}\right]+\sum_{i=1}^{+\infty}t_{i}\mathbb{P}\left[t_{i-1}&lt;\left|X-x\right|\le t_{i}\right]\\\
&amp; \le a\sqrt{\log(b)}+\sum_{i=1}^{+\infty}t_{i}\mathbb{P}\left[\left|X-x\right|&gt; t_{i-1}\right]\\\
&amp; \le a\sqrt{\log(b)}+\sum_{i=1}^{+\infty}a\left(i+\sqrt{\log(b)}\right)2be^{-\left(i-1+\sqrt{\log(b)}\right)^{2}}\\\
&amp; = a\sqrt{\log(b)}+2ab\sum_{i=1}^{+\infty}\left(i+\sqrt{\log(b)}\right)e^{-\left(i-1+\sqrt{\log(b)}\right)^{2}}\\\
&amp; \le a\sqrt{\log(b)}+2ab\int_{\sqrt{\log(b)}}^{+\infty} xe^{-\left(x-1\right)^{2}} dx\\\
&amp; = a\sqrt{\log(b)}+2ab\int_{\sqrt{\log(b)}-1}^{+\infty} (y+1)e^{y^{2}} dy\\\
&amp; \le a\sqrt{\log(b)}+4ab\int_{\sqrt{\log(b)}-1}^{+\infty} ye^{y^{2}} dy\\\
&amp; = a\sqrt{\log(b)}+2ab\left[-e^{-y^2}\right]_{\sqrt{\log(b)}-1}^{+\infty}\\\
&amp; =  a\sqrt{\log(b)}+2abe^{-\log(b)+2\sqrt{\log(b)}-1}\\\
&amp; =  a\sqrt{\log(b)}+2ae^{2\sqrt{\log(b)}-1}\\\
&amp; = a(2e^{2\sqrt{\log(b)}-1}+\sqrt{\log{b}})
\end{align}
\]</span></p>
<ol type="1">
<li>第一个不等号为分区间放大期望</li>
<li>第二个不等号为把概率放大</li>
<li>第三个不等号为将条件带入</li>
<li>第四个等号为常数放到前面</li>
<li>第五个不等号可由<span class="math inline">\(xe^{-\left(x-1\right)^{2}}\)</span>的单调递减性得到(<strong>原证明则是在这一步的积分下限错误的放大为<span class="math inline">\(1+\sqrt{\log(b)}\)</span></strong>)</li>
<li>第六个不等号为变量代换,</li>
<li>第七个不等号由<span class="math inline">\(b&gt;e^{4}\)</span>知积分下限大于1,故可将将1放大为<span class="math inline">\(y\)</span></li>
<li>第八个等号为求积分</li>
<li>第九、十和十一个等号均为化简.</li>
</ol>
<p>那么上述第7步应当做相应修正,此时的VC bound应当修正为: <span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le\frac{2e^{2\sqrt{\log\left(\tau_{\mathcal{h}}(2m)\right)}-1}+\sqrt{\log\left(  \tau_{\mathcal{h}}(2m) \right)}}{\delta\sqrt{2m}}
\]</span><code>p.s.</code>上面那个界可以说非常丑, 只能怪笔者没能把引理中后面那部分放缩到简单情形, 望读者老爷们不吝赐教. 最后要注意的是该VC bound 2要求假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维要满足<span class="math inline">\(b=\left|\mathcal{H}_{C}\right|&gt;e^{4}\)</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>VC bound</tag>
      </tags>
  </entry>
  <entry>
    <title>个人博客进化史</title>
    <url>/2018/03/24/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E8%BF%9B%E5%8C%96%E5%8F%B2/</url>
    <content><![CDATA[<p>前言:本文记录了本博客的进化史. <span id="more"></span></p>
<h2 id="将next主题更新至6.0.6">将next主题更新至6.0.6</h2>
<p><strong>2018/03/24</strong>: 将主题升级至6.0.6, 并在主题配置文件界面开启了访问统计(不蒜子统计)及站内搜索功能, 主要参考了Next<a href="http://theme-next.iissnan.com/getting-started.html">主题文档</a>.</p>
<h2 id="用mathjax引擎渲染latex公式">用mathjax引擎渲染latex公式</h2>
<p><strong>2018/03/24</strong>: 在主题配置文件开启了latex公式支持功能, 但没法用宏包, 主要参考了Next<a href="http://theme-next.iissnan.com/getting-started.html">主题文档</a>. 它的使用方法很简答,只需分三步:</p>
<ol type="1">
<li>将主题配置文件中mathjax引擎的开关打开 <figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Math Equations Render Support</span></span><br><span class="line"><span class="attr">math:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default(true) will load mathjax/katex script on demand</span></span><br><span class="line">  <span class="comment"># That is it only render those page who has &#x27;mathjax: true&#x27; in Front Matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax/katex srcipt EVERY PAGE.</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex</span></span><br></pre></td></tr></table></figure></li>
<li>hexo的配置文件最后加入一行: <figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>使用前记得在front-matter中将开关打开,如: <figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 凸优化学习笔记</span><br><span class="line">date: 2018-02-23 15:05:49</span><br><span class="line">tags: [凸优化]</span><br><span class="line">categories: 凸优化</span><br><span class="line"><span class="section">mathjax: true</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="数学公式相关问题">数学公式相关问题</h2>
<p><strong>2018/03/24</strong>: 输入latex公式时发现不少问题,如想输入大括号没法显示: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$\&#123;x|a&#x27;x=b\&#125;$</span><br></pre></td></tr></table></figure> 期望上述公式显示为:<span class="math inline">\(\{x|a&#39;x=b\}\)</span>.</p>
<p>或是想输入下标却被hexo渲染为<code>&lt;em&gt;</code>标签: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$C_&#123;1&#125;$</span><br></pre></td></tr></table></figure> 期望上述公式显示为:<span class="math inline">\(C_{1}\)</span>.</p>
<p>原因是 _ , { , } 等比较特殊,它们会被渲染成其他标签. 一个有效的方案是把它们的特殊性手动去掉. 找到hexo根目录下面的<code>node_modules\marked\lib\marked.js</code>,查找将其中第451行<code>escape</code>和459行<code>em</code>做如下修改,修改前如下: <figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  <span class="attr">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line">  autolink: <span class="regexp">/^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/</span>,</span><br><span class="line">  url: noop,</span><br><span class="line">  <span class="attr">tag</span>: <span class="regexp">/^&lt;!--[\s\S]*?--&gt;|^&lt;\/?\w+(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;/</span>,</span><br><span class="line">  link: <span class="regexp">/^!?\[(inside)\]\(href\)/</span>,</span><br><span class="line">  reflink: <span class="regexp">/^!?\[(inside)\]\s*\[([^\]]*)\]/</span>,</span><br><span class="line">  nolink: <span class="regexp">/^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/</span>,</span><br><span class="line">  strong: <span class="regexp">/^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/</span>,</span><br><span class="line">  em: <span class="regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  code: <span class="regexp">/^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/</span>,</span><br><span class="line">  br: <span class="regexp">/^ &#123;2,&#125;\n(?!\s*$)/</span>,</span><br><span class="line">  del: noop,</span><br><span class="line">  <span class="attr">text</span>: <span class="regexp">/^[\s\S]+?(?=[\\&lt;!\[_*`]| &#123;2,&#125;\n|$)/</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure> 修改后如下: <figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  <span class="attr">escape</span>: <span class="regexp">/^\\([\\`*\[\]()#+\-.!_&gt;])/</span>,</span><br><span class="line">  autolink: <span class="regexp">/^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/</span>,</span><br><span class="line">  url: noop,</span><br><span class="line">  <span class="attr">tag</span>: <span class="regexp">/^&lt;!--[\s\S]*?--&gt;|^&lt;\/?\w+(?:&quot;[^&quot;]*&quot;|&#x27;[^&#x27;]*&#x27;|[^&#x27;&quot;&gt;])*?&gt;/</span>,</span><br><span class="line">  link: <span class="regexp">/^!?\[(inside)\]\(href\)/</span>,</span><br><span class="line">  reflink: <span class="regexp">/^!?\[(inside)\]\s*\[([^\]]*)\]/</span>,</span><br><span class="line">  nolink: <span class="regexp">/^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/</span>,</span><br><span class="line">  strong: <span class="regexp">/^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/</span>,</span><br><span class="line">  em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  code: <span class="regexp">/^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/</span>,</span><br><span class="line">  br: <span class="regexp">/^ &#123;2,&#125;\n(?!\s*$)/</span>,</span><br><span class="line">  del: noop,</span><br><span class="line">  <span class="attr">text</span>: <span class="regexp">/^[\s\S]+?(?=[\\&lt;!\[_*`]| &#123;2,&#125;\n|$)/</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2 id="latex多行公式对齐">latex多行公式对齐</h2>
<p><strong>2018/06/14</strong>: 解决了latex多行公式对齐问题. <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;aligned&#125; \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#125;(h)-L_&#123;\mathcal&#123;D&#125;&#125;(h)\right|\right] &amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|\mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right] \\</span><br><span class="line">&amp; \le  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\</span><br><span class="line">&amp; \le  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\</span><br><span class="line">&amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S,S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\</span><br><span class="line">&amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S,S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125; \frac&#123;1&#125;&#123;m&#125;\left|\sum_&#123;i=1&#125;^&#123;m&#125;\left(l(h,x_&#123;i&#125;&#x27;)-l(h,x_&#123;i&#125;)\right)\right|\right] \end&#123;aligned&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure> 正常latex多行公式对齐如上,但显示结果却在一行上,观察一番后发现因为转义的原因此时换行需要三个反斜杠<code>\\\</code>,正确的latex代码如下,特此记录 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;aligned&#125; \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#125;(h)-L_&#123;\mathcal&#123;D&#125;&#125;(h)\right|\right] &amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|\mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right] \\\</span><br><span class="line">&amp; \le  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\\</span><br><span class="line">&amp; \le  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\\</span><br><span class="line">&amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S,S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125;\left|L_&#123;S&#x27;&#125;(h)-L_&#123;S&#125;(h)\right|\right]\\\</span><br><span class="line">&amp; =  \mathop&#123;\mathbb&#123;E&#125;&#125;_&#123;S,S&#x27;\sim\mathcal&#123;D&#125;^&#123;m&#125;&#125;\left[  \sup_&#123;h\in\mathcal&#123;H&#125;&#125; \frac&#123;1&#125;&#123;m&#125;\left|\sum_&#123;i=1&#125;^&#123;m&#125;\left(l(h,x_&#123;i&#125;&#x27;)-l(h,x_&#123;i&#125;)\right)\right|\right] \end&#123;aligned&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure> <span class="math display">\[
\begin{aligned} \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\right] &amp; =  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|\mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}L_{S&#39;}(h)-L_{S}(h)\right|\right] \\\
&amp; \le  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; \le  \mathop{\mathbb{E}}_{S\sim\mathcal{D}^{m}}\left[  \mathop{\mathbb{E}}_{S&#39;\sim\mathcal{D}^{m}}\sup_{h\in\mathcal{H}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; =  \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}}\left|L_{S&#39;}(h)-L_{S}(h)\right|\right]\\\
&amp; =  \mathop{\mathbb{E}}_{S,S&#39;\sim\mathcal{D}^{m}}\left[  \sup_{h\in\mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^{m}\left(l(h,x_{i}&#39;)-l(h,x_{i})\right)\right|\right] \end{aligned}
\]</span></p>
<h2 id="将博客部署到coding.net上及被谷歌收录">将博客部署到coding.net上及被谷歌收录</h2>
<p><strong>2018/09/04</strong>: 此次升级将博客部署到了coding.net上, 主要参考了<a href="https://www.jianshu.com/p/b8c9beb07d1c">这篇博文</a>也就是说:</p>
<ul>
<li>http://msgsxj.cn/</li>
<li>http://msgsxj.coding.me/</li>
<li>http://msgsxj.github.io/</li>
</ul>
<p>这三个url均能到达本站. 另外被谷歌收录(百度已经收录过了), 也就是说能在google中搜到本站, 之前不行.</p>
<h2 id="阅读统计量清零">阅读统计量清零</h2>
<p><strong>2018/09/15</strong>: 从18年3月开始用不蒜子统计来统计每篇文章阅读量及站点访问量, 但此次一不小心把统计结果清零了, 特此记录</p>
<h2 id="为自己的博客加上小锁">为自己的博客加上小锁</h2>
<p><strong>2018/09/22</strong>: 为了给个人博客加上小锁, 主要做了以下三件事情:</p>
<ul>
<li>按照chrome浏览器的控制台的error与warning, 在主题配置文件中关闭了一些功能, 包括自动推送url到百度</li>
<li>在主题配置文件关闭了livere评论功能</li>
<li>最后申请coding.net的三个月一期的免费证书(过三个月得续一次), 成功加上小锁</li>
</ul>
<h2 id="把图片从图床搬到本地">把图片从图床搬到本地</h2>
<p><strong>2018/11/08</strong>: 七牛云开始收费了, 于是手动把图片般到本地路径, 正好把小锁配齐.</p>
<h2 id="博客写作平台迁移至mac">博客写作平台迁移至mac</h2>
<p><strong>2020/01/01</strong>: 前些日子入了台mac, 这里简单记录一下将hexo从windows迁移至mac的过程.</p>
<ol type="1">
<li><p>terminal下安装git,node以及hexo, 参考了<a href="https://www.jianshu.com/p/5637ced606d4">这篇博客</a> <figure class="highlight plaintext"><figcaption><span>[install hexo]</span></figcaption><table><tr><td class="code"><pre><span class="line">brew install git</span><br><span class="line">brew install node</span><br><span class="line">npm install hexo g</span><br></pre></td></tr></table></figure> 安装node过程中会在系统用户目录下生成一个<code>node_modules</code>文件夹并抛出<span class="label warning">warning</span>后生成一个<code>package-lock.json</code>文件, 这只是警告系统用户目录下没有<code>package.json</code>文件.</p></li>
<li><p>初始化hexo,继续使用next主题 <figure class="highlight plaintext"><figcaption><span>[use next theme]</span></figcaption><table><tr><td class="code"><pre><span class="line">mkdir hexo</span><br><span class="line">cd hexo</span><br><span class="line">hexo init</span><br><span class="line">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure> 接着就是source文件的覆盖,<a href="https://hexo.io/docs/configuration.html">hexo配置</a>,next主题文件的配置.</p></li>
<li><p>仍然在hexo目录下将hexo的markdown引擎从默认的<code>marked</code>更换为<code>pandoc</code>以更好的支持latex,参考了next主题中的一篇<a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/MATH.md">doc</a> <figure class="highlight plaintext"><figcaption><span>[pandoc]</span></figcaption><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br><span class="line">npm install hexo-renderer-pandoc</span><br></pre></td></tr></table></figure> <code>markdown-it</code>与<code>pandoc</code>最大的区别就是需要更多的空行, 所以遇到显示不正常不妨试着加空行.</p></li>
</ol>
]]></content>
      <categories>
        <category>聊聊生活</category>
      </categories>
      <tags>
        <tag>博客的进化史</tag>
        <tag>latex</tag>
        <tag>加上小锁</tag>
      </tags>
  </entry>
  <entry>
    <title>典型相关分析</title>
    <url>/2020/06/20/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>前言: 最近在读脑磁共振影像数据的时空分析一书, 本文主要关于典型相关分析(CCA)的数学原理介绍 <span id="more"></span></p>
<h2 id="典型相关分析基本原理">典型相关分析基本原理</h2>
<p>统计学中, 典型相关分析(canonical correlation analysis, CCA)是研究两组随机变量之间关系的方法, 其本质上是用一对<strong>典型变量</strong>的相关系数去衡量两组随机变量之间的关系, 而这里的典型变量选用<strong>变量的线性组合</strong>的方式去构造. 我们有两组随机变量<span class="math inline">\(X=(x_1,...,x_m)\)</span>, <span class="math inline">\(Y=(y_1,...,y_n)\)</span>, CCA目标是求得向量<span class="math inline">\(a_1\in\mathbb{R}^m\)</span>, <span class="math inline">\(b_1\in\mathbb{R}^n\)</span>, 使得<span class="math inline">\(u_1=a_1^TX\)</span>和<span class="math inline">\(v_1=b_1^TY\)</span>的皮尔逊相关系数</p>
<p><span class="math display">\[corr(u_1, v_1)=\frac{cov(a_1^TX, b_1^TY)}{\sqrt{cov(a_1^TX,a_1^TX)}\sqrt{cov(b_1^TY,b_1^TY)}}=\frac{a_1^T\Sigma_{X,Y}b_1}{\sqrt{a_1^T\Sigma_{X,X}a_1}\sqrt{b_1^T\Sigma_{Y,Y}b_1}}\]</span> 最大, 其中<span class="math inline">\(\Sigma_{X,Y}=cov(X,Y)\)</span>. 显然<span class="math inline">\(\Sigma_{X,X}\)</span>, <span class="math inline">\(\Sigma_{Y,Y}\)</span>半正定(p.s. 一般可认为是正定的, 只要<span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>中不包含方差为0的随机变量), 因此具有<a href="https://en.wikipedia.org/wiki/Definite_symmetric_matrix">唯一的正定平方根</a><span class="math inline">\(\Sigma_{X,X}^{1/2}\)</span>,<span class="math inline">\(\Sigma_{Y,Y}^{1/2}\)</span>. 如下定义<span class="math inline">\(c_1=\Sigma_{X,X}^{1/2}a_1\)</span>, <span class="math inline">\(d_1=\Sigma_{Y,Y}^{1/2}b_1\)</span>, 问题转化为关于<span class="math inline">\(c_1\in\mathbb{R}^m\)</span>, <span class="math inline">\(d_1\in\mathbb{R}^n\)</span>最大化</p>
<p><span class="math display">\[\frac{c_1^T\Sigma_{X,X}^{-1/2}\Sigma_{X,Y}\Sigma_{Y,Y}^{-1/2}d_1}{\sqrt{c_1^Tc_1}\sqrt{d_1^Td_1}}\]</span> 由Cauchy-Schwarz不等式可得</p>
<p><span class="math display">\[c_1^{T}\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1/2}d_1\leq \sqrt{c_1^{T}\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_1}\sqrt{d_1^{T}d_1}\]</span> 即</p>
<p><span class="math display">\[\frac{c_1^T\Sigma_{X,X}^{-1/2}\Sigma_{X,Y}\Sigma_{Y,Y}^{-1/2}d_1}{\sqrt{c_1^Tc_1}\sqrt{d_1^Td_1}}\le \frac{\sqrt{c_1^{T}\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_1}}{\sqrt{c_1^Tc_1}}\]</span> 且当且仅当<span class="math inline">\(d_1\)</span>与<span class="math inline">\(\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_1\)</span>共线时取等号. 而对于不等式的右边, 由下述矩阵的实对称性易得当<span class="math inline">\(c_1\)</span>为矩阵</p>
<p><span class="math display">\[\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}\]</span> 的最大特征值<span class="math inline">\(\lambda_1\)</span>对应的特征向量时, 不等式右边取最大值<span class="math inline">\(\sqrt{\lambda_m}\)</span>. 我们也可以接着去寻找第二对典型变量, 这里遵循的原则是在<span class="math inline">\(u_2,v_2\)</span>与第一对规范变量不相关的约束下:</p>
<p><span class="math display">\[cov(u_2,u_1)=0, cov(v_2,v_1)=0\]</span> 寻求使相同的相关性最大化的向量<span class="math inline">\(\alpha_1\)</span>. 注意到<span class="math inline">\(\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}\)</span>为实对称矩阵, 因此其特征值对应的特征向量互相正交(<span class="math inline">\(\alpha_1^T\alpha_2=0\)</span>), 因此只需取<span class="math inline">\(u_2=a_2^TX\)</span>, <span class="math inline">\(a_2=\Sigma_{X,X}^{-1/2}c_2\)</span>, 这里<span class="math inline">\(c_2\)</span>相应取为矩阵第二大的特征值<span class="math inline">\(\lambda_2\)</span>对应的特征向量<span class="math inline">\(\alpha_2\)</span>. 就能轻易验证</p>
<p><span class="math display">\[cov(u_2,u_1)=cov(a_2^TX,a_1^TX)=a_2^T\Sigma_{X,X}a_1=c_2^Tc_1=\alpha_2^T\alpha_1=0\]</span> 而<span class="math inline">\(v_2\)</span>只需与<span class="math inline">\(\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_2\)</span>共线, 也能轻易验证</p>
<p><span class="math display">\[\begin{aligned}
cov(v_2,v_1)&amp;=cov(\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_2,\Sigma _{Y,Y}^{-1/2}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_1)\\
&amp;=c_2^T\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}c_1\\
&amp;=\lambda_1 c_2^Tc_1\\
&amp;=0
\end{aligned}\]</span> 上述过程可重复<span class="math inline">\(\min\{m,n\}\)</span>遍, 可得<span class="math inline">\(\min\{m,n\}\)</span>对典型变量. 事实上, 通过简单的变换, 典型变量的参数<span class="math inline">\(a\)</span>只需通过求<span class="math inline">\(\Sigma _{X,X}^{-1}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}\)</span>的特征向量就能得到, 此时<span class="math inline">\(b\)</span>只需与<span class="math inline">\(\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}a\)</span>共线.</p>
<h2 id="特征向量与svd">特征向量与SVD</h2>
<p>实际计算时, 要求<span class="math inline">\(AA^T\)</span>的特征向量<span class="math inline">\(u\)</span>, 其实等价于对<span class="math inline">\(A=U\Sigma V^T\)</span>做奇异值分解(SVD), 此时要求的<span class="math inline">\(u\)</span>为<span class="math inline">\(U\)</span>列向量. 这是因为<span class="math inline">\(AA^T=U\Sigma V^*V\Sigma U^*\)</span>, 即</p>
<p><span class="math display">\[AA^TU=U\Sigma^2\]</span></p>
<p>因此求<span class="math inline">\(\Sigma _{X,X}^{-1/2}\Sigma _{X,Y}\Sigma _{Y,Y}^{-1}\Sigma _{Y,X}\Sigma _{X,X}^{-1/2}\)</span>的特征向量等价于对<span class="math inline">\(\Sigma_{X,X}^{-1/2}\Sigma_{X,Y}\Sigma_{Y,Y}^{-1/2}\)</span>做奇异值分解(SVD).</p>
<h2 id="cca与fmri数据">CCA与fMRI数据</h2>
<p>具体到fMRI数据中, 我们假定观测收集到的数据为一个三维张量<span class="math inline">\(x(t)\)</span>, 其中<span class="math inline">\(t\)</span>为时间变量, 因此输入数据为带时间维度的四维张量. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.476.486&amp;rep=rep1&amp;type=pdf">这篇文章</a>提出令<span class="math inline">\(y(t)=x(t-1)\)</span>, 因此若我们把CCA应用在<span class="math inline">\(x(t)\)</span>, <span class="math inline">\(y(t)\)</span>上, 此时找出的典型变量不仅是数据数据<span class="math inline">\(x(t)\)</span>的<strong>线性组合</strong>, 而且具有<strong>时间维度上的强自相关性(差分为1)</strong>, 而我们所期望得到的神经响应信号(呼吸产生的噪声等)也具有这样的特征. 当然缺点也显然, 这样的思路没能够用好输入数据的空间结构, 此外, 该文作者还给出一种试图寻求空间维度上的强自相关性的思路, 详见论文链接中的式<span class="math inline">\((15)\)</span>.</p>
<h2 id="cca-ica和pca">CCA, ICA和PCA</h2>
<p>换一个角度看, 将CCA应用到<span class="math inline">\(x(t)\)</span>, <span class="math inline">\(x(t-1)\)</span>可视为是在对<span class="math inline">\(x(t)\)</span>进行<strong>线性降维</strong>的操作, 只不过需要满足构造出特征的<strong>时间维度的自相关性</strong>尽可能大且不同的构造特征之间<strong>互不相关</strong>的约束, 这就能很自然的联想到另外两种降维的方法, 这里仅简单介绍:</p>
<ol type="1">
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">主成分分析</a>(Principal component analysis, PCA)的出发点为降维后的数据点到新子空间的距离都尽可能的近, 或者等价地, 降维后的数据在新子空间上的投影都尽可能的分散, 其构造出的特征<strong>方差尽可能大</strong>且不同的构造特征之间<strong>互不相关</strong>.</li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis">独立成分分析</a>(Independent components analysis, ICA), 多用于<a href="https://en.wikipedia.org/wiki/Signal_separation">盲源分离问题</a>, 比如采集fMRI数据可能会有大脑信号、眼动、呼吸等多个源, 而我们感兴趣的可能只是大脑信号, 因此需要将其分离出来. ICA最大的假设是不同的构造特征之间<strong>相互独立</strong>且构造特征都是<strong>非高斯分布</strong>.</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CCA</tag>
        <tag>SVD</tag>
        <tag>特征向量</tag>
        <tag>ICA</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title>变分贝叶斯方法</title>
    <url>/2018/09/05/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>前言:本文主要是关于变分贝叶斯方法, 这是一类用简单分布去逼近目标分布的方法, 主要参考了Coursera上国立高等经济大学Advanced Machine Learning系列课程Course3: Bayesian Methods for Machine Learning Week3. <span id="more"></span></p>
<hr />
<p>本文主要是关于变分推断, 或者说<a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">变分贝叶斯方法</a>, 它是一类用来计算难以计算积分的方法. 本文的定位是对EM算法的有益补充, 因此会结合EM算法去讲一讲什么是变分推断及平均场近似, 并得到EM算法的三种变体.</p>
<p>关于EM算法可以参见我的另一篇博文<a href="https://msgsxj.cn/2018/09/02/EM%E7%AE%97%E6%B3%95/">EM算法</a>, 简单的说,EM算法是专门处理隐变量模型的算法. 下文我们会以隐狄利克雷分配模型(Latent Dirichlet Allocation: LDA)为例来说明本文的主角, 当然这是一个隐变量模型.</p>
<hr />
<h2 id="隐变量模型">隐变量模型</h2>
<p><a href="https://en.wikipedia.org/wiki/Latent_variable_model">隐变量模型</a>, 隐变量模型含有未观测到值的变量, 且该隐变量解释了观测到的变量之间的相关性(<a href="https://en.wikipedia.org/wiki/Local_independence">局部独立性</a>), 简言之引进的隐变量不能没有用, 举个例子来说明这个性质(来自维基百科):</p>
<table>
<thead>
<tr class="header">
<th>本科生</th>
<th>读过机器学习</th>
<th>没读过机器学习</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>读过凸优化</strong></td>
<td>260</td>
<td>140</td>
</tr>
<tr class="even">
<td><strong>没读过凸优化</strong></td>
<td>240</td>
<td>360</td>
</tr>
</tbody>
</table>
<p>容易看到读过机器学习的概率是<span class="math inline">\(\frac{500}{1000}=0.5\)</span>, 读过凸优化的概率是<span class="math inline">\(\frac{400}{1000}=0.4\)</span>, 但是两者都读过的概率是<span class="math inline">\(\frac{260}{1000}=0.26\)</span>, 不等于<span class="math inline">\(\frac{500}{1000}\times\frac{400}{1000}=0.2\)</span>, 因此并不独立. 若我们引进变量本科高年级生和本科低年级生:</p>
<table>
<thead>
<tr class="header">
<th>高年级/低年级</th>
<th>读过机器学习</th>
<th>没读过机器学习</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>读过凸优化</strong></td>
<td>240/20</td>
<td>60/80</td>
</tr>
<tr class="even">
<td><strong>没读过凸优化</strong></td>
<td>160/80</td>
<td>40/320</td>
</tr>
</tbody>
</table>
<p>就会发现, 对高年级生<span class="math inline">\(\frac{400}{500}\times\frac{300}{500}=\frac{240}{500}\)</span>; 对低年级生<span class="math inline">\(\frac{100}{500}\times\frac{100}{500}=\frac{20}{500}\)</span>, 此时相关性已经被隐变量解释掉了.</p>
<hr />
<h2 id="隐狄利克雷分配模型">隐狄利克雷分配模型</h2>
<p>下面举个隐变量模型的例子, 隐狄利克雷分配模型. 假如我读过东野圭吾的新参者, 我希望推荐系统能把白夜行也推荐给我. 这里直接给出模型的完整形式, 具体怎么来的请见博文<a href="https://msgsxj.cn/2018/10/16/Latent-Dirichlet-Allocation/">Latent Dirichlet Allocation</a>的第一节. <span class="math display">\[P(W, Z, \Theta\mid \varphi, \alpha)=\prod_{d=1}^{D}P(\theta_d)\prod_{n=1}^{N_d}P(z_{dn}\mid\theta_d)P(w_{dn}\mid z_{dn})\]</span>其中:</p>
<ul>
<li><span class="math inline">\(\theta_d\sim Dir(\alpha)=\frac{1}{B(\alpha)}\prod_{t=1}^{T}\theta_{dt}^{\alpha_t-1}\)</span>, 其中<span class="math inline">\(d\in\{1,2,...,D\}\)</span>, <span class="math inline">\(T\)</span>给定.</li>
<li><span class="math inline">\(P(z_{dn}\mid\theta_d)=\theta_{dz_{dn}}\)</span>, 其中<span class="math inline">\(n\in\{1,2,...,N_d\}\)</span>.</li>
<li><span class="math inline">\(P(w_{dn}\mid z_{dn})=\varphi_{w_{dn}z_{dn}}\)</span>, 给定词汇表<span class="math inline">\(w_{set}\)</span>, <span class="math inline">\(w_{dn}\in\{1,2,...,\left|w_{set}\right|\}\)</span>.</li>
</ul>
<p>大小字母表示小写字母的总和, 参数仍简记为小写字母, 总结如下:</p>
<ul>
<li>未知:<span class="math inline">\(Z, \Theta\)</span>为隐变量; <span class="math inline">\(\varphi, \alpha\)</span>未知参数.</li>
<li>已知:<span class="math inline">\(W\)</span>为数据集, <span class="math inline">\(D\)</span>为文档数量, <span class="math inline">\(N_d\)</span>为每篇文档单词数.</li>
<li>超参:主题数<span class="math inline">\(T\)</span>, 词汇表<span class="math inline">\(w_{set}\)</span>(可以取为这些文档出现过的单词组成的集合).</li>
</ul>
<hr />
<h2 id="变分贝叶斯方法">变分贝叶斯方法</h2>
<h3 id="em算法">EM算法</h3>
<p>下面尝试用4种方法来解决这个模型, 首先从我们最熟悉的EM算法开始(不熟悉的话可以参见我的另一篇博文<a href="https://msgsxj.cn/2018/09/02/EM%E7%AE%97%E6%B3%95/">EM算法</a>): EM算法的目标是极大化对数似然函数<span class="math inline">\(\log P(W\mid\varphi, \alpha)\)</span>, 且每一步迭代又给出隐变量<span class="math inline">\(Z, \Theta\)</span>的概率分布以及<span class="math inline">\(\varphi, \alpha\)</span>的点估计. 第k+1次迭代, 当前参数为<span class="math inline">\(\varphi^k, \alpha^k\)</span>:</p>
<ul>
<li>E步求期望:求<span class="math inline">\(\mathbb{E}_{q^{k+1}}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span>, 其中<span class="math inline">\(q^{k+1}(Z, \Theta)=P(Z, \Theta\mid W, \varphi^k, \alpha^k)\)</span>.</li>
<li>M步求max: <span class="math inline">\((\varphi^{k+1}, \alpha^{k+1})=\arg\max_{(\varphi, \alpha)}\mathbb{E}_{q^{k+1}}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span>.</li>
</ul>
<p><strong>p.s.</strong> EM算法的迭代过程没有涉及参数<span class="math inline">\(\varphi, \alpha\)</span>分布, 且没有引进先验分布, 因而仍然算是频率学派的方法, 下面来看一种明显是属于贝叶斯学派的方法:变分推断</p>
<h3 id="变分推断">变分推断</h3>
<p>根据贝叶斯思想, 我们将参数<span class="math inline">\(\varphi, \alpha\)</span>视为随机变量, 因此必须给出参数<span class="math inline">\(\varphi, \alpha\)</span>的先验假设<span class="math inline">\(P(\varphi), P(\alpha)\)</span>, 如果能直接算出隐变量及未知参数<span class="math inline">\(Z, \Theta, \varphi, \alpha\)</span>的概率分布<span class="math inline">\(P(Z, \Theta, \varphi, \alpha\mid W)\)</span>, 那么就能很方便的得到想要的参数值(比如说通过极大后验分布MAP). 事实上, 我们已经有了<span class="math inline">\(P(W, Z, \Theta\mid \varphi, \alpha)\)</span>, 根据贝叶斯公式:<span class="math display">\[P(Z, \Theta, \varphi, \alpha\mid W)=\frac{P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)}{P(W)}=\frac{P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)}{\int_{Z, \Theta, \varphi, \alpha} P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)}\]</span>通过分析不难发现, 如果给出的参数<span class="math inline">\(\varphi, \alpha\)</span>的先验恰为共轭先验, 那么上式就非常好求, 否则就不可避免的要计算<span class="math inline">\(P(W)\)</span>, 而这事实上相当难处理.</p>
<p>变分推断则是试图绕过求<span class="math inline">\(P(W)\)</span>, 去找一个对<span class="math inline">\(P(Z, \Theta, \varphi, \alpha\mid W)\)</span>的良好逼近<span class="math inline">\(q(Z, \Theta, \varphi, \alpha)\)</span>, 其中<span class="math inline">\(q(Z, \Theta, \varphi, \alpha)\)</span>会来自一个简单得多的分布族, 比如<span class="math inline">\(Q=N(\mu, \Sigma)\)</span>, 其中<span class="math inline">\(\Sigma\)</span>为对角阵; 而逼近的方法一般则是最小化<span class="math inline">\(\mathcal{KL}\)</span>散度:<span class="math display">\[q=\arg\min_{q\in Q}\mathcal{KL}(q(Z, \Theta, \varphi, \alpha)\Vert P(Z, \Theta, \varphi, \alpha\mid W))\]</span>关于<span class="math inline">\(\mathcal{KL}\)</span>散度的定义见<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">维基百科</a>, 这里简单理解为衡量两个分布的距离就好. 下式告诉我们, 变分推断确实可以绕开<span class="math inline">\(P(W)\)</span>, 下面把<span class="math inline">\(q(Z, \Theta, \varphi, \alpha)\)</span>简记为<span class="math inline">\(q\)</span>: <span class="math display">\[\begin{aligned} \mathcal{KL}(q\Vert P(Z, \Theta, \varphi, \alpha\mid W)) &amp; =  \mathcal{KL}(q\Vert \frac{P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)}{P(W)}) \\\
&amp; =\int  q\log \frac{q}{P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)/P(W)} dZd \Theta d\varphi d\alpha\\\
&amp; = \int \left( q\log \frac{q}{P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha)}dZd \Theta  +  q\log P(W) \right)dZd \Theta d\varphi d\alpha\\\
&amp; =  \mathcal{KL}(q\Vert P(W, Z, \Theta\mid \varphi, \alpha)P(\varphi, \alpha))+\log P(W) \end{aligned}\]</span>其中第一个等号为简单带入, 第二个等号由<span class="math inline">\(\mathcal{KL}\)</span>散度的定义, 第三个等号为简单化简, 第四个等号由<span class="math inline">\(\mathcal{KL}\)</span>散度定义. 容易看到, 最后一个等式的后一项<span class="math inline">\(\log P(W)\)</span>与<span class="math inline">\(q\)</span>无关, 因而成功绕开了<span class="math inline">\(P(W)\)</span>, 这便是变分推断.</p>
<p><strong>p.s.</strong> 变分推断需要给出参数的先验分布和分布族<span class="math inline">\(Q\)</span>. 如果<span class="math inline">\(Q\)</span>足够大, 则有<span class="math display">\[q(Z, \Theta, \varphi, \alpha)=P(Z, \Theta, \varphi, \alpha\mid W)\]</span>并没有简化计算. 所以这个分布族<span class="math inline">\(Q\)</span>的选取也是一件麻烦的事情, 既要考虑<span class="math inline">\(Z, \Theta\)</span>实际上是有关的, 又要考虑参数<span class="math inline">\(\varphi, \alpha\)</span>与隐变量<span class="math inline">\(Z, \Theta\)</span>是无关的. 一个合理的想法是预先假定<span class="math inline">\(\varphi, \alpha\)</span>与<span class="math inline">\(Z, \Theta\)</span>是无关的, 在变分推断的基础上于是有了平均场近似的变分推断:</p>
<h3 id="平均场近变分推断">平均场近变分推断</h3>
<p>其想法很简单, 在变分推断<span class="math display">\[q=\arg\min_{q\in Q}\mathcal{KL}(q(Z, \Theta, \varphi, \alpha)\Vert P(Z, \Theta, \varphi, \alpha\mid W))\]</span>的基础上, 预先假定参数<span class="math inline">\(\varphi, \alpha\)</span>与隐变量<span class="math inline">\(Z, \Theta\)</span>的独立性, 得到 <span class="math display">\[\begin{aligned}q(Z, \Theta)&amp;=\arg\min_{q(Z, \Theta)\in Q_1}\mathcal{KL}(q(Z, \Theta) q(\varphi, \alpha)\Vert P(Z, \Theta, \varphi, \alpha\mid W)) \\\
q(\varphi, \alpha)&amp;=\arg\min_{q(\varphi, \alpha)\in Q_2}\mathcal{KL}(q(Z, \Theta)q(\varphi, \alpha)\Vert P(Z, \Theta, \varphi, \alpha\mid W))\end{aligned}\]</span>这样计算上有了一定程度的减小, 当然准确度也有可能有一定的损失, 但其实际上不失为一个好的假设. 下面来看平均长近似后的形式, 为了表述方便, 记<span class="math inline">\(q(Z, \Theta)=q_1\)</span>, <span class="math inline">\(q(\varphi, \alpha)=q_2\)</span>, <span class="math inline">\(P(Z, \Theta, \varphi, \alpha\mid W)=\hat{P}\)</span>, <span class="math inline">\(dZd \Theta d\varphi d\alpha=d\hat{Z}\)</span>, 目标则简化为<span class="math display">\[q_i=\arg\min_{q_i\in Q_i}\mathcal{KL}(\prod_{i=1}^2 q_i\Vert \hat{P})\]</span>下面来看, 目标是对<span class="math inline">\(q_k\in Q_k\)</span>求最小值点, <span class="math inline">\(k=1, 2\)</span>: <span class="math display">\[\begin{aligned} \mathcal{KL}(\prod_{i=1}^2 q_i\Vert \hat{P}) &amp; =  \int  \prod_{j=1}^2 q_j\log \frac{\prod_{i=1}^2 q_i}{\hat{P}} d\hat{Z} \\\
&amp; =\sum_{i=1}^2\int   \prod_{j=1}^2q_j\log q_i d\hat{Z}-\int   \prod_{j=1}^2q_j\log \hat{P}d\hat{Z}\\\
&amp; = \int   \prod_{j=1}^2q_j\log q_k d\hat{Z}+\sum_{i\neq k}^2\int   \prod_{j=1}^2q_j\log q_i d\hat{Z}-\int   \prod_{j=1}^2q_j\log \hat{P} d\hat{Z} \\\
&amp;=\int   q_k\log q_k d\hat{Z}_k+\sum_{i\neq k}^2\int   \prod_{j\neq k}^2q_j\log q_i d\hat{Z}_{-k}-\int   \prod_{j=1}^2q_j\log \hat{P} d\hat{Z} \\\
&amp;= \int   q_k\log q_k d\hat{Z}_k-\int q_k\left[\int   \prod_{j\neq k}^2q_j\log \hat{P} d\hat{Z}_{-k}\right]d\hat{Z}_k + C\\\
&amp;=\int   q_k\left[\log q_k -\int   \prod_{j\neq k}^2q_j\log \hat{P} d\hat{Z}_{-k}\right]d\hat{Z}_k + C \end{aligned}\]</span>这里的<span class="math inline">\(d\hat{Z}\)</span>为<span class="math inline">\(dZd \Theta d\varphi d\alpha\)</span>, <span class="math inline">\(d\hat{Z}_k\)</span>为对<span class="math inline">\(q_k\)</span>的变量积分, 比方说<span class="math inline">\(k=1\)</span>, 即为<span class="math inline">\(dZ d\Theta\)</span>, <span class="math inline">\(d\hat{Z}_{-k}\)</span>为对除<span class="math inline">\(q_k\)</span>的之外的变量积分.</p>
<p>稍作解释:第一个等号由<span class="math inline">\(\mathcal{KL}\)</span>散度的定义; 第二个等号为简单化简; 第三个等号为将第一项拆成<span class="math inline">\(i=k\)</span>和<span class="math inline">\(i\neq k\)</span>两项; 第四个等号为对第一第二项中能积分的先积掉; 第五个等号为将第三项对<span class="math inline">\(q_k\)</span>的积分拎到最前面, 且此时第二项与<span class="math inline">\(q_k\)</span>无关, 视为常数<span class="math inline">\(C\)</span>; 第六个等号即为简单合并.</p>
<p>下面继续, 把我们把<span class="math inline">\(\int \prod_{j\neq k}^2q_j\log \hat{P} d\hat{Z}_{-k}=\mathbb{E}_{q_{-k}}\log \hat{P}\)</span>记作<span class="math inline">\(h(\hat{Z}_k)\)</span>, 再令<span class="math inline">\(t(\hat{Z}_k)=\frac{exp(h(\hat{Z}_k))}{\int exp(h(\hat{Z}_k))d\hat{Z}_k}\)</span>, 除以正则项确保是个概率分布, 上式可继续化为: <span class="math display">\[\begin{aligned} \mathcal{KL}(\prod_{i=1}^2 q_i\Vert \hat{P}) &amp; =  \int   q_k\left[\log q_k -\int   \prod_{j\neq k}^2q_j\log \hat{P} d\hat{Z}_{-k}\right]d\hat{Z}_k + C\\\
&amp; =  \int   q_k\left[\log q_k d\hat{Z}_k-h(\hat{Z}_k)\right]d\hat{Z}_k + C\\\
&amp;=\int   q_k\log \frac{q_k}{t} d\hat{Z}_k + C \end{aligned}\]</span>注意到第一项即为KL散度, 那么此时关于<span class="math inline">\(q_k\)</span>取极小只需令<span class="math inline">\(q_k=t=\frac{exp(h(\hat{Z}_k))}{\int exp(h(\hat{Z}_k))d\hat{Z}_k}\)</span>, 其中<span class="math inline">\(h(\hat{Z}_k)=\mathbb{E}_{q_{-k}}\log \hat{P}\)</span>再取对数得到<span class="math display">\[\log q_k=\mathbb{E}_{q_{-k}}\log \hat{P}+C\]</span>其中<span class="math inline">\(\hat{P}=P(Z, \Theta, \varphi, \alpha\mid W)\)</span>在LDA模型这个例子中, 上述结果化为: <span class="math display">\[\begin{aligned}\log q(Z, \Theta)&amp;=\mathbb{E}_{q(\varphi, \alpha)}\log P(Z, \Theta, \varphi, \alpha\mid W)+C_1  \\\
\log q(\varphi, \alpha)&amp;=\mathbb{E}_{q(Z, \Theta)}\log P(Z, \Theta, \varphi, \alpha\mid W)+C_2\end{aligned}\]</span></p>
<p><strong>p.s.</strong> 相比变分推断, 这个平均场近似的变分推断实际是将参数<span class="math inline">\(\varphi, \alpha\)</span>与隐变量<span class="math inline">\(Z, \Theta\)</span>进行了分开处理, 降低了模型复杂度, 同时明显也降低了计算的维度, 但由于对参数<span class="math inline">\(\varphi, \alpha\)</span>与隐变量<span class="math inline">\(Z, \Theta\)</span>的独立性假设非常合理, 所以也不会有太多精度上的损失, 甚至会使得模型更稳健. 另外顺便得到了一个非常有用的式子, 这个式子能在<a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">变分贝叶斯方法</a> 的维基百科的Mean field approximation一节找到, 只是没有上述的推导过程.</p>
<p>EM算法也是将参数<span class="math inline">\(\varphi, \alpha\)</span>与隐变量<span class="math inline">\(Z, \Theta\)</span>进行了分开处理, 并且对隐变量<span class="math inline">\(Z, \Theta\)</span>同样是通过对<span class="math inline">\(\mathcal{KL}\)</span>散度最小化求得分布, 而EM算法对未知参数则是直接采用了点估计的方法而不是变分推断. 因此相比平均场近似的变分推断, EM算法进一步降低了计算复杂度, , 我想这大概就是EM算法流行的理由之一.</p>
<h3 id="平均场em算法">平均场EM算法</h3>
<p>平均场EM算法实际上是在EM算法基础上进一步对隐变量<span class="math inline">\(Z, \Theta\)</span>进行了独立性假设. 下面分两步在EM算法的基础上推导出平均场EM算法:</p>
<ol type="1">
<li>EM算法: 第k+1次迭代, 当前参数为<span class="math inline">\(\varphi^k, \alpha^k\)</span>:
<ul>
<li>E步: <span class="math inline">\(q^{k+1}(Z, \Theta)=\arg\min_{q(Z, \Theta)\in Q_1}\mathcal{KL}(q(Z, \Theta)\Vert P(Z, \Theta\mid W, \varphi, \alpha))\)</span>.</li>
<li>M步: <span class="math inline">\((\varphi^{k+1}, \alpha^{k+1})=\arg\max_{(\varphi, \alpha)}\mathbb{E}_{q^{k+1}}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span>.</li>
</ul></li>
<li>假定<span class="math inline">\(q(Z, \Theta)=q(Z)q(\Theta)\)</span>, 由平均场近似得到的公式, 得到平均场EM算法: 第k+1次迭代, 当前参数为<span class="math inline">\(\varphi^k, \alpha^k\)</span>:
<ul>
<li>E步:<span class="math inline">\(\begin{aligned}\log q^{k+1}(Z)&amp;=\mathbb{E}_{q(\Theta)}\log P(Z, \Theta\mid \varphi^k, \alpha^k, W)+C_1 \\\
\log q^{k+1}(\Theta)&amp;=\mathbb{E}_{q(Z)}\log P(Z, \Theta\mid \varphi^k, \alpha^k, W)+C_2\end{aligned}\)</span></li>
<li>M步: <span class="math inline">\((\varphi^{k+1}, \alpha^{k+1})=\arg\max_{(\varphi, \alpha)}\mathbb{E}_{q^{k+1}(Z, \Theta)}\log P(W, Z, \Theta\mid\varphi, \alpha)\)</span></li>
</ul></li>
</ol>
<p><strong>p.s.</strong> 平均场EM算法比EM算法多了一个隐变量之间独立的假设, 但一般隐变量之间应该不独立, 比方说这个LDA模型中<span class="math inline">\(Z\)</span>代表每篇文档每个单词的主题分布, <span class="math inline">\(\Theta\)</span>代表每篇文档的主题分布, 两者应该不独立, 所以平均场EM算法不适合这个例子. 另外相比EM算法, 平均场EM算法因为多了一个独立性假设, 模型应该会变得简单一些, 但精度有可能会有些损失.</p>
<h3 id="总结">总结</h3>
<p>最后用一张表格总结上述4中算法:</p>
<table>
<thead>
<tr class="header">
<th>算法</th>
<th>模型复杂度</th>
<th>需要的计算资源</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>变分推断</strong></td>
<td>4</td>
<td>4</td>
</tr>
<tr class="even">
<td><strong>平均场变分推断</strong></td>
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td><strong>EM算法</strong></td>
<td>2</td>
<td>2</td>
</tr>
<tr class="even">
<td><strong>平均场EM算法</strong></td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>EM算法作为其中比较折中的选择, 兼顾了模型复杂度与计算资源.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LDA模型</tag>
        <tag>变分贝叶斯</tag>
        <tag>变分推断</tag>
        <tag>平均场近似</tag>
        <tag>KL散度</tag>
      </tags>
  </entry>
  <entry>
    <title>拉德马赫复杂度</title>
    <url>/2018/11/09/%E6%8B%89%E5%BE%B7%E9%A9%AC%E8%B5%AB%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
    <content><![CDATA[<p>前言: 本文主要讲了拉德马赫复杂度(<a href="https://en.wikipedia.org/wiki/Rademacher_complexity">Rademacher complexity</a>), 主要参考了Understanding machine learning一书26章.</p>
<span id="more"></span>
<hr />
<p>首先应当接触以下几个概念:</p>
<ul>
<li>样本空间: 对于监督学习而言为带标签的样本<span class="math inline">\((x,y)\)</span>的取值空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>, 学习的目标是映射<span class="math inline">\(f: \mathcal{X}\to\mathcal{Y}\)</span>, 以用于产生对新样本<span class="math inline">\(x_{new}\)</span>的标签的可靠预测<span class="math inline">\(f(x_{new})\)</span>. <code>p.s.</code>对无监督学习而言为不带标签的样本<span class="math inline">\(x\)</span>的取值空间<span class="math inline">\(\mathcal{X}\)</span>, 学习目标为对<span class="math inline">\(\mathcal{X}\)</span>的可靠分类.</li>
<li>目标函数: 通常记为<span class="math inline">\(f\)</span>, 即上述监督学习中所说的映射<span class="math inline">\(f\)</span>. 对于分类问题就是我们希望得到的真实的分类函数.</li>
<li>训练集: 通常记为<span class="math inline">\(S\)</span>, 由定义在样本空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>上的概率分布<span class="math inline">\(\mathcal{D}\)</span>抽样得到 <span class="math display">\[S=\{(x_{1},y_{1}),....,(x_{m},y_{m})\}\]</span></li>
<li>学习算法: 通常记为<span class="math inline">\(A\)</span>, 即我们构建的、能从数据中学到东西的学习算法, 期望其能从假设类中返回良好的假设.</li>
<li>假设类: 通常记为<span class="math inline">\(\mathcal{H}\)</span>, 对目标函数<span class="math inline">\(f\)</span>的假设构成的空间, 学习器从中选择.</li>
<li>预测器(predictor): 或者说分类器(classifier)、假设(hypothesis), 通常记为<span class="math inline">\(h\)</span>, 即学习器输出的一个函数:<span class="math inline">\(h:\mathcal{X}\to\mathcal{Y}\)</span>, 且应当和目标函数<span class="math inline">\(f\)</span>有相近的函数值.</li>
<li>损失函数: 通常记为<span class="math inline">\(l(h, (x, y))\)</span>, 其定义在预测器<span class="math inline">\(h\)</span>和单个样本<span class="math inline">\((x, y)\)</span>上. 对于二分类问题, 一个常用的损失函数为<span class="math inline">\(l(h, (x, y))=I\{y\neq h(x)\}\)</span>, 即分对了就为0, 分错了就为1.</li>
<li>训练误差: 通常记为<span class="math inline">\(L_{S}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在训练集<span class="math inline">\(S\)</span>上的平均损失<span class="math display">\[L_{S}(h)=\frac{1}{m}\sum_{i=1}^m l(h, (x_i, y_i))\]</span></li>
<li>泛化误差: 通常记为<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在<span class="math inline">\((\mathcal{X}, \mathcal{Y})\)</span>上的平均损失<span class="math display">\[\mathbb{E}_{(x, y)\sim (\mathcal{X}, \mathcal{Y})}l(h, (x, y))\]</span></li>
<li><span class="math inline">\(\varepsilon\)</span>-代表性样本: 一个训练集被称为<span class="math inline">\(\varepsilon\)</span>-代表性样本(定义在域<span class="math inline">\(Z\)</span>, 假设类<span class="math inline">\(\mathcal{H}\)</span>, 损失函数<span class="math inline">\(l\)</span>和分布<span class="math inline">\(\mathcal{D}\)</span>上), 如果满足<span class="math display">\[\sup_{h\in\mathcal{H}}|L_{S}(h)-L_{\mathcal{D}}(h)|\le\varepsilon\]</span></li>
</ul>
<p>接着应当知道如下基本事实:</p>
<blockquote>
<p>如果<span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon/2\)</span>-代表性样本, 那么在ERM准则下它是<span class="math inline">\(\varepsilon\)</span>-一致的, 即<span class="math display">\[L_{\mathcal{D}}(ERM_{\mathcal{H}}(S))\le \min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)+\varepsilon\]</span></p>
</blockquote>
<hr />
<h3 id="拉德马赫复杂度">拉德马赫复杂度</h3>
<p>回顾之前针对二分类问题建立的<a href="https://msgsxj.cn/2018/06/14/VC-bound/">VC bound</a>:<span class="math display">\[
\sup_{h\in\mathcal{H}}\left|L_{S}(h)-L_{\mathcal{D}}(h)\right|\le \sqrt{\frac{8}{m}\log\left(\frac{4}{\delta}\right)+\frac{8d_{\mathcal{H}}}{m}\log\left(\frac{2em}{d_\mathcal{H}}\right)}
\]</span>注意到要定义训练误差与泛化误差必须要定义损失函数, 损失函数实际上选定了二分类问题的常用的损失函数0-1损失: <span class="math inline">\(l(h, (x, y))=I\{y\neq h(x)\}\)</span>; 因此上述VC bound又可以写作:<span class="math display">\[
\sup_{f\in\mathcal{F}}\left|L_{S}(f)-L_{\mathcal{D}}(f)\right|\le \sqrt{\frac{8}{m}\log\left(\frac{4}{\delta}\right)+\frac{8d_{\mathcal{H}}}{m}\log\left(\frac{2em}{d_\mathcal{H}}\right)}
\]</span>这里的<span class="math inline">\(\mathcal{F}\)</span>定义为<span class="math display">\[l \circ \mathcal{H}=\{z\mapsto l(h, z):h\in\mathcal{H}\}\]</span> 相应的<span class="math inline">\(L_{S}(f)=\frac{1}{m}\sum_{i=1}^m f(z_i)\)</span>, <span class="math inline">\(L_{\mathcal{D}}(f)=\mathbb{E}_{z\sim \mathcal{D}}f(z)\)</span>, 当然这里的<span class="math inline">\(l\)</span>已经固定为0-1损失. 定义训练集<span class="math inline">\(S\)</span>在<span class="math inline">\(\mathcal{F}\)</span>上的代表性为:<span class="math display">\[Rep_{\mathcal{D}}(\mathcal{F}, S)\doteq\sup_{f\in\mathcal{F}}(L_{\mathcal{D}}(f)-L_{S}(f))\]</span>一般来说, 训练误差<span class="math inline">\(L_{S}\)</span>能通过优化达到一个非常接近于0的数值, 比如<a href="https://arxiv.org/abs/1811.03804">这篇文章</a>说明了用梯度下降训练ResNet能在多项式时间内达到0训练误差, 即达到全局最优; 泛化误差<span class="math inline">\(L_{\mathcal{D}}\)</span>则会略大于训练误差. 显然当<span class="math inline">\(S\)</span>越能代表<span class="math inline">\(\mathcal{D}\)</span>, 上数值会越接近于0. 现在想要仅仅依靠训练集<span class="math inline">\(S\)</span>本身去估计其代表性, 一个自然地想法是将<span class="math inline">\(S\)</span>分成两个不相交的子集<span class="math inline">\(S=S_1\cup S_2\)</span>, <span class="math inline">\(S_1\)</span>作为验证集, <span class="math inline">\(S_2\)</span>作为训练集, 此时训练集代表性的一个近似为:<span class="math display">\[\sup_{f\in\mathcal{F}}(L_{\mathcal{S_1}}(f)-L_{S_2}(f))\]</span>再设多元随机变量<span class="math display">\[\sigma=(\sigma_1, \sigma_2,...,\sigma_m)\in\{\pm 1\}^m\]</span> 并按如下方式取定随机变量的值<span class="math inline">\(S_1=\{z_i:\sigma_i=1\}\)</span>, <span class="math inline">\(S_2=\{z_i:\sigma_i=-1\}\)</span>, 上式可写成更简单的形式:<span class="math display">\[\frac{1}{m}\sup_{f\in\mathcal{F}}\sum_{i=1}^m \sigma_i f(z_i)\]</span> 此时随机变量的不同的取值代表不同的验证集和训练集的组合, <strong>拉德马赫复杂度</strong>定义为上式近似对随机变量<span class="math inline">\(\sigma\)</span>的期望:<span class="math display">\[R(\mathcal{F}\circ S)\doteq\frac{1}{m}\mathop{\mathbb{E}}_{\sigma\sim\{\pm 1\}^m} [\sup_{f\in\mathcal{F}}\sum_{i=1}^m \sigma_i f(z_i)]\]</span>可以证明, 在期望意义下, 集合<span class="math inline">\(S\)</span>的代表性不超过2倍的拉德马赫复杂度<span class="math display">\[\mathop{\mathbb{E}}_{S\sim\mathcal{D}^m}[Rep_{\mathcal{D}}(\mathcal{F}, S)]\le 2\mathop{\mathbb{E}}_{S\sim\mathcal{D}^m}[R(\mathcal{F}\circ S)]\]</span>证明见Understanding machine learning一书引理26.2.</p>
<hr />
<h3 id="数据相关界">数据相关界</h3>
<p>首先来看一个重要的不等式, 有限偏差集中不等式, 又称<a href="https://en.wikipedia.org/wiki/Doob_martingale#McDiarmid&#39;s_inequality">McDiarmid 不等式</a>, 这个不等式可见维基, 这个不等式可用来推出一个与训练集<span class="math inline">\(S\)</span>相关的界:</p>
<blockquote>
<p>假设对所有的<span class="math inline">\(z\)</span>和<span class="math inline">\(h\in\mathcal{H}\)</span>, <span class="math inline">\(|l(h, z)|\le c\)</span>, 那么以至少<span class="math inline">\(1-\delta\)</span>的概率, 对所有的<span class="math inline">\(h\in\mathcal{H}\)</span>有<span class="math display">\[L_{\mathcal{D}}(h)-L_{S}(h)\le 2\mathop{\mathbb{E}}_{S\sim\mathcal{D}^m}[R(l \circ \mathcal{H}\circ S)]+c\sqrt{\frac{2\log(2/\delta)}{m}}\]</span>以至少<span class="math inline">\(1-\delta\)</span>的概率, 对所有的<span class="math inline">\(h\in\mathcal{H}\)</span>有<span class="math display">\[L_{\mathcal{D}}(h)-L_{S}(h)\le 2R(l \circ \mathcal{H}\circ S)+4c\sqrt{\frac{2\log(4/\delta)}{m}}\]</span> 以至少<span class="math inline">\(1-\delta\)</span>的概率, 对所有的<span class="math inline">\(h\in\mathcal{H}\)</span>有<span class="math display">\[L_{\mathcal{D}}(ERM_{\mathcal{H}}(S))-L_{\mathcal{D}}(h)\le 2R(l \circ \mathcal{H}\circ S)+5c\sqrt{\frac{2\log(8/\delta)}{m}}\]</span></p>
</blockquote>
<p>有兴趣的朋友不妨试着去推一推, 上述1, 2的界的推导类似于西瓜书定理12.5, 唯一不同的是这里满足在常数为<span class="math inline">\(2c/m\)</span>下的有界偏差条件, 而西瓜书直接为1; 第三个不等式则是多一步霍夫丁不等式.</p>
<ul>
<li>注意到第2, 3个界实际上由具体的训练集<span class="math inline">\(S\)</span>计算出来的, 因此这两个界被称为是数据相关界, 这也是与VC bound最大的不同之处.</li>
<li>第二个界可以用来评估当前分类器<span class="math inline">\(h\)</span>在实际数据集<span class="math inline">\(\mathcal{D}\)</span>上的表现, 即<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>.</li>
<li>第三个界可以用来评估利用ERM原则得到的分类器<span class="math inline">\(h\)</span>与理论最佳分类器的实际差距, 因此希望这个界越小越好; 显然假设类<span class="math inline">\(\mathcal{H}\)</span>越复杂, 这个界越大, 因此某种意义上这个界能用来评估假设类<span class="math inline">\(\mathcal{H}\)</span>的优劣.</li>
</ul>
<hr />
<h3 id="拉德马赫复杂度的性质">拉德马赫复杂度的性质</h3>
<p>首先给出拉德马赫复杂度的更一般化定义: 给定一个向量集合<span class="math inline">\(A\in\mathbb{R}^m\)</span> <span class="math display">\[R(A)\doteq\frac{1}{m}\mathop{\mathbb{E}}_{\sigma\sim\{\pm 1\}^m} [\sup_{a\in A}\sum_{i=1}^m \sigma_i a_i]\]</span>这里例举拉德马赫复杂度的几条性质, 具体见Understanding machine learning一书26章:</p>
<ul>
<li>A的凸包与A具有一样的拉德马赫复杂度.</li>
<li>一个有限集合的拉德马赫复杂度随着集合大小增长而对数增长.</li>
<li>如果给集合A作用一个Lipschitz连续的函数, 并不会增大其拉德马赫复杂度.</li>
</ul>
<hr />
<h3 id="线性类的拉德马赫复杂度">线性类的拉德马赫复杂度</h3>
<p>本节给出两个线性的假设类的拉德马赫复杂度, 假定<span class="math inline">\(x\)</span>的维数为<span class="math inline">\(n\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}_1=\{x\mapsto&lt;\omega, x&gt;:\Vert\omega\Vert_1\le 1\}\)</span></li>
<li><span class="math inline">\(\mathcal{H}_2=\{x\mapsto&lt;\omega, x&gt;:\Vert\omega\Vert_2\le 1\}\)</span></li>
</ul>
<p><span class="math inline">\(\mathcal{H}_1\circ S\)</span>定义为<span class="math inline">\(\{(&lt;\omega, x_i&gt;, i=1...,m):\Vert\omega\Vert_1\le 1\}\)</span>, <span class="math inline">\(\mathcal{H}_2\circ S\)</span>类似的定义, 有如下两个结果, 证明见[UML]一书引理26.10、26.11:</p>
<ul>
<li><span class="math inline">\(R(\mathcal{H}_1\circ S) \le \max_i\Vert x_i\Vert_{\infty}\sqrt{\frac{2\log(2n)}{m}}\)</span></li>
<li><span class="math inline">\(R(\mathcal{H}_2\circ S) \le\frac{\max_i\Vert x_i\Vert_2}{\sqrt{m}}\)</span></li>
</ul>
<p><strong>p.s.</strong> 此外还有软硬间隔SVM的泛化误差界的推导, 见26.3, 26.4; 另一种度量集合复杂度的方法为覆盖数, 见27章.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>拉德马赫复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title>凸优化学习笔记</title>
    <url>/2018/02/23/%E5%87%B8%E4%BC%98%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>前言:主要参考了凸优化理论一书及讨论班上的ppt, <span class="math inline">\(\mathbb{R}\)</span>表示实数域. <span id="more"></span></p>
<h1 id="基本概念">基本概念</h1>
<h2 id="凸集与凸函数">凸集与凸函数</h2>
<h3 id="凸集">凸集</h3>
<p>凸集:</p>
<blockquote>
<p><span class="math inline">\(\mathbb{R}^{n}\)</span>的子集<span class="math inline">\(C\)</span>被称为凸集,如果其满足<span class="math inline">\(\alpha x+(1-\alpha)y\in C,\forall x,y\in C,\forall \alpha \in [0,1]\)</span>.</p>
</blockquote>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Convex_polygon_illustration1.svg/153px-Convex_polygon_illustration1.svg.png" /></p>
<p>锥体(cone):</p>
<blockquote>
<p><span class="math inline">\(0\in C\)</span>,如果对 <span class="math inline">\(\forall x \in C , \forall \lambda &gt; 0, \lambda x \in C\)</span> ,则称<span class="math inline">\(C\)</span>为锥体.</p>
</blockquote>
<h4 id="保持集合凸性的运算">保持集合凸性的运算</h4>
<p>凸集在以下几种运算下能保持凸性:</p>
<ul>
<li>任意多个凸集的交集是凸集.</li>
<li>任意两个凸集的向量和是凸集.</li>
<li>对任意的凸集<span class="math inline">\(C\)</span>和标量<span class="math inline">\(\lambda\)</span>,集合<span class="math inline">\(\lambda C\)</span>是凸集.</li>
<li>凸集的闭包(closure)和内点集(interior)是凸集.</li>
<li>凸集在<a href="https://en.wikipedia.org/wiki/Affine_transformation">仿射函数</a>下的像和原像是凸集.</li>
</ul>
<h4 id="几个特殊的凸集">几个特殊的凸集</h4>
<p>下面是最常见的几个凸集的例子:</p>
<ul>
<li>超平面(hyperplane)形如<span class="math inline">\(\{x|a&#39;x=b\}\)</span>,其中<span class="math inline">\(a\)</span>为非零向量而<span class="math inline">\(b\)</span>为标量.</li>
<li>半空间(half space)形如<span class="math inline">\(\{x|a&#39;x\le b\}\)</span>,其中<span class="math inline">\(a\)</span>为非零向量而<span class="math inline">\(b\)</span>为标量.</li>
<li>多面体是有限个半空间的非空交集,形如<span class="math inline">\(\{x|a&#39;_{j}x≤b_{j},j=1,...,r\}\)</span>,其中<span class="math inline">\(a_{1},...,a_{r}\)</span>和<span class="math inline">\(b_{1},...,b_{r}\)</span>分别为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的一组向量和标量.</li>
<li>多面体锥(polyhedral cone)形如<span class="math inline">\(\{x|a&#39;_{j}x≤0,j=1,...,r\}\)</span>,其中<span class="math inline">\(a_{1},...,a_{r}\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的一组向量.</li>
</ul>
<h3 id="凸函数">凸函数</h3>
<h4 id="实值凸函数">实值凸函数</h4>
<p>凸函数：</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>的凸子集,称函数<span class="math inline">\(f:C\mapsto \mathbb{R}\)</span>为凸函数如果<span class="math display">\[f(\alpha x+(1-\alpha)x)\le \alpha f(x)+(1-\alpha)f(y),\forall x,y\in C,\forall \alpha \in[0,1]\]</span></p>
</blockquote>
<p>两个凸函数的典型例子是:</p>
<ul>
<li>仿射函数(affine function),这类函数形如<span class="math inline">\(f(x)=a&#39;x+b\)</span>,其中<span class="math inline">\(a\in \mathbb{R}^{n}\)</span>而<span class="math inline">\(b\in \mathbb{R}\)</span>.</li>
<li>一般的范数函数<span class="math inline">\(║║\)</span>,因其满足三角不等式,故均为凸函数.</li>
</ul>
<p>便捷起见,一般假设函数在定义域上处处取有限值域,但很多优化问题的实际情况中,目标函数尝尝取到无穷大,例如 <span class="math inline">\(f(x)=\sup \limits _ {i\in I} f_{i}(x)\)</span>;此外,还有一些函数仅仅定义在某凸子集上,例如<span class="math inline">\(f:(0,+\infty)\mapsto R\)</span>,无法把定义域拓展到整个<span class="math inline">\(\mathbb{R}\)</span>上,这时最方便的做法就是拓展的部分直接取无穷大.</p>
<h4 id="扩充实值函数">扩充实值函数</h4>
<p>基于上述原因,下面引入扩充实值函数的概念,首先引入几个定义:</p>
<h5 id="上图">上图</h5>
<p>考虑定义域为<span class="math inline">\(R^{n}\)</span>中的子集<span class="math inline">\(X\)</span>的函数<span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>,其上图是<span class="math inline">\(\mathbb{R}^{n+1}\)</span>的子集,其定义如下:</p>
<blockquote>
<p><span class="math inline">\(epi(f)=\{(x,\omega)|x\in X,\omega\in \mathbb{R},f(x)\le \omega\}\)</span>.</p>
</blockquote>
<p>这里引用维基上关于<a href="https://en.wikipedia.org/wiki/Epigraph">上图的定义</a>的一张图简单说明<span class="math inline">\(epi(f)\)</span>, 绿色部分就是<span class="math inline">\(epi(f)\)</span>.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Epigraph_convex.svg/512px-Epigraph_convex.svg.png" /></p>
<h5 id="有效定义域">有效定义域</h5>
<p>有效定义域(effective domain):</p>
<blockquote>
<p><span class="math inline">\(dom(f)=\{x\in X|f(x)&lt;\infty\}=\{x|\exists \omega\in \mathbb{R},s.t.(x,\omega)\in epi(f)\}\)</span></p>
</blockquote>
<h4 id="扩充实值凸函数">扩充实值凸函数</h4>
<p>扩充实值凸函数:</p>
<blockquote>
<p>令<span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>的凸子集,则扩充的实值函数<span class="math inline">\(f:C\mapsto [-\infty,+\infty]\)</span>称为凸函数当<span class="math inline">\(epi(f)\)</span>是<span class="math inline">\(\mathbb{R}^{n+1}\)</span>的凸子集.</p>
</blockquote>
<p>仔细比较与上文凸函数的定义不难发现:</p>
<ul>
<li><span class="math inline">\(f\)</span>的值域由<span class="math inline">\(\mathbb{R}\)</span>变成了<span class="math inline">\([-\infty,+\infty]\)</span>.</li>
<li>条件<span class="math inline">\(f(\alpha x+(1-\alpha)y)≤\alpha f(x)+(1-\alpha)f(y),\forall x,y\in C,\forall\alpha\in[0,1]\)</span>变成了当<span class="math inline">\(epi(f)\)</span>是<span class="math inline">\(\mathbb{R}^{n+1}\)</span>的凸子集.</li>
</ul>
<p>不难看出,若是按照上文的方法定义扩充实值凸函数,容易碰到<span class="math inline">\(\alpha f(x)+(1-\alpha)f(y)\)</span>为<span class="math inline">\(\infty\)</span>的情况.</p>
<p>** 至此,扩充实值函数的凸性与其上图的凸性的等价关系已经建立. **</p>
<h4 id="在c上的凸函数">在C上的凸函数</h4>
<p>出于严谨性,最后补充一种情况,有时还会遇到定义域<span class="math inline">\(C\)</span>非凸的函数,且若限制定义域为<span class="math inline">\(C\)</span>的凸子集,限制后的函数为凸函数,下面对这种情况的函数<span class="math inline">\(f\)</span>给出定义:</p>
<p>在<span class="math inline">\(C\)</span>上的凸函数:</p>
<blockquote>
<p><span class="math inline">\(C,X\)</span>为<span class="math inline">\(R^{n}\)</span>的子集其中<span class="math inline">\(C\subset X\)</span>,则称扩充的实值函数<span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>称为在<span class="math inline">\(C\)</span>上的凸函数,若<span class="math inline">\(f|_{C}\)</span>为凸函数.</p>
</blockquote>
<p>把定义域<span class="math inline">\(X\)</span>替换成<span class="math inline">\(C\)</span>,原函数也就是凸函数了,因此这类函数可直接视为凸函数,对其直接用实凸函数的结论.</p>
<h3 id="函数的闭性与半连续性">函数的闭性与半连续性</h3>
<h4 id="闭性">闭性</h4>
<p>闭函数:</p>
<blockquote>
<p>如果某个函数<span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>的上图是闭集,称<span class="math inline">\(f\)</span>为闭函数.</p>
</blockquote>
<p>至此,扩充实值函数的闭性与其上图的闭性的等价关系已经建立.</p>
<h4 id="下半连续性">下半连续性</h4>
<p>下半连续(lower semicontinuous):</p>
<blockquote>
<p>函数<span class="math inline">\(f\)</span>是在向量<span class="math inline">\(x\in X\)</span>处是下半连续的,如果<span class="math inline">\(f(x)\le \liminf\limits _ {k\to +\infty}f(x_{k})\)</span>对每个满足<span class="math inline">\(x_{k}\to x\)</span>的点列<span class="math inline">\(\{x_{k}\}\subset X\)</span>成立.</p>
</blockquote>
<h4 id="闭性与下半连续性的等价">闭性与下半连续性的等价</h4>
<p>下述定理把函数的下半连续性与闭性等价起来:</p>
<blockquote>
<p>对于函数<span class="math inline">\(f:\mathbb{R}^{n}\mapsto [-\infty,+\infty]\)</span>,下列陈述等价:</p>
<ul>
<li>水平集<span class="math inline">\(V_{\gamma}=\{x|f(x)\le\gamma\}\)</span>对<span class="math inline">\(\forall\gamma\in R\)</span>闭.</li>
<li><span class="math inline">\(f\)</span>下半连续.</li>
<li><span class="math inline">\(f\)</span>闭.</li>
</ul>
</blockquote>
<h3 id="凸函数的运算">凸函数的运算</h3>
<p>类似于在凸集一节中提到的保持集合凸性的运算,保持函数凸性的运算主要有以下几种:</p>
<ul>
<li>线性变换.</li>
<li>与非负标量相乘.</li>
<li>取上确界.</li>
<li>对部分变量最小化.</li>
</ul>
<h3 id="可微凸函数">可微凸函数</h3>
<h4 id="一阶可微凸函数">一阶可微凸函数</h4>
<p>下面这个命题想必大家都不会陌生:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭凸子集,且<span class="math inline">\(f:\mathbb{R}^{n}\mapsto \mathbb{R}\)</span>在包含<span class="math inline">\(C\)</span>的开集上可微,那么函数<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上凸当且仅当<span class="math display">\[f(z)\ge f(x)+∇f(x)&#39;(z-x),\forall x,z\in C\]</span></p>
</blockquote>
<p>一张图秒懂上述命题: <img src="/pictures/%E5%87%B8%E4%BC%98%E5%8C%96_1.png" alt="avatar" /></p>
<p>上述命题能导出一个最优性条件:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭凸子集,且<span class="math inline">\(f:\mathbb{R}^{n}\mapsto \mathbb{R}\)</span>在包含<span class="math inline">\(C\)</span>的开集上可微,则向量<span class="math inline">\(x_{0}\)</span>在<span class="math inline">\(C\)</span>上取最小当且仅当 <span class="math inline">\(∇f(x_{0})&#39;(z-x_{0})\ge 0,\forall z\in C\)</span>.</p>
</blockquote>
<p>简单证明:</p>
<ul>
<li>充分性可有<a href="https://en.wikipedia.org/wiki/Fermat%27s_theorem">Fermat引理</a>得到;<br />
</li>
<li>必要性则可由¥<span class="math inline">\(f(z)\ge f(x_{0})+∇f(x)&#39;(z-x_{0})\ge f(x_{0}),\forall z\in C\)</span>.</li>
</ul>
<p>而上述最优性条件又能来证明如下投影定理(Projection Theorem):</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭凸子集,<span class="math inline">\(z\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的一个向量,则在<span class="math inline">\(x\in C\)</span>上存在唯一的向量,使得<span class="math inline">\(║z-x║\)</span>最小,这个向量称为<span class="math inline">\(z\)</span>在<span class="math inline">\(C\)</span>上的投影,记为<span class="math inline">\(x_{0}\)</span>.<span class="math inline">\(x_{0}\)</span>为<span class="math inline">\(z\)</span>在<span class="math inline">\(C\)</span>上的投影当且仅当<span class="math display">\[(z-x_{0})&#39;(x-x_{0})\le 0,\forall x\in C\]</span></p>
</blockquote>
<p>简单证明:</p>
<ul>
<li><span class="math inline">\(x_{0}\)</span>为<span class="math inline">\(z\)</span>在<span class="math inline">\(C\)</span>上的投影等价于<span class="math inline">\(x_{0}\)</span>为<span class="math inline">\(║z-x║\)</span>最小值点;</li>
<li>又等价于<span class="math inline">\(x_{0}\)</span>为<span class="math inline">\(f(x)=1/2║z-x║^{2}\)</span>的最小值点;</li>
<li>再将<span class="math inline">\(f(x)\)</span>带入上述最优性条件可知<span class="math inline">\(x_{0}\)</span>使<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上取最小当且仅当<span class="math inline">\((x_{0}-z)&#39;(x-x_{0})\ge 0,\forall x\in C\)</span>.</li>
</ul>
<h4 id="二阶可微凸函数">二阶可微凸函数</h4>
<p>下面这个命题想必大家也不会陌生:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭凸子集,且<span class="math inline">\(f:\mathbb{R}^{n}\mapsto \mathbb{R}\)</span>在包含<span class="math inline">\(C\)</span>的开集上二阶可微.</p>
<ul>
<li>如果<span class="math inline">\(∇^{2}f(x)\)</span>对所有的<span class="math inline">\(x\in C\)</span>半正定,那么函数<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上为凸.</li>
<li>如果<span class="math inline">\(∇^{2}f(x)\)</span>对所有的<span class="math inline">\(x\in C\)</span>正定,那么函数<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上为严格凸.</li>
<li>如果<span class="math inline">\(C\)</span>为开,且函数<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上凸,那么<span class="math inline">\(∇^{2}f(x)\)</span>对所有的<span class="math inline">\(x\in C\)</span>半正定.</li>
</ul>
</blockquote>
<h2 id="凸包仿射包及carathéodory定理">凸包、仿射包及Carathéodory定理</h2>
<h3 id="凸包">凸包</h3>
<p>凸包(convex hull):</p>
<blockquote>
<p>集合<span class="math inline">\(X\)</span>的凸包,指所有包含<span class="math inline">\(X\)</span>的凸集的交集,记作<span class="math inline">\(conv(X)\)</span>.</p>
</blockquote>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/ConvexHull.svg/258px-ConvexHull.svg.png" /></p>
<p><span class="math inline">\(X\)</span>由图中的小黑点构成,外层黑线就是包含<span class="math inline">\(X\)</span>的一个凸集,此时<span class="math inline">\(conv(X)\)</span>就是图中的蓝线,就像一根橡皮筋那样. 且由任意多个凸集的交集是凸集可得<span class="math inline">\(conv(X)\)</span>仍为凸集.</p>
<p>另外, 维基百科上对<a href="https://en.wikipedia.org/wiki/Convex_hull">凸包</a>有四种不同的定义:</p>
<ul>
<li>包含<span class="math inline">\(X\)</span>的(唯一的)最小凸集.</li>
<li>所有包含<span class="math inline">\(X\)</span>的凸集的交集.</li>
<li><span class="math inline">\(X\)</span>中所有点的凸组合.</li>
<li>所有单形体的并集.</li>
</ul>
<p>其中第一第二条定义好理解,第三条中的<a href="https://en.wikipedia.org/wiki/Convex_combination">凸组合</a>定义如下:</p>
<blockquote>
<p>给定<span class="math inline">\(x_{1},...,x_{m}\)</span>,凸组合为<span class="math inline">\(\{\sum\limits_{i = 1}^m\alpha_{i}x_{i}\)</span>|<span class="math inline">\(\alpha_{i}\ge 0,\sum\limits_{i = 1}^m\alpha_{i}=1\}\)</span>.</p>
</blockquote>
<p>比如这张图中的点<span class="math inline">\(P\)</span>是<span class="math inline">\(x_{1},x_{2},x_{3}\)</span>的凸组合,点<span class="math inline">\(Q\)</span>则不是:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Convex_combination_illustration.svg/115px-Convex_combination_illustration.svg.png" /></p>
<p>至于第四条中的<a href="https://en.wikipedia.org/wiki/Simplex">单形体</a>指的是是将三角形或四面体概念推广到任意维的概念,具体来说,<span class="math inline">\(k\)</span>单形体是一个<span class="math inline">\(k\)</span>维多面体,它是<span class="math inline">\(k+1\)</span>顶点的凸包,比如说下图就是一个正的3单形体,它是4个顶点的凸包:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Tetrahedron.png/127px-Tetrahedron.png" /></p>
<p>那么此时第四条就可以这样理解,还是这张图为例,此时的单形体就是所有以黑点为顶点的三角形,并起来就是蓝线内部.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/ConvexHull.svg/258px-ConvexHull.svg.png" /></p>
<h3 id="仿射包">仿射包</h3>
<p>仿射包(affine hull):</p>
<blockquote>
<p>集合<span class="math inline">\(X\)</span>的仿射包,指所有包含<span class="math inline">\(X\)</span>的仿射集的交集,记作<span class="math inline">\(aff(X)\)</span>.</p>
</blockquote>
<p>这里的仿射集是指:</p>
<blockquote>
<p>集合<span class="math inline">\(M\)</span>称为是仿射的,如果它包含所有穿过满足<span class="math inline">\(x,y\in M\)</span>且<span class="math inline">\(x\neq y\)</span>条件的点对<span class="math inline">\(x,y\)</span>的直线.</p>
</blockquote>
<p>维基上对<a href="https://en.wikipedia.org/wiki/Affine_hull">仿射包</a>定义有三:</p>
<ul>
<li>包含<span class="math inline">\(X\)</span>的最小仿射集.</li>
<li>所有包含<span class="math inline">\(X\)</span>的仿射集的交集.</li>
<li>所有<span class="math inline">\(X\)</span>中元素的仿射组合,即<span class="math inline">\(aff(X)=\{\sum\limits_{i = 1}^m \alpha_{i}x_{i}|\sum\limits_{i = 1}^m\alpha_{i}=1\}\)</span>.</li>
</ul>
<p>打开维基查询<a href="https://en.wikipedia.org/wiki/Affine_space">仿射集</a>,发现该词条被并入仿射空间,这是因为:具有仿射结构的集合就是一个仿射空间,故仿射集即仿射空间.</p>
<p>维基上对仿射空间及仿射结构有这样一段有意思的描述(可略过):</p>
<p>仿射空间:忘记原点之后剩余的矢量空间;在仿射空间中,点与点之间做差可以得到向量,点与向量做加法将得到另一个点,但是点与点之间不可以做加法.</p>
<p>仿射结构:想象一下,<span class="math inline">\(Alice\)</span>知道某个点是真正的原点,但<span class="math inline">\(Bob\)</span>认为另一点<span class="math inline">\(p\)</span>是原点,现求两个向量<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的和.<span class="math inline">\(Bob\)</span>画出<span class="math inline">\(p\)</span>到<span class="math inline">\(a\)</span>和<span class="math inline">\(p\)</span>到<span class="math inline">\(b\)</span>的箭头,然后用平行四边形法则找到他认为的向量<span class="math inline">\(a+b\)</span>,但是<span class="math inline">\(Alice\)</span>知道他实际计算了<span class="math inline">\(p+(a-p)+(b-p)\)</span>.</p>
<p>类似地,<span class="math inline">\(Alice\)</span>和<span class="math inline">\(Bob\)</span>可以计算向量<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的线性组合,通常会得到不同的答案.但是,如果线性组合中系数的和为1,则<span class="math inline">\(Alice\)</span>和<span class="math inline">\(Bob\)</span>将得到相同的答案.</p>
<p>举个例子:如果爱丽丝计算<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的线性组合,<span class="math inline">\(\lambda a+(1-\lambda )b\)</span>,那么鲍勃可以得到相同的结果, <span class="math inline">\(p+\lambda(a-p)+(1-\lambda)(b-p)=\lambda a+(1-\lambda)b\)</span>.</p>
<p>在这种情况下,尽管使用不同的原点,但对于所有系数<span class="math inline">\(\lambda+(1-\lambda)=1\)</span>,<span class="math inline">\(Alice\)</span>和<span class="math inline">\(Bob\)</span>用相同的线性组合描述相同的点.</p>
<p>虽然<span class="math inline">\(Alice\)</span>知道线性结构,但<span class="math inline">\(Alice\)</span>和<span class="math inline">\(Bob\)</span>都知道仿射结构,即仿射组合的值,定义为系数之和为1的线性组合.</p>
<h3 id="非负组合">非负组合</h3>
<p>非负组合:</p>
<blockquote>
<p><span class="math inline">\(X\)</span>中元素的非负组合为<span class="math inline">\(\{\sum\limits_{i = 1}^m\alpha_{i}x_{i}|\alpha_{i}\ge 0\}\)</span>,正组合为<span class="math inline">\(\{\sum\limits_{i = 1}^m\alpha_{i}x_{i}|\alpha_{i}&gt; 0\}\)</span>.</p>
</blockquote>
<p>显然和凸组合的定义相比,非负组合去除了<span class="math inline">\(\sum\limits_{i = 1}^m\alpha_{i}=1\)</span>的限制.</p>
<p>由<span class="math inline">\(X\)</span>生成的锥体(cone(X)):</p>
<blockquote>
<p>由<span class="math inline">\(X\)</span>生成的锥体,指<span class="math inline">\(X\)</span>中所有元素的非负组合构成的集合,即<span class="math inline">\(\{\sum\limits_{i = 1}^m\alpha_{i}x_{i}|\alpha_{i}\ge 0\}\)</span>,记作<span class="math inline">\(cone(X)\)</span>.</p>
</blockquote>
<p>注意与锥体定义的区别,锥体(cone):</p>
<blockquote>
<p><span class="math inline">\(0\in X\)</span>,如果对 <span class="math inline">\(\forall x \in X , \forall \lambda &gt; 0, \lambda x \in X\)</span> ,则称<span class="math inline">\(X\)</span>为锥体.</p>
</blockquote>
<h3 id="carathéodory定理">Carathéodory定理</h3>
<p>Carathéodory定理:</p>
<blockquote>
<p>令<span class="math inline">\(X\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的一个非空子集. - 每个取自<span class="math inline">\(X\)</span>生成的锥体<span class="math inline">\(cone(X)\)</span>的非零向量都可以表示成<span class="math inline">\(X\)</span>中线性无关向量的正组合. - 每个取自<span class="math inline">\(X\)</span>的凸包<span class="math inline">\(conv(X)\)</span>的向量都可以表示成<span class="math inline">\(X\)</span>中不超过<span class="math inline">\(n+1\)</span>个向量的凸组合.</p>
</blockquote>
<p>第一条很显然,第二条还是参照下图,<span class="math inline">\(\mathbb{R}^{n}\)</span>平面内,任意的蓝线内的点,总能找到<span class="math inline">\(2+1\)</span>个点形成三角形,使点在该三角形内.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/ConvexHull.svg/258px-ConvexHull.svg.png" /></p>
<p>Carathéodory定理可以用来证明下述命题:</p>
<blockquote>
<p>紧集的凸包是紧的.</p>
</blockquote>
<p>证明略,有兴趣的朋友可以去读凸优化理论p21.</p>
<h2 id="相对内点集与闭包">相对内点集与闭包</h2>
<h3 id="相对内点集">相对内点集</h3>
<p>相对内点集:</p>
<blockquote>
<p>令<span class="math inline">\(C\)</span>为非空凸集,称<span class="math inline">\(x\)</span>为<span class="math inline">\(C\)</span>的相对内点,如果<span class="math inline">\(x\in C\)</span>并且存在以<span class="math inline">\(x\)</span>为中心的开球<span class="math inline">\(S\)</span>,使得<span class="math inline">\(aff(C)\cap S\subset C\)</span>,即<span class="math inline">\(x\)</span>是相对于<span class="math inline">\(C\)</span>的仿射包的内点.<span class="math inline">\(C\)</span>的所有相对内点的集合称为<span class="math inline">\(C\)</span>的相对内点集,并记作<span class="math inline">\(ri(C)\)</span>.</p>
</blockquote>
<p>集合<span class="math inline">\(C\)</span>称为是相对开的,如果<span class="math inline">\(ri(C)=C\)</span>.</p>
<h3 id="闭包与相对边界点">闭包与相对边界点</h3>
<p><a href="https://en.wikipedia.org/wiki/Closure_(mathematics)">闭包(Closure)</a>,维基上对其定义如下:</p>
<blockquote>
<p>A set has closure under an operation if performance of that operation on members of the set always produces a member of the same set; in this case we also say that the set is closed under the operation.</p>
</blockquote>
<p>比如欧氏空间<span class="math inline">\(\mathbb{R}^{n}\)</span>中,集合<span class="math inline">\(C\)</span>(未必是闭的)在取极限操作下有闭包<span class="math inline">\(cl(C)\)</span>,意味着对<span class="math inline">\(C\)</span>中收敛的点列取极限后得到的点仍在<span class="math inline">\(cl(C)\)</span>中(对于不收敛的点列取极限为空,也可以说在<span class="math inline">\(cl(C)\)</span>中).</p>
<p>记<span class="math inline">\(cl(C)\)</span>为<span class="math inline">\(C\)</span>的闭包,<span class="math inline">\(cl(C)\)</span>中不是相对内点的点称为是<span class="math inline">\(C\)</span>的相对边界点,这些点的集合称为是<span class="math inline">\(C\)</span>的相对边界.</p>
<p>例如,令<span class="math inline">\(C\)</span>为<span class="math inline">\(R\)</span>上的<span class="math inline">\([a,b]\)</span>,则<span class="math inline">\(aff(C)\)</span>则为<span class="math inline">\(\mathbb{R}\)</span>,那么<span class="math inline">\(ri(C)\)</span>则是<span class="math inline">\((a,b)\)</span>,<span class="math inline">\(C\)</span>的相对边界则是<span class="math inline">\(\{a,b\}\)</span>.特别的,若<span class="math inline">\(C\)</span>本身是仿射集,那么此时<span class="math inline">\(aff(C)=C=ri(C)\)</span>,相对边界是<span class="math inline">\(\emptyset\)</span>.</p>
<h3 id="相对内点的性质">相对内点的性质</h3>
<h4 id="线段原理">线段原理</h4>
<p>如下命题给出了相对内点的性质:</p>
<p>线段原理(Line Segment Principle):</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为非空凸集,如果<span class="math inline">\(x\in ri(C)\)</span>,并且<span class="math inline">\(y\in cl(C)\)</span>,那么连接两点<span class="math inline">\(x,y\)</span>的线段上的点,除了<span class="math inline">\(y\)</span>,都<span class="math inline">\(\in ri(C)\)</span>.</p>
</blockquote>
<h4 id="相对内点的非空性">相对内点的非空性</h4>
<p>下述命题由线段原理推出:</p>
<p>相对内点的非空性:</p>
<blockquote>
<p>令<span class="math inline">\(C\)</span>为非空凸集.则</p>
<ul>
<li>集合<span class="math inline">\(C\)</span>的相对内点集<span class="math inline">\(ri(C)\)</span>是非空凸集,并且和<span class="math inline">\(C\)</span>具有相同的仿射包,即<span class="math inline">\(aff(C)=aff(ri(C))\)</span>.</li>
<li>如果<span class="math inline">\(C\)</span>的仿射包<span class="math inline">\(aff(C)\)</span>的维数<span class="math inline">\(m&gt;0\)</span>的,那么必存在向量<span class="math inline">\(x_{0},...,x_{m}\in ri(C)\)</span>,使得<span class="math inline">\(x_{1}-x_{0},...,x_{m}-x_{0}\)</span>所张成的子空间平行于<span class="math inline">\(aff(C)\)</span>.</li>
</ul>
</blockquote>
<p>这里<span class="math inline">\(aff(C)\)</span>的维数定义为平行于<span class="math inline">\(aff(C)\)</span>的子空间的维数.</p>
<h4 id="延伸引理">延伸引理</h4>
<p>另一有用结论同样可由线段原理得到:</p>
<p>延伸引理(Prolongation Lemma):</p>
<blockquote>
<p>令<span class="math inline">\(C\)</span>为非空凸集.<span class="math inline">\(x\in ri(C)\)</span>,当且仅当对所有<span class="math inline">\(y\in C\)</span>,存在<span class="math inline">\(\gamma &gt;0\)</span>,使得<span class="math inline">\(x+\gamma (x-y)\in C\)</span>.</p>
</blockquote>
<p>即<span class="math inline">\(C\)</span>中以<span class="math inline">\(x\)</span>为端点的所有线段可以延伸超过<span class="math inline">\(x\)</span>而不必离开<span class="math inline">\(C\)</span>.</p>
<h4 id="相对内点与对偶理论">相对内点与对偶理论</h4>
<p>后面的章节中将看到相对内点的概念在凸优化和对偶理论中具有广泛应用,例如,当代价函数是凹的时,我们可以给出最后解集的一个刻画.</p>
<blockquote>
<p>令<span class="math inline">\(X\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中一个非空凸子集,且函数<span class="math inline">\(f:X\mapsto R\)</span>为凹,<span class="math inline">\(X^{+}\)</span>为使<span class="math inline">\(f\)</span>在<span class="math inline">\(X\)</span>上达到最小的向量集,即: <span class="math inline">\(X^{+}=\{x^{+}\in X|f(x^{+})=\inf \limits_{x\in X}f(x)\}\)</span>.如果<span class="math inline">\(X^{+}\)</span>包含<span class="math inline">\(X\)</span>的相对内点,那么<span class="math inline">\(f\)</span>在<span class="math inline">\(X\)</span>上必为常数.</p>
</blockquote>
<h3 id="相对内点集与闭包的演算">相对内点集与闭包的演算</h3>
<p>为了研究凸集的集合运算,我们同样需要得到相应的相对内点集与闭包的运算结果,这里仅作概述(其中<span class="math inline">\(C,C_{1},C_{2}\)</span>为非空凸集,<span class="math inline">\(A\)</span>为线性变换):</p>
<ul>
<li>两个凸集具有相同的<span class="math inline">\(cl\)</span>当且仅当它们具有相同的<span class="math inline">\(ri\)</span>.</li>
<li><span class="math inline">\(A•ri(C)=ri(A•C);A•cl(C)⊂cl(A•C)\)</span>且<span class="math inline">\(C\)</span>有界时取等号.</li>
<li>特别的,<span class="math inline">\(A\)</span>为<span class="math inline">\((x_{1},x_{2})\)</span>到<span class="math inline">\(x_{1}+x_{2}\)</span>的线性变换时,有<span class="math inline">\(ri(C_{1}+C_{2})=ri(C_{1})+ri(C_{2})\)</span>; <span class="math inline">\(cl(C_{1}+C_{2})\subset cl(C_{1})+cl(C_{2})\)</span>且<span class="math inline">\(C_{1},C_{2}\)</span>至少一个有界时取等号.</li>
<li><span class="math inline">\(ri(C_{1})\cap ri(C_{2})\subset ri(C_{1}\cap C_{2})\)</span>且<span class="math inline">\(ri(C_{1})\cap ri(C_{2})\neq\emptyset\)</span>时取等号.</li>
<li><span class="math inline">\(cl(C_{1}\cap C_{2})\subset cl(C_{1})\cap cl(C_{2})\)</span>且<span class="math inline">\(ri(C_{1})\cap ri(C_{2})\neq\emptyset\)</span>时取等号.</li>
<li>如果<span class="math inline">\(A^{-1}•ri(C)\)</span>非空:<span class="math inline">\(ri(A^{-1}•C)=A^{-1}•ri(C)\)</span>;<span class="math inline">\(cl(A^{-1}•C)=A^{-1}•cl(C)\)</span>.</li>
</ul>
<p>第二条说明闭凸集在线性变换下不一定是闭的,这是凸优化分析上存在困难的一个主要原因.</p>
<h3 id="凸函数的连续性">凸函数的连续性</h3>
<p>凸函数有一个基本的性质:</p>
<p>凸函数的连续性:</p>
<blockquote>
<p>如果<span class="math inline">\(f:\mathbb{R}^{n}\mapsto \mathbb{R}\)</span>是凸的,那么它一定是连续的.更一般的,如果<span class="math inline">\(f:\mathbb{R}^{n}\mapsto (-\infty,+\infty]\)</span>,那么限制在<span class="math inline">\(dom(f)\)</span>上的函数<span class="math inline">\(f\)</span>在<span class="math inline">\(dom(f)\)</span>的相对内点集上是连续的.</p>
</blockquote>
<p>该命题蕴含实值凸函数是连续的,进而是闭的(<span class="math inline">\(f\)</span>下半连续等价于<span class="math inline">\(f\)</span>闭),对于单变量函数的情形,我们还有如下更强的结论:</p>
<blockquote>
<p>如果<span class="math inline">\(C\)</span>是<span class="math inline">\(\mathbb{R}\)</span>上的一个闭区间,且<span class="math inline">\(f:C\mapsto \mathbb{R}\)</span>是闭的和凸的,那么<span class="math inline">\(f\)</span>在<span class="math inline">\(C\)</span>上连续.</p>
</blockquote>
<p>事实上,数学分析中关于凸函数有如下推论(斐礼文的数学分析中的典型问题与方法):</p>
<blockquote>
<p>若<span class="math inline">\(f\)</span>在区间<span class="math inline">\(I\)</span>上凸,那么<span class="math inline">\(f\)</span>在任意内点上连续.</p>
</blockquote>
<p>那么利用上述推论或者利用凸函数的连续性,再加上<span class="math inline">\(f\)</span>闭不难得出<span class="math inline">\(f\)</span>在闭区间<span class="math inline">\(C\)</span>连续.</p>
<h3 id="函数的闭包">函数的闭包</h3>
<p>本节研究可以把给定函数变换为闭凸函数,同时又能保留该函数主要特性的运算.</p>
<p><span class="math inline">\(\mathbb{R}^{n+1}\)</span>的非空子集<span class="math inline">\(E\)</span>是某个函数的上图,如果它满足:对每个点<span class="math inline">\((x,\omega)\in E\)</span>,集合<span class="math inline">\(\{\omega|(x,\omega)\in E\}\)</span>要么是实数直线,要么是下方有界的半直线.</p>
<h4 id="闭包">闭包</h4>
<p><span class="math inline">\(f\)</span>的闭包(closure):</p>
<blockquote>
<p>函数<span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>的上图的闭包可以看做是另一个函数的上图.这个函数被称为是<span class="math inline">\(f\)</span>的闭包,记作<span class="math inline">\(clf\)</span>. <span class="math inline">\(clf :\mathbb{R}^{n}\mapsto [-\infty,+\infty]\)</span>,且<span class="math inline">\((clf )(x)=\inf\{\omega|(x,\omega)\in cl(epi(f))\},\forall x\in \mathbb{R}^{n}\)</span>.</p>
</blockquote>
<p>当<span class="math inline">\(f\)</span>凸时,<span class="math inline">\(epi(f)\)</span>凸,那么<span class="math inline">\(cl(epi(f))\)</span>是闭的凸的(凸集的闭包仍凸).这意味着$clf <span class="math inline">\(是闭的凸的,因为依据定义有\)</span>epi(clf)=cl(epi(f))$.</p>
<h4 id="凸闭包">凸闭包</h4>
<p><span class="math inline">\(f\)</span>的凸闭包(convex closure):</p>
<blockquote>
<p><span class="math inline">\(f\)</span>的上图的凸包的闭包是某个函数的上图.这个函数被称为是<span class="math inline">\(f\)</span>的凸闭包,记作<span class="math inline">\(\hat{cl}f\)</span>, 令<span class="math inline">\(F(x)=\inf\{\omega|(x,\omega)\in conv(epi(f))\},\forall x\in \mathbb{R}^{n}\)</span>,那么<span class="math inline">\(\hat{cl}f\)</span>是函数<span class="math inline">\(F\)</span>的闭包.</p>
</blockquote>
<p><span class="math inline">\(F\)</span>是凸的,但它未必是闭的.<span class="math inline">\(\hat{cl}f\)</span> 是闭的凸的.</p>
<h4 id="闭包的性质">闭包的性质</h4>
<p>从优化的角度看,下述命题叙述了一条重要性质,即<span class="math inline">\(f\)</span>,<span class="math inline">\(clf\)</span> ,<span class="math inline">\(F\)</span>和<span class="math inline">\(\hat{cl}f\)</span>的最小值是一样的:</p>
<blockquote>
<p><span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>为一函数,有<span class="math display">\[\inf\limits_{x\in X}f(x)=\inf\limits_{x\in X}clf(x)=\inf\limits_{x\in \mathbb{R}^{n}}clf(x)=\inf\limits_{x\in \mathbb{R}^{n}}F(x)=\inf\limits_{x\in \mathbb{R}^{n}}\hat{cl}f(x)\]</span></p>
</blockquote>
<p>下面的命题给出闭包的一个特性:</p>
<blockquote>
<p><span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>为一函数,</p>
<ul>
<li><span class="math inline">\(clf\)</span>是被<span class="math inline">\(f\)</span>控制的最大闭函数.</li>
<li><span class="math inline">\(\hat{cl}f\)</span>是被<span class="math inline">\(f\)</span>控制的最大闭凸函数.</li>
</ul>
</blockquote>
<p>这里被控制即如果<span class="math inline">\(g\)</span>是闭的且<span class="math inline">\(g\le f\)</span>,那么<span class="math inline">\(g\le clf\)</span>.</p>
<p>研究凸函数的闭包是因为在某种意义下闭包与原函数差别最小.特别的,可以证明凸函数在它的定义域的相对内点集上与它的闭包一致:</p>
<blockquote>
<p><span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>为一凸函数,则:</p>
<ol type="1">
<li><span class="math inline">\(cl(dom(f))=cl(dom(clf))\)</span>,<span class="math inline">\(ri(dom(f))=ri(dom(clf))\)</span></li>
<li><span class="math inline">\((clf )(x)=f(x),\forall x\in ri(dom(f))\)</span>,<span class="math inline">\(clf\)</span>是真的当且仅当<span class="math inline">\(f\)</span>是真的.</li>
<li>如果<span class="math inline">\(x\in ri(dom(f))\)</span>,我们有:<span class="math inline">\((clf)(y)=\lim\limits_{\alpha\downarrow 0}f(y+\alpha (x-y)),\forall y\in \mathbb{R}^{n}\)</span>.</li>
</ol>
</blockquote>
<p>下面两个命题是刻画凸函数经过线性变换之后所得函数的闭包:</p>
<blockquote>
<p><span class="math inline">\(f:X\mapsto [-\infty,+\infty]\)</span>为一凸函数,且A为一<span class="math inline">\(m×n\)</span>矩阵使得A的值域包含<span class="math inline">\(ri(dom(f))\)</span>中的一个点.那么由 <span class="math inline">\(F(x)=f(Ax)\)</span>定义的函数<span class="math inline">\(F\)</span>是凸的,并且<span class="math display">\[(clF)(x)=(clf)(Ax),\forall x\in \mathbb{R}^{n}\]</span></p>
</blockquote>
<p>下述命题本质上式上述命题的一个特例:</p>
<blockquote>
<p><span class="math inline">\(f_{i}:\mathbb{R}^{n}\mapsto [-\infty,+\infty],i=1,...,m\)</span>,为凸函数,且<span class="math inline">\(\cap_{i=1,...,m}ri(dom(f_{i}))\neq\emptyset\)</span>.那么由<span class="math inline">\(F(x)=f_{1}(x)+...+f_{m}(x)\)</span>定义的函数<span class="math inline">\(F\)</span>是凸的,并且<span class="math display">\[(clF)(x)=(clf_{m})(x)+...+(clf_{m})(x),\forall x\in \mathbb{R}^{n}\]</span></p>
</blockquote>
<h2 id="回收锥">回收锥</h2>
<h3 id="回收锥与线形空间">回收锥与线形空间</h3>
<h4 id="回收锥-1">回收锥</h4>
<h5 id="回收方向">回收方向</h5>
<p>回收方向(direction of recession):</p>
<blockquote>
<p>给定非空凸集<span class="math inline">\(C\)</span>,我们说<span class="math inline">\(d\)</span>是<span class="math inline">\(C\)</span>的一个回收方向,如果<span class="math inline">\(x+\alpha d\in C\)</span>对所有的<span class="math inline">\(x\)</span>属于<span class="math inline">\(C\)</span>和<span class="math inline">\(\alpha\ge 0\)</span>都成立.</p>
</blockquote>
<p>通俗的解释就是我们从<span class="math inline">\(C\)</span>中任意点出发,沿着<span class="math inline">\(d\)</span>的方向走到无穷,而永远不穿过<span class="math inline">\(C\)</span>的相对边界点跑到<span class="math inline">\(C\)</span>之外.</p>
<h5 id="回收锥-2">回收锥</h5>
<p>回收锥(recession cone):</p>
<blockquote>
<p>所有回收方向的集合是一个包含原点的锥体.称之为<span class="math inline">\(C\)</span>的回收锥,记作<span class="math inline">\(R_{C}\)</span>.</p>
</blockquote>
<p>闭凸集的一条重要性质就是为检验<span class="math inline">\(d\in R_{C}\)</span>是否成立,只需验证<span class="math inline">\(x+\alpha d\in C\)</span>对单一的<span class="math inline">\(x\in C\)</span>成立就可以了.这就是下述命题的下半部分.</p>
<h5 id="回收锥定理">回收锥定理</h5>
<p>回收锥定理(Recession Cone Theorem):</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为非空闭凸集.</p>
<ul>
<li>回收锥<span class="math inline">\(R_{C}\)</span>是闭的凸的.</li>
<li>向量<span class="math inline">\(d\in R_{C}\)</span>当且仅当存在向量<span class="math inline">\(x\in C\)</span>使得<span class="math inline">\(x+\alpha d\in C\)</span>对所有的<span class="math inline">\(\alpha\ge 0\)</span>成立.</li>
</ul>
</blockquote>
<h5 id="回收锥的性质">回收锥的性质</h5>
<p>下述命题给出回收锥的一些其他性质.</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为非空闭凸集.</p>
<ul>
<li><span class="math inline">\(R_{C}\)</span>包含一个非零的方向当且仅当<span class="math inline">\(C\)</span>是无界的.</li>
<li><span class="math inline">\(R_{C}\)</span>=<span class="math inline">\(R_{ri(C)}\)</span>.</li>
<li>对任意一组闭凸集<span class="math inline">\(C_{i},i\in I\)</span>,并且<span class="math inline">\(\cap_{i\in I}C_{i}\neq\emptyset\)</span>,有<span class="math inline">\(R_{\cap_{i\in I}C_{i}}=\cap_{i\in I}R_{C_{i}}\)</span>.</li>
<li><span class="math inline">\(W\)</span>为<span class="math inline">\(\mathbb{R}^{m}\)</span>的紧的凸子集,并令<span class="math inline">\(A\)</span>为一<span class="math inline">\(m×n\)</span>矩阵.集合<span class="math inline">\(V=\{x\in C|Ax\in W\}\)</span>的回收锥是<span class="math inline">\(R_{C}\cap N(A)\)</span>,其中<span class="math inline">\(N(A)\)</span>是<span class="math inline">\(A\)</span>的零空间(nullspace).</li>
</ul>
</blockquote>
<h4 id="线形空间">线形空间</h4>
<h5 id="线形空间-1">线形空间</h5>
<p>凸集<span class="math inline">\(C\)</span>的回收锥有一个重要子集,称为其线形空间(lineality space),</p>
<blockquote>
<p>线形空间,记作<span class="math inline">\(L_{C}\)</span>,定义为<span class="math inline">\(-d\)</span>也是回收方向的方向<span class="math inline">\(d\)</span>的集合:<span class="math inline">\(L_{C}=R_{C}\cap (-R_{C})\)</span>.</p>
</blockquote>
<p>因此<span class="math inline">\(d\in L_{C}\)</span>当且仅当整个直线<span class="math inline">\(\{x+\alpha d|\alpha\in R\}\)</span>对于每个<span class="math inline">\(x\in C\)</span>都包含于<span class="math inline">\(C\)</span>中.</p>
<h5 id="线形空间的性质">线形空间的性质</h5>
<p>线形空间仍然有我们已经有的回收锥的若干性质:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>的非空闭凸集.</p>
<ul>
<li><span class="math inline">\(L_{C}\)</span>是<span class="math inline">\(\mathbb{R}^{n}\)</span>的子空间.</li>
<li><span class="math inline">\(L_{C}=L_{ri(C)}\)</span>.</li>
<li>对任意一组闭凸集<span class="math inline">\(C_{i},i\in I\)</span>,并且<span class="math inline">\(\cap_{i\in I}C_{i}\neq\emptyset\)</span>,有<span class="math inline">\(L_{\cap_{i\in I}C_{i}}=\cap_{i\in I}L_{C_{i}}\)</span>.</li>
<li><span class="math inline">\(W\)</span>为<span class="math inline">\(\mathbb{R}^{m}\)</span>的紧的凸子集,并令<span class="math inline">\(A\)</span>为一<span class="math inline">\(m×n\)</span>矩阵.集合<span class="math inline">\(V=\{x\in C|Ax\in W\}\)</span>的线形空间是<span class="math inline">\(L_{C}\cap N(A)\)</span>,其中<span class="math inline">\(N(A)\)</span>是<span class="math inline">\(A\)</span>的零空间(nullspace).</li>
</ul>
</blockquote>
<h4 id="回收锥与线形空间的一个例子">回收锥与线形空间的一个例子</h4>
<p>举例来说,考虑集合<span class="math inline">\(C=\{x|x&#39;Qx+c&#39;x+b\le 0\}\)</span>,<span class="math inline">\(Q\)</span>为对称正定的<span class="math inline">\(n\)</span>维方阵,<span class="math inline">\(c\)</span>为<span class="math inline">\(n\)</span>维向量. 向量<span class="math inline">\(d\)</span>是回收方向当且仅当<span class="math inline">\((x+\alpha d)&#39;Q(x+\alpha d)+c&#39;(x+\alpha d)+b\le 0,\forall\alpha&gt;0\)</span>.显然不可能有<span class="math inline">\(d&#39;Qd&gt;0\)</span>,所以只能<span class="math inline">\(d&#39;Qd=0\)</span>,又<span class="math inline">\(Q\)</span>半正定,故存在<span class="math inline">\(M\)</span>,<span class="math inline">\(s.t.Q=M&#39;M\)</span>,这使得<span class="math inline">\(Md=0\)</span>,这表明<span class="math inline">\(Qd=M&#39;Md=0\)</span>.故原式等价于<span class="math inline">\(x&#39;Qx+c&#39;x+b+\alpha c&#39;d\le0,\forall\alpha&gt;0\)</span>.而此式成立当且仅当<span class="math inline">\(c&#39;d\le 0\)</span>.于是: <span class="math display">\[R_{C}=\{d|Qd=0,c&#39;d≤0\}$;$L_{C}=\{d|Qd=0,c&#39;d=0\}\]</span></p>
<h4 id="凸集分解">凸集分解</h4>
<p>最后是一个有用的结论,它使得我们可以吧凸集按照它的线形空间(的子空间)和它的正交补进行分解:</p>
<p>凸集分解:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>的非空闭凸集.则对于每个包含在线形空间<span class="math inline">\(L_{C}\)</span>中的子空间<span class="math inline">\(S\)</span>,我们都有:<span class="math inline">\(C=S+(C+S_{\perp})\)</span>.</p>
</blockquote>
<h3 id="凸函数的回收方向与不变空间">凸函数的回收方向与不变空间</h3>
<p>与上节对应的,函数中也有类似的概念. 这个概念的重要性在于凸优化问题解得存在性.<span class="math inline">\(epi(f)\)</span>回收锥的方向可以得到<span class="math inline">\(f\)</span>不单调增加的方向.</p>
<h4 id="凸函数的回收方向">凸函数的回收方向</h4>
<blockquote>
<p><span class="math inline">\(f:\mathbb{R}^{n}\mapsto (-\infty,+\infty]\)</span>为闭的真凸函数,考虑水平集<span class="math inline">\(V_{\gamma},\gamma\in R\)</span>.则:</p>
<ul>
<li>所有的非空水平集<span class="math inline">\(V_{\gamma}\)</span>都具有相同的回收锥,记作<span class="math inline">\(R_{f}\)</span>,由<span class="math inline">\(R_{f}=\{d|(d,0)\in R_{epi(f)}\}\)</span>给出.</li>
<li>如果某个非空水平集<span class="math inline">\(V_{\gamma}\)</span>是紧的,那么所有这些水平集都是紧的.</li>
</ul>
</blockquote>
<p><span class="math inline">\(f\)</span>的回收方向:</p>
<blockquote>
<p>向量<span class="math inline">\(d\in R_{f}\)</span>称为<span class="math inline">\(f\)</span>的回收方向,其中<span class="math inline">\(R_{f}\)</span>由<span class="math inline">\(R_{f}=\{d|(d,0)\in R_{epi(f)}\}\)</span>给出.</p>
</blockquote>
<p>一个直观的理解就是f的回收方向是对f连续不升的方向,反之,如果我们从某个<span class="math inline">\(x\in dom(f)\)</span>出发,而且在沿着方向<span class="math inline">\(d\)</span>移动时,我们遇到点<span class="math inline">\(z\)</span>满足<span class="math inline">\(f(z)&gt;f(x)\)</span>,那么<span class="math inline">\(d\)</span>不可能称为回收方向.由<span class="math inline">\(f\)</span>的水平集的凸性,一旦我们跨过水平集的相对边界,我们将永远不会再次跨过该边界,而且,一个方向若不是<span class="math inline">\(f\)</span>的回收方向,其将最终为<span class="math inline">\(f\)</span>连续上升的方向.</p>
<h4 id="凸函数的不变空间">凸函数的不变空间</h4>
<p>闭的真凸函数<span class="math inline">\(f\)</span>的回收锥<span class="math inline">\(R_{f}\)</span>的线形空间记为<span class="math inline">\(L_{f}\)</span>.它是由<span class="math inline">\(d\)</span>和<span class="math inline">\(-d\)</span>都是<span class="math inline">\(f\)</span>的回收方向的所有<span class="math inline">\(d\)</span>构成子空间,即<span class="math inline">\(L_{f}=R_{f}∩(-R_{f})\)</span>.</p>
<p><span class="math inline">\(f\)</span>的不变空间(constancy space):</p>
<blockquote>
<p>向量<span class="math inline">\(d\in L_{f}\)</span>称为让<span class="math inline">\(f\)</span>常值的方向,而<span class="math inline">\(L_{f}=R_{f}∩(-R_{f})\)</span>称为<span class="math inline">\(f\)</span>的不变空间.</p>
</blockquote>
<p>举例来说,如果<span class="math inline">\(f(x)=x&#39;Qx+c&#39;x+b\)</span>为二次函数,<span class="math inline">\(Q\)</span>为对称正定的<span class="math inline">\(n\)</span>维方阵,<span class="math inline">\(c\)</span>为<span class="math inline">\(n\)</span>维向量,那么它的回收锥和不变空间是: <span class="math inline">\(R_{f}=\{d|Qd=0,c&#39;d\le 0\};L_{f}=\{d|Qd=0,c&#39;d=0\}\)</span>.</p>
<h4 id="回收函数">回收函数</h4>
<p>我们已经有,如果<span class="math inline">\(d\)</span>是f的回收方向,那么<span class="math inline">\(f\)</span>沿着射线<span class="math inline">\(x+\alpha d\)</span>是渐进非增的,但事实上我们有更强的性质,<span class="math inline">\(f\)</span>沿着<span class="math inline">\(d\)</span>的渐进斜率独立于起点<span class="math inline">\(x\)</span>.现在引入一个函数表示闭的真凸函数沿着某个方向的渐进斜率,即回收函数:</p>
<blockquote>
<p><span class="math inline">\(r_{f}\)</span>称为是<span class="math inline">\(f\)</span>的回收函数,如果满足<span class="math inline">\(epi(r_{f})=R_{epi(f)}\)</span>.</p>
</blockquote>
<p>注意到<span class="math inline">\(R_{epi(f)}\)</span>是另外一个闭的真凸函数的上图,即<span class="math inline">\(r_{f}\)</span>是闭的真凸函数.</p>
<h4 id="刻画凸函数的回收锥和不变空间">刻画凸函数的回收锥和不变空间</h4>
<p>回收函数<span class="math inline">\(r_{f}\)</span>可以用来刻画凸函数的回收锥和不变空间.</p>
<blockquote>
<p><span class="math inline">\(f:\mathbb{R}^{n}\mapsto (-\infty,+\infty]\)</span>为闭的真凸函数.则<span class="math inline">\(f\)</span>的回收锥和不变函数可以用它的回收函数给出:<span class="math display">\[R_{f}=\{d|r_{f}(d)\le 0\}, L_{f}=\{d|r_{f}(d)=r_{f}(-d)=0\}\]</span></p>
</blockquote>
<p><span class="math inline">\(d\)</span>是<span class="math inline">\(f\)</span>的回收方向,那么<span class="math inline">\(f\)</span>沿着射线<span class="math inline">\(x+\alpha d\)</span>是渐进非增的,即<span class="math inline">\(r_{f}(d)\le 0\)</span>,这就是上文中渐进斜率的含义.下述命题给出了回收函数的显示表达:</p>
<blockquote>
<p><span class="math inline">\(f:\mathbb{R}^{n}\mapsto (-\infty,+\infty]\)</span>为闭的真凸函数.则对于所有的<span class="math inline">\(x\in dom(f)\)</span>和<span class="math inline">\(d\in \mathbb{R}^{n}\)</span>, <span class="math display">\[r_{f}(d)=\sup\limits_{\alpha&gt;0}(f(x+\alpha d)-f(x))/\alpha=\lim\limits_{\alpha\to +\infty}(f(x+\alpha d)-f(x))/\alpha\]</span></p>
</blockquote>
<p>最后是和的回收函数:</p>
<blockquote>
<p><span class="math inline">\(f_{i}:\mathbb{R}^{n}\mapsto (-\infty,+\infty],i=1,...,m\)</span>,为闭的真凸函数,且<span class="math inline">\(f=f_{1}+...+f_{m}\)</span>为真.则<span class="math display">\[r_{f}(d)=r_{f_{1}}(d)+...+r_{f_{m}}(d),\forall d\in \mathbb{R}^{n}\]</span></p>
</blockquote>
<h3 id="闭集交的非空性与线性变换下的闭性">闭集交的非空性与线性变换下的闭性</h3>
<h4 id="闭集交的非空性">闭集交的非空性</h4>
<p>回收锥与线形空间的概念可以用来把紧集合的某些性质推广到闭凸集上去:</p>
<ul>
<li>嵌套的非空闭集<span class="math inline">\(\{C_{k}\}\)</span>具有非空紧交集.</li>
<li>紧集在线性变换下是紧的.</li>
</ul>
<p>考虑<span class="math inline">\(\mathbb{R}^{n}\)</span>中嵌套的非空闭集<span class="math inline">\(\{C_{k}\}\)</span>,满足有<span class="math inline">\(C_{k+1}⊂C_{k}\)</span>,证明<span class="math inline">\(\cap_{k=0,...,+\infty}C_{k}\)</span>是否为空的问题会在以下背景中出现:</p>
<ul>
<li>函数<span class="math inline">\(f:\mathbb{R}^{n}\mapsto\mathbb{R}\)</span>在集合<span class="math inline">\(X\)</span>上是否会达到最小值?该问题的答案为当且仅当<span class="math inline">\(\cap^{\infty}_{k=0}\{x\in X|f(x)\le\gamma_{k}\}\)</span>非空,其中<span class="math inline">\(\gamma_{k}\downarrow\inf_{x\in X}f(x)\)</span>.</li>
<li>如果<span class="math inline">\(C\)</span>是闭集,<span class="math inline">\(A\)</span>是矩阵,<span class="math inline">\(AC\)</span>是否为闭集?令<span class="math inline">\(\{y_{k}\}\to y\)</span>为<span class="math inline">\(AC\)</span>中一个收敛的序列,接着证<span class="math inline">\(y\in AC\)</span>.一种方法是引入集合<span class="math inline">\(C_{k}=C\cap N_{k}\)</span>,其中<span class="math inline">\(N_{k}=\{x|║Ax-y║\le║y_{k}-y║\}\)</span>,那么一个充分条件就是<span class="math display">\[\cap_{k=0,...,+\infty}C_{k}\neq\emptyset\]</span></li>
</ul>
<p>一个重要的事实是<span class="math inline">\(\cap_{k=0,...,+\infty}C_{k}\)</span>为空当且仅当<span class="math inline">\(\{C_{k}\}\)</span>中的序列<span class="math inline">\(\{x_{k}\}\)</span>都是无界的.因此我们的思路是引入保证并非所有这样的序列都是无界的,事实上只要关心如下定义中沿着公共方向发散到<span class="math inline">\(+\infty\)</span>的无界序列就可以了.</p>
<p>渐进序列:</p>
<blockquote>
<p><span class="math inline">\(\{C_{k}\}\)</span>为嵌套的非空闭凸集序列,我们说<span class="math inline">\(\{x_{k}\}\)</span>是<span class="math inline">\(\{C_{k}\}\)</span>的一个渐进序列,如果<span class="math inline">\(x_{k}\neq0\)</span>,<span class="math inline">\(x_{k}\in C_{k}\)</span>对所有的<span class="math inline">\(k\)</span>成立,并且<span class="math display">\[║x_{k}║\to+\infty,x_{k}/║x_{k}║→d/║d║\]</span>其中<span class="math inline">\(d\in\cap_{k=0,...,+∞}R_{C_{k}}\)</span>是公共非零回收方向.</p>
</blockquote>
<p>下面引入具有我们所期望性质的一类序列.</p>
<p>收缩的(retractive):</p>
<blockquote>
<p><span class="math inline">\(\{C_{k}\}\)</span>为嵌套的非空闭凸集序列,我们说<span class="math inline">\(\{x_{k}\}\)</span>为收缩的,如果对上述渐进序列的定义中对应于<span class="math inline">\(\{x_{k}\}\)</span>的方向<span class="math inline">\(d\)</span>,存在下标<span class="math inline">\(\bar{k}\)</span>,使得<span class="math inline">\(x_{k}-d\in C_{k},\forall k\ge \bar{k}\)</span>.</p>
</blockquote>
<p>我们称<span class="math inline">\(\{C_{k}\}\)</span>是收缩的,如果它的所有渐进序列都是收缩的.下图形象的说明了收缩这一概念,显然<span class="math inline">\((a)\)</span>是收缩的而<span class="math inline">\((b)\)</span>不是.</p>
<p><img src="/pictures/%E5%87%B8%E4%BC%98%E5%8C%96_2.png" /></p>
<p>多面体集合是最重要的一类收缩集:</p>
<blockquote>
<p>多面体集合是收缩的.</p>
</blockquote>
<p>收缩序列的重要性可从如下命题中看出:</p>
<blockquote>
<p>收缩的嵌套非空闭凸集序列具有非空交集.</p>
</blockquote>
<p>上述命题适用的一个简单例子就是圆柱形集合序列,其中<span class="math inline">\(R_{C_{k}}=L_{C_{k}}=L\)</span>.下面命题给出了一个重要的拓展结果:</p>
<blockquote>
<p><span class="math inline">\(\{C_{k}\}\)</span>为非空闭凸集序列,记<span class="math inline">\(R=\cap_{k=0,...,+\infty}R_{C_{k}},L=\cap_{k=0,...,+\infty}L_{C_{k}}\)</span>.</p>
<ul>
<li>如果<span class="math inline">\(R=L\)</span>,那么<span class="math inline">\(\{C_{k}\}\)</span>是收缩的,且<span class="math inline">\(\cap_{k=0,...,+\infty}C_{k}\neq\emptyset\)</span>,进而<span class="math inline">\(\cap_{k=0,...,+\infty}C_{k}=L+\hat{C}\)</span>,其中<span class="math inline">\(\hat{C}\)</span>为一非空紧集.</li>
<li><span class="math inline">\(X\)</span>为收缩的闭凸集,假定所有集合<span class="math inline">\(\bar{C_{k}}=X\cap C_{k}\neq\emptyset\)</span>,且<span class="math inline">\(R_{X}\cap R\subset L\)</span>,则<span class="math inline">\(\{\bar{C_{k}}\}\)</span>是收缩的,且<span class="math inline">\(\cap_{k=0,...,+\infty}\bar{C_{k}}\neq\emptyset\)</span>.</li>
</ul>
</blockquote>
<p>下面是一个重要应用:</p>
<p>凸二次规划解的存在性:</p>
<blockquote>
<p>令<span class="math inline">\(Q\)</span>为对称半正定<span class="math inline">\(n\)</span>阶方阵,<span class="math inline">\(c\)</span>和<span class="math inline">\(a_{1},...,a_{r}\)</span>为<span class="math inline">\(n\)</span>维向量,<span class="math inline">\(b_{1},...,b_{r}\)</span>为标量,假定优化问题:<span class="math display">\[minx&#39;Qx+c&#39;x\]</span> <span class="math display">\[s.t.a&#39;_{j}x\le b_{j},j=1,...,r\]</span>的最优值为有限,则该问题至少有一个最优解.</p>
</blockquote>
<h4 id="线性变换下的闭性">线性变换下的闭性</h4>
<p>上节得到的闭凸集序列交的非空性可以转化为保证闭凸集<span class="math inline">\(C\)</span>在线性变换<span class="math inline">\(A\)</span>下的像<span class="math inline">\(AC\)</span>的闭性条件,这就是下述命题:</p>
<blockquote>
<p><span class="math inline">\(X\)</span>和<span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭集,<span class="math inline">\(A\)</span>为<span class="math inline">\(m×n\)</span>矩阵,且记<span class="math inline">\(N(A)\)</span>为其零化空间.如果<span class="math inline">\(X\)</span>是收缩的闭凸集且<span class="math inline">\(R_{X}\cap R_{C}\cap N(A)\subset L_{C}\)</span>,那么<span class="math inline">\(A(X\cap C)\)</span>为闭集.</p>
<ul>
<li>令<span class="math inline">\(C=\mathbb{R}^{n}\)</span>,<span class="math inline">\(X\)</span>为多面体集,则<span class="math inline">\(L_{C}=\mathbb{R}^{n}\)</span>,上述命题条件自动满足,于是<span class="math inline">\(AX\)</span>为闭,因此多面体集在线性变换下的像为闭集.</li>
<li>令<span class="math inline">\(X=\mathbb{R}^{n}\)</span>,则如果<span class="math inline">\(C\)</span>的每个属于<span class="math inline">\(N(A)\)</span>的回收方向都属于<span class="math inline">\(L_{C}\)</span>,那么<span class="math inline">\(AC\)</span>闭.特别的,<span class="math inline">\(R_{C}\cap N(A)={0}\)</span>时必然成立.</li>
</ul>
</blockquote>
<p>作为特例,该结果可以用来得到保证闭凸集向量和的闭性的条件.</p>
<blockquote>
<p><span class="math inline">\(C_{1},...,C_{m}\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空闭凸子集,假定等式<span class="math inline">\(d_{1}+...+d_{m}=0\)</span>对某些向量<span class="math inline">\(d_{i}\in R_{C_{i}}\)</span>成立就意味着<span class="math inline">\(d_{i}\in L_{C_{i}}\)</span>对所有<span class="math inline">\(i=1,...,m\)</span>成立.则<span class="math inline">\(C_{1}+...+C_{m}\)</span>是闭集.</p>
</blockquote>
<p>特别的取<span class="math inline">\(m=2\)</span>,且<span class="math inline">\(C_{1},C_{2}\)</span>有界,显然<span class="math inline">\(R_{C_{1}}=R_{C_{2}}={0}\)</span>,此时条件自动成立,那么<span class="math inline">\(C_{1}+C_{2}\)</span>为闭集.</p>
<h2 id="超平面">超平面</h2>
<h3 id="分离超平面">分离超平面</h3>
<p>超平面:</p>
<blockquote>
<p>超平面是<span class="math inline">\(\mathbb{R}^{n}\)</span>中形如<span class="math inline">\(\{x|a&#39;x=b\}\)</span>的集合,其中<span class="math inline">\(a\)</span>是<span class="math inline">\(n\)</span>维向量,<span class="math inline">\(b\)</span>为标量.</p>
</blockquote>
<p>任取超平面内的向量都满足<span class="math inline">\(a&#39;\bar x=b\)</span>,所以该超平面亦可被等价定义为<span class="math inline">\(H=\{x|a&#39;x=a&#39;\bar x\}\)</span>或<span class="math inline">\(H=\bar x+\{x|a&#39;x=0\}\)</span>,<span class="math inline">\(a\)</span>被称为<span class="math inline">\(H\)</span>的法向量.</p>
<p>集合<span class="math inline">\(\{x|a&#39;x\ge b\}\)</span>与<span class="math inline">\(\{x|a&#39;x\le b\}\)</span>被称为与超平面关联的闭半空间,两者也分别被称为正、负半空间. 集合<span class="math inline">\(\{x|a&#39;x&gt; b\}\)</span>与<span class="math inline">\(\{x|a&#39;x&lt; b\}\)</span>则被称为与超平面关联的开半空间.</p>
<p>分离:</p>
<blockquote>
<p>称集合<span class="math inline">\(C_{1}\)</span>,<span class="math inline">\(C_{2}\)</span>是可被超平面<span class="math inline">\(\{x|a&#39;x=b\}\)</span>分离的,如果两个集合分别被包含于与H关联的两个闭半空间内,也即满足:<span class="math inline">\(a&#39;x_{1}\le b\le a&#39;x_{2},\forall x_{1}\in C_{1},\forall x_{2}\in C_{2}\)</span>成立.</p>
</blockquote>
<p>支撑:</p>
<blockquote>
<p>在集合<span class="math inline">\(C\)</span>的闭包内任取向量<span class="math inline">\(\bar x\)</span>如果一超平面能将<span class="math inline">\(C\)</span>与单点集{<span class="math inline">\(\bar x\)</span>}分离,则称该超平面在<span class="math inline">\(\bar x\)</span>上支撑<span class="math inline">\(C\)</span>.即存在非零向量<span class="math inline">\(a\)</span>使得<span class="math inline">\(a&#39;\bar x\le a&#39;x,\forall x\in C\)</span>.</p>
</blockquote>
<p>等价的,由于<span class="math inline">\(\bar x\)</span>是<span class="math inline">\(C\)</span>的闭包点(closure point),所以<span class="math inline">\(a&#39;\bar x=\inf \limits_{x\in C}a&#39;x\)</span>.</p>
<p>支撑超平面定理(Supporting Hyperplane Theorem):</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集,<span class="math inline">\(\bar x\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中向量,如果<span class="math inline">\(\bar x\)</span>不是<span class="math inline">\(C\)</span>的内点,则存在通过<span class="math inline">\(\bar x\)</span>的超平面使得<span class="math inline">\(C\)</span>包含于其某个闭半空间内,也即存在向量<span class="math inline">\(a\neq 0\)</span>使得<span class="math inline">\(a&#39;\bar x\le a&#39;x,\forall x\in C\)</span>.</p>
</blockquote>
<p>分离超平面定理:</p>
<blockquote>
<p><span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集,如果<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>不交(disjoint),则存在分离超平面.即存在非零向量<span class="math inline">\(a\)</span>使得<span class="math inline">\(a&#39;x_{1}\le a&#39;x_{2},\forall x_{1}\in C_{1},\forall x_{2}\in C_{2}\)</span>.</p>
</blockquote>
<p>严格分离(strictly separate):</p>
<blockquote>
<p>称超平面<span class="math inline">\(\{x|a&#39;x=b\}\)</span>严格分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>,如果其不仅能分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>,还与两个集合都没有交点,即<span class="math inline">\(a&#39;x_{1}&lt; b&lt; a&#39;x_{2},\forall x_{1}\in C_{1},\forall x_{2}\in C_{2}\)</span>或<span class="math inline">\(a&#39;x_{2}&lt; b&lt; a&#39;x_{1},\forall x_{1}\in C_{1},\forall x_{2}\in C_{2}\)</span>成立.</p>
</blockquote>
<p>严格分离定理:</p>
<blockquote>
<p><span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集,且<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>不交(disjoint),当下列任一条件满足时,<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>能被严格分离.</p>
<ul>
<li><span class="math inline">\(C_{2}-C_{1}\)</span>是闭集.</li>
<li><span class="math inline">\(C_{1}\)</span>是闭集且<span class="math inline">\(C_{2}\)</span>是紧集.</li>
<li><span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>都是多面体.</li>
<li><span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>都是闭集且满足<span class="math inline">\(R_{C_{1}}\cap R_{C_{2}}=L_{C_{1}}\cap L_{C_{2}}\)</span>.</li>
<li><span class="math inline">\(C_{1}\)</span>是闭集,<span class="math inline">\(C_{2}\)</span>是多面体且满足<span class="math inline">\(R_{C_{1}}\cap R_{C_{2}}\subset L_{C_{1}}\)</span>.</li>
</ul>
</blockquote>
<p>上述结论的一个推论是,闭集<span class="math inline">\(C\)</span>与任何不属于<span class="math inline">\(C\)</span>的向量<span class="math inline">\(\bar x\)</span>总可以被严格分离.根据这个性质,可以得到闭凸集如下重要性质:</p>
<blockquote>
<p>集合<span class="math inline">\(C\)</span>的凸包是包含<span class="math inline">\(C\)</span>的所有闭半空间的交集.特别的,一个闭凸集是包含它的所有闭半空间的交集.</p>
</blockquote>
<h3 id="真分离超平面">真分离超平面</h3>
<p>真分离(proper separate):</p>
<blockquote>
<p>称超平面<span class="math inline">\(\{x|a&#39;x=b\}\)</span>真分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>,如果其不仅分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>且不同时包含<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>.即<span class="math display">\[\sup \limits _ {x_{1}\in C_{1}} a&#39;x_{1} \le \inf  \limits _ {x_{2}\in C_{2}} a&#39;x_{2} ,\inf \limits _ {x_{1}\in C_{1}} a&#39;x_{1} &lt; \sup  \limits _ {x_{2}\in C_{2}} a&#39;x_{2}\]</span></p>
</blockquote>
<p>下证严格分离&gt;分离,真分离&gt;分离: 由定义,显然有严格分离<span class="math inline">\(\ge\)</span>分离,真分离<span class="math inline">\(\ge\)</span>分离. 想象<span class="math inline">\(\mathbb{R}^{2}\)</span>空间中<span class="math inline">\(C_{1}=\{(x,y)|x\in [0,2],y=0\}\)</span>和<span class="math inline">\(C_{2}=\{(x,y)|x\in [1,3],y=0\}\)</span>,显然<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>为凸集,<span class="math inline">\(\{(x,y)|y=0\}\)</span>能够分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>,但不能真分离或是严格分离<span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>.即有严格分离&gt;分离,真分离&gt;分离.</p>
<p>真分离定理:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集,<span class="math inline">\(\bar x\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中向量,当且仅当<span class="math inline">\(\bar x\notin ri(C)\)</span>时,存在<span class="math inline">\(C\)</span>与<span class="math inline">\(\bar x\)</span>的真分离超平面.</p>
</blockquote>
<p>两个凸集的真分离定理:</p>
<blockquote>
<p><span class="math inline">\(C_{1}\)</span>与<span class="math inline">\(C_{2}\)</span>为<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集,则其存在真分离超平面当且仅当<span class="math inline">\(ri(C_{1})\cap ri(C_{2})=\emptyset\)</span>.</p>
</blockquote>
<p>多面体的真分离定理:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>与<span class="math inline">\(P\)</span>是<span class="math inline">\(\mathbb{R}^{n}\)</span>中的非空凸子集且<span class="math inline">\(P\)</span>是多面体.则存在分离<span class="math inline">\(C\)</span>与<span class="math inline">\(P\)</span>且其不包含<span class="math inline">\(C\)</span>的超平面存在当且仅当<span class="math inline">\(ri(C)\cap P=\emptyset\)</span>.</p>
</blockquote>
<h3 id="非竖直超平面">非竖直超平面</h3>
<p>在优化问题中,常常用支撑超平面来分析函数的上图,对于定义在<span class="math inline">\(\mathbb{R}^{n}\)</span>上的函数,其上图是<span class="math inline">\(\mathbb{R}^{n+1}\)</span>的子集,此时考虑<span class="math inline">\(\mathbb{R}^{n+1}\)</span>中的超平面,并将其法向量记为<span class="math inline">\((\mu,\beta)\)</span>的n+1维向量,其中<span class="math inline">\(\mu\in \mathbb{R}^{n}\)</span>而<span class="math inline">\(\beta\in \mathbb{R}\)</span>,如果<span class="math inline">\(\beta=0\)</span>,称该超平面是竖直的(vertical).</p>
<p>非竖直超平面定理:</p>
<blockquote>
<p><span class="math inline">\(C\)</span>为<span class="math inline">\(\mathbb{R}^{n+1}\)</span>中的非空凸子集,且<span class="math inline">\(C\)</span>不包含任何直线,标记<span class="math inline">\(\mathbb{R}^{n+1}\)</span>中的向量为<span class="math inline">\((u,w)\)</span>,其中<span class="math inline">\(u\in \mathbb{R}^{n}\)</span>而<span class="math inline">\(w\in \mathbb{R}\)</span>,则有:</p>
<ol type="1">
<li>存在非竖直超平面,使得其关联的闭半空间包含<span class="math inline">\(C\)</span>,也即存在向量<span class="math inline">\(\mu\in \mathbb{R}^{n}\)</span>,标量<span class="math inline">\(\beta\neq 0\)</span>,及标量<span class="math inline">\(\gamma\)</span>使成立:<span class="math inline">\(\mu&#39;u+\beta w\ge \gamma\)</span>.</li>
<li>如果向量<span class="math inline">\((\bar u,\bar w)\notin cl(C)\)</span>,则存在非竖直超平面使得<span class="math inline">\((\bar u,\bar w)\)</span>与<span class="math inline">\(C\)</span>严格分离.</li>
</ol>
</blockquote>
<h2 id="共轭函数">共轭函数</h2>
<p>最后一节介绍凸优化中一个基本概念叫函数的共轭变换(conjugate transformation),它有另外一个名字:Legendre–Fenchel transformation.</p>
<h3 id="共轭函数-1">共轭函数</h3>
<p>共轭函数(conjugate function):</p>
<blockquote>
<p>扩充的实值函数<span class="math inline">\(f:\mathbb{R}^{n}\mapsto[-\infty,+\infty]\)</span>,其共轭函数<span class="math inline">\(f^{+}:\mathbb{R}^{n}\mapsto[-\infty,+\infty]\)</span>如下定义:<span class="math display">\[f^{+}(y)=\sup  \limits _ {x\in \mathbb{R}^{n}}\{x&#39;y-f(x)\},\forall y\in \mathbb{R}^{n}\]</span></p>
</blockquote>
<p>值得注意的是,无论<span class="math inline">\(f\)</span>是否为凸函数,其共轭函数<span class="math inline">\(f^{+}\)</span>总是闭凸函数,因为仿射函数族<span class="math inline">\(x&#39;y-f(x),\forall x\in dom(f)\)</span>处处收敛到<span class="math inline">\(f^{+}\)</span>,也即<span class="math inline">\(f^{+}\)</span>是它们的上确界.还需注意的是,无论<span class="math inline">\(f\)</span>是否为真函数,<span class="math inline">\(f^{+}\)</span>都不一定是真函数.</p>
<h3 id="双重共轭函数">双重共轭函数</h3>
<p>双重共轭函数(double conjugate):</p>
<blockquote>
<p>考虑共轭函数<span class="math inline">\(f^{+}\)</span>的共轭函数,记作<span class="math inline">\(f^{+ +}\)</span>.</p>
</blockquote>
<h3 id="共轭定理">共轭定理</h3>
<p>共轭定理:</p>
<blockquote>
<p><span class="math inline">\(f^{+}\)</span>为函数<span class="math inline">\(f:\mathbb{R}^{n}\mapsto[-\infty,+\infty]\)</span>的共轭函数,<span class="math inline">\(f^{++}\)</span>为对应双重共轭函数.则有:</p>
<ul>
<li>恒有<span class="math inline">\(f(x)\ge f^{++}(x),\forall x\in \mathbb{R}^{n}\)</span>.</li>
<li>如果<span class="math inline">\(f\)</span>是凸函数,则如果<span class="math inline">\(f,f^{+},f^{++}\)</span>中有任一函数是真函数,则另外两个也是真函数.</li>
<li>如果<span class="math inline">\(f\)</span>是真的闭凸函数,则<span class="math inline">\(f=f^{++}\)</span>.</li>
<li><span class="math inline">\(f^{++}\)</span>与凸闭包函数<span class="math inline">\(\hat{cl}f\)</span>相等,此外,如果<span class="math inline">\(\hat{cl}f\)</span>是真函数,则<span class="math inline">\(\hat{cl}f=f^{++}\)</span>.</li>
</ul>
</blockquote>
<p>上述定理不难看出共轭函数与凸闭包函数的相似之处,不同的是共轭函数完全通过非竖直超平面定义的,而图闭包函数的定义既用到了非竖直超平面,也用到了竖直超平面.</p>
<h3 id="两个例子">两个例子</h3>
<p>例1(示性/支撑函数的共轭): 非空集合<span class="math inline">\(X\)</span>示性函数(indicator function)的定义如下:<span class="math display">\[\delta_{X}(x)=0,x\in X\]</span>而<span class="math inline">\(\delta_{X}(x)\)</span>的共轭函数:<span class="math display">\[\sigma_{X}(x)=\sup  \limits _ {x\in X} y&#39;x\]</span>并称为X的支撑函数.由共轭定理第四条,<span class="math inline">\(X,cl(X),conv(X),cl(conv(X))\)</span>有相同的支撑函数.</p>
<p>例2(锥的支撑函数): <span class="math inline">\(C\)</span>为凸锥,根据例1,C的示性函数<span class="math inline">\(\delta_{C}\)</span>的共轭即<span class="math inline">\(C\)</span>的支撑函数<span class="math display">\[\sigma_{C}(y)=\sup  \limits _ {x\in C} y&#39;x\]</span>由于<span class="math inline">\(C\)</span>是锥体,可得:<span class="math display">\[\sigma_{C}(y)=0,y&#39;x\le 0,\forall x \in C;\infty,otherwise\]</span>因此支撑函数<span class="math inline">\(\sigma_{C}\)</span>恰为如下锥<span class="math inline">\(C^{+}\)</span>的示性函数<span class="math inline">\(\delta_{C^{+}}\)</span>,<span class="math display">\[C^{+}=\{y|y&#39;x\le 0,\forall x \in C\}\]</span>称<span class="math inline">\(C^{+}\)</span>为<span class="math inline">\(C\)</span>的极锥(polar cone). 最后是常见的函数的共轭变换: <img src="/pictures/convex%20conjugates.png" /></p>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>凸优化</tag>
      </tags>
  </entry>
  <entry>
    <title>拉普拉斯方法</title>
    <url>/2018/09/28/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>前言: 本文主要是关于拉普拉斯近似方法, 这是一种用来近似计算积分的方法, 主要参考了维基百科及PRML一书6.4节. <span id="more"></span></p>
<hr />
<h2 id="拉普拉斯近似方法">拉普拉斯近似方法</h2>
拉普拉斯近似方法(<a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace's method</a>)是一种近似计算积分的方法, 目标积分形如<span class="math display">\[\int_a^be^{Mf(x)}dx\]</span>其中<span class="math inline">\(f(x)\)</span>要求是二次可微的, <span class="math inline">\(M\)</span>要求是一个较大的数. 其想法是假定<span class="math inline">\(x_0\)</span>是<span class="math inline">\(f(x)\)</span>唯一的最大值点, 因此<span class="math inline">\(f(x_0)&gt;f(x)\)</span>, 乘上一个较大的M, 在经过指数函数的作用, 其差值事实上会变得很大, 不难想象随着<span class="math inline">\(M\)</span>的增大积分内的函数会越来越像一个单峰函数, 事实上确实会逼近对一个正态分布进行积分的值. 举个例子<span class="math inline">\(f(x)=sinx/x\)</span>, 分别画出<span class="math inline">\(e^{sinx/x}\)</span>和<span class="math inline">\(e^{3sinx/x}\)</span>的函数图像:
<div data-align="center">
<img src="/pictures/laplace_method/exp%283sinx_x%29.png" width="60%" height="50%" />
</div>
<div data-align="center">
<img src="/pictures/laplace_method/exp%28sinx_x%29.png" width="60%" height="50%" />
</div>
<p>考虑多元情形, 假定<span class="math inline">\(x_0\)</span>为f(x)的最大值点, 因此有<span class="math inline">\(\nabla f(x_0)=0\)</span>且Hessian矩阵<span class="math inline">\(H(x_0)负定\)</span>. 将<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x_0\)</span>处泰勒展开得<span class="math display">\[f(x)\approx f(x_0)+\frac{1}{2!}(x-x_0)^TH(x_0)(x-x_0)\]</span>此时积分近似为<span class="math display">\[\int_a^be^{Mf(x)}dx\approx e^{Mf(x_0)}\int_a^b e^{\frac{M}{2}(x-x_0)^TH(x_0)(x-x_0)}dx\]</span>假设<span class="math inline">\(x_0\)</span>包含在区间<span class="math inline">\([a,b]\)</span>内, 那么右式可以直接视为在<span class="math inline">\((-\infty,+\infty)\)</span>上的积分, 因为随着<span class="math inline">\(M\)</span>的增大区间之外的积分会越来越小. 而此时右边的积分即为一个对多元正态分布的积分, 且该积分可以通过变量代换<span class="math inline">\(t=H(x_0)^{\frac{1}{2}}(x-x_0)\)</span>化成标准的多元正态分布计算出来:<span class="math display">\[\int_a^b e^{\frac{M}{2}(x-x_0)^TH(x_0)(x-x_0)}dx\approx \int_{-\infty}^{+\infty} e^{\frac{M}{2}(x-x_0)^TH(x_0)(x-x_0)}dx=\left(\frac{2\pi}{M}\right)^\frac{d}{2}\left|-H(x_0)\right|^{-\frac{1}{2}}(M\to+\infty)\]</span>这里的<span class="math inline">\(d\)</span>指的是自变量<span class="math inline">\(x\)</span>的维度, <span class="math inline">\(H(x_0)\)</span>指<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x_0\)</span>处的Hessian矩阵. 因此得到结论:<span class="math display">\[\int_a^be^{Mf(x)}dx\approx  e^{Mf(x_0)}\left(\frac{2\pi}{M}\right)^\frac{d}{2}\left|-H(x_0)\right|^{-\frac{1}{2}}(M\to+\infty)\]</span>本质上来说, 倘若暂时先不考虑<span class="math inline">\(M\)</span>的大小, 用二阶泰勒展开对<span class="math inline">\(f(x)\)</span>进行近似的方法实际上是把被积函数视为多元正态分布. 一元情形的相关定理描述及证明请移步<a href="https://en.wikipedia.org/wiki/Laplace%27s_method">维基百科</a>, 最后来看一个更一般的结论:<span class="math display">\[\int_a^bh(x)e^{Mf(x)}dx\approx  h(x_0)e^{Mf(x_0)}\left(\frac{2\pi}{M}\right)^\frac{d}{2}\left|-H(x_0)\right|^{-\frac{1}{2}}(M\to+\infty)\]</span>这里要求<span class="math inline">\(h(x)&gt;0\)</span>.</p>
<hr />
<h2 id="例子">例子</h2>
<p>例子参考了PRML一书6.4节, 来自高斯过程分类中的预测问题, 目标也是求一个积分<span class="math display">\[P(a_{N+1}\mid t_N)=\int P(a_{N+1}\mid a_N)P(a_N\mid t_N)da_N\]</span>这里的目标变量<span class="math inline">\(t\in\{0, 1\}\)</span>为二分变量; 被积函数第一部分<span class="math inline">\(P(a_{N+1}\mid a_N)\)</span>是个正态分布, 即为高斯过程回归做预测时的核心结论. 本质上来说, 拉普拉斯近似方法把被积函数或者是被积函数的一部分代换成多元正态分布<span class="math inline">\(N(\mu, \Sigma)\)</span>, 这里仅把被积函数的第二部分代换掉. 回想多元正态分布的<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">密度函数</a>, 因此只要计算<span class="math inline">\(\log P(a_N\mid t_N)\)</span>关于变量<span class="math inline">\(a_N\)</span>的梯度<span class="math inline">\(\nabla\)</span>和Hessian矩阵<span class="math inline">\(\nabla^2\)</span>, 此时<span class="math inline">\(\mu\)</span>即为<span class="math inline">\(\nabla=0\)</span>的解, <span class="math inline">\(\Sigma=-\nabla^2\)</span>. 接着由两个正态分布积分的一些性质(PRML一书2.115), 可以得到目标积分的近似结果仍为一个正态分布. 然后我们可以用它来做预测<span class="math display">\[P(t_{N+1}=1\mid t_N)=\int P(t_{N+1}=1\mid a_{N+1})P(a_{N+1}\mid t_N)da_{N+1}\]</span>具体计算时被积函数的第一部分利用自己本身的性质近似成了正态分布的分布函数(而非密度函数)(PRML一书4.153). <code>p.s.</code>: 高斯过程分类中的参数也可以用拉普拉斯近似方法得到, 这里就不再赘述, 最后建议细细读一读PRML一书6.4节.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>拉普拉斯方法</tag>
        <tag>积分近似</tag>
      </tags>
  </entry>
  <entry>
    <title>智能时代-读书笔记</title>
    <url>/2018/05/02/%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>前言:之前读过吴军博士的浪潮之巅,让我对各大IT企业有了一个了解,这次又买了吴博士的智能时代,特此记录我的一些读书笔记. ps:由于笔者从本科到研究生都在数学系,所以习惯用从定义到例子这样的方式去理解一个概念,定义内容主要参考了维基百科.</p>
<span id="more"></span>
<hr />
<h2 id="数据-人类文明的基石">数据-人类文明的基石</h2>
<p>如果我们把资本和机械动能作为大航海时代以来全球近代化的推动力,那么数据将成为智能革命的核心动力. <a href="https://en.wikipedia.org/wiki/Data">数据</a>:</p>
<blockquote>
<p>Data is a set of values of qualitative or quantitative variables.</p>
</blockquote>
<p>值得注意的是数据虽然本身是客观存在的,但他的范畴是随着文明进程不断变化和扩大的;计算机出现以前,一般书籍上的文字不认为是数据,而在今天它确实很重要的数据形式,可以想象不久的将来数据的范围一定会进一步扩大.</p>
<p><a href="https://en.wikipedia.org/wiki/Information">信息</a>:</p>
<blockquote>
<p>Information is any entity or form that resolves uncertainty or provides the answer to a question of some kind.</p>
</blockquote>
<p>通常我们的目标就是从数据中挖掘出有用的信息来为我们所用,但我们能拿到的数据往往是有用的数据和没用的数据(伪造数据)混在一起,这无疑会干扰我们获取有用的信息.</p>
<p><a href="https://en.wikipedia.org/wiki/Mathematical_model">数学模型</a>:</p>
<blockquote>
<p>A mathematical model is a description of a system using mathematical concepts and language.A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.</p>
</blockquote>
<p>很多时候,我们没法直接使用数据,但我们可以将相关的信息量化,然后通过数学模型,间接的得到想要的信息. 另外我们通常对数据有两个方面的要求,质和量,质表示得到数据必须具有一定的代表性,量则是数据量必须充足.</p>
<p><a href="https://en.wikipedia.org/wiki/Data-driven">数据驱动</a>:</p>
<blockquote>
<p>The adjective data-driven means that progress in an activity is compelled by data.</p>
</blockquote>
<p>数学模型的选择不是一件容易的事情,其中数据驱动方法是目前的主流,它的主要内容就是选择一个或简单或复杂的模型,然后用很多数据去拟合.比方说DNN的数学模型可以说非常简单了,但它往往都有很多层隐藏层,从这个层面讲这个模型可以说的上是复杂. 数据驱动方法最大优势在于他可以最大程度上得益于计算机技术的进步,尽管一开始数据量不够、计算力不够时会显得粗糙,随着时间的推移,数据驱动方法可以非常准确.相比之下非数据驱动方法的进步需要理论上的突破,因此改进周期长.</p>
<hr />
<h2 id="大数据与机器智能">大数据与机器智能</h2>
<p>在有大数据之前,计算机并不擅长解决需要人类智能的问题,但今天这些问题能够得以解决,其核心就是变智能问题为数据问题,由此开始了新一轮技术革命-智能革命. 机器智能又名<a href="https://en.wikipedia.org/wiki/AI">人工智能</a>:</p>
<blockquote>
<p>In computer science AI research is defined as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.</p>
</blockquote>
<p>人工智能1.0:1956年夏天的<a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">Dartmouth workshop</a></p>
<p><a href="https://en.wikipedia.org/wiki/Big_data">大数据</a>:</p>
<blockquote>
<p>Big data is data sets that are so voluminous and complex that traditional data-processing application software are inadequate to deal with them.</p>
</blockquote>
<p>大数据的三个特征:</p>
<ul>
<li>Vast(2005是大数据的元年,第一次参加<a href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology">NIST</a>评测的Google机器翻译团队以巨大优势取得第一,原因在于使用了万倍的数据量建立了六元模型而不是三元模型).</li>
<li>Variety(百度发布的<a href="https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E5%8D%81%E5%A4%A7%E5%90%83%E8%B4%A7%E7%9C%81%E5%B8%82%E6%8E%92%E8%A1%8C%E6%A6%9C/15408979">中国十大吃货省市排行榜</a>说明了正是大数据的多样性,我们才能从中获得更多有价值的结果)</li>
<li>Completeness(<a href="https://en.wikipedia.org/wiki/Nate_Silver">Nade Silver</a>通过搜集社交网络上的相对反映真实想法的数据,对2012美国大选做出非常漂亮的预测)</li>
</ul>
<hr />
<h2 id="思维的革命">思维的革命</h2>
<p>数据所包含的信息能帮助我们消除不确定性,数据的相关性一定程度上能取代原来的因果关系. 上一章我们从技术层面分析了大数据的重要性,这一张我们将从方法论层面描述其重要性. <a href="https://en.wikipedia.org/wiki/Mechanical_philosophy">机械思维</a>:</p>
<blockquote>
<p>the universe is reducible to completely mechanical principles—that is, the motion and collision of matter.</p>
</blockquote>
<p>机械思维的形成:</p>
<ul>
<li><p>欧几里得创立了基于公理化体系的几何学</p></li>
<li><p>托勒密总结出:通过观察获得数学模型的雏形,然后利用数据来细化模型</p></li>
<li><p>笛卡尔总结出:大胆假设,小心求证</p></li>
<li><p>牛顿不仅把欧几里得通过逻辑建立起来的方法论从数学扩展到自然科学领域,而且把托勒密用机械运动描述天体的规律扩展到对世界任何规律的描述,后来人们将牛顿的方法论概括为机械思维,其核心思想有三:</p>
<ol type="1">
<li>世界的变化规律是确定的</li>
<li>规律不仅可以被认知,而且可以用简单的语言描述</li>
<li>这些规律可以在未知领域指导实践</li>
</ol></li>
</ul>
<p>机械思维的局限性:</p>
<ul>
<li>否认了不确定性(量子力学中测不准原理<span class="math inline">\(\Delta t\cdot\Delta p&gt;\varepsilon\)</span>)</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Entropy">熵</a>:</p>
<blockquote>
<p>In statistical mechanics, entropy is an extensive property of a thermodynamic system. It is closely related to the number Ω of microscopic configurations (known as microstates) that are consistent with the macroscopic quantities that characterize the system (such as its volume, pressure and temperature). Under the assumption that each microstate is equally probable, the entropy S is the natural logarithm of the number of microstates, multiplied by the Boltzmann constant <span class="math inline">\(k_{B}\)</span>. Formally,<span class="math inline">\(S=k_{B}\ln\Omega\)</span>.</p>
</blockquote>
<p>香农在信息论中借用了热力学中熵的概念,描述了一个信息系统的不确定性.<a href="https://en.wikipedia.org/wiki/Information_theory">信息论</a>主要包括自信息(self-information),互信息(mutual-information),香农第一定律(噪声信道编码定理)和香农第二定律:</p>
<ul>
<li>自信息:<span class="math inline">\(H(X)=\mathbb {E}_{X}[I(x)]=-\sum_{x\in \mathbb {X} }p(x)\log p(x)\)</span></li>
<li>互信息:<span class="math inline">\(I(X;Y)=\mathbb {E}_{X,Y}[SI(x,y)]=\sum_{x,y}p(x,y)\log {\frac {p(x,y)}{p(x)\,p(y)}}\)</span></li>
<li>香农第一定律(<a href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">Shannon's source coding theorem</a>):对于信号源发出的所有信息设置一种编码,那么编码的平均长度一定大于该源的信息熵;且一定存在一种编码,这种编码的平均长度能无限接近于它的信息熵(这种编码又称为是<a href="https://en.wikipedia.org/wiki/Huffman_coding">霍夫曼编码</a>)</li>
<li>香农第二定律(<a href="https://en.wikipedia.org/wiki/Noisy-channel_coding_theorem">Noisy-channel coding theorem</a>):信息传播速率不可能超过信道的容量</li>
</ul>
<p>关于信息论,还有一个原理必须了解,那就是<a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">最大熵原理</a>:</p>
<blockquote>
<p>The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge is the one with largest entropy, in the context of precisely stated prior data (such as a proposition that expresses testable information).</p>
</blockquote>
<p>这原理的大意为,当我们要对未知寻求一个概率模型时,这个模型应当满足我们所有已经看到的数据,但对未知的情况不要做任何假设.这事实上截然不同于笛卡尔总结出:大胆假设,小心求证的方法论.</p>
<p>在无法确定因果关系时,数据所包含的信息能帮助我们消除不确定性,研究数据的相关性为我们从数据中得到信息提供一种更高效的方法(而不是直接研究因果关系),这两点便是大数据思维的核心.</p>
<hr />
<h2 id="大数据与商业">大数据与商业</h2>
<p>可以预见的是,在未来,大数据会如同水电,由专门的公司提供给全社会使用.</p>
<hr />
<h2 id="大数据和智能革命的技术挑战">大数据和智能革命的技术挑战</h2>
<p>大数据的数据量大、维度多、数据完备等特点使得它从数据收集到存储和处理都与过去的数据方法有很大不同,因此,这需要我们在技术上采用不同的方法.</p>
<hr />
<h2 id="未来智能化产业">未来智能化产业</h2>
<p>现有产业+机器智能=新产业</p>
<hr />
<h2 id="智能革命和未来社会">智能革命和未来社会</h2>
<p>在美国,很多道路在交通高峰期要求车上必须有两个及以上人才能使用快速车道,这些车道被称为拼车车道.</p>
<hr />
<h3 id="区块链">区块链</h3>
<p>区块链(<a href="https://en.wikipedia.org/wiki/Blockchain">Block Chain</a>):</p>
<ul>
<li>Block:一个账户存储信息</li>
<li>Chain:一连串的交易信息</li>
</ul>
<p>区块链的一项重要应用比特币实际上是由随机数算法产生的随机数,这个随机数在整个互联网上是唯一的,而且是可以验其真伪的.比特币在被矿工挖出时,就产生一个带有这样的特殊随机数的Block,当这个比特币通过交易到达第二个人手里时,在该Block中就记录下了这笔交易的信息,这个过程本质上是一个加密信息传输过程,一旦交易完成了,它就会被广播到整个互联网上.所有的比特币散布在整个互联网上,且通过公开秘钥来发送和传播,且没有一个中心去控制.</p>
<p>比特币(<a href="https://en.wikipedia.org/wiki/Bitcoin">Bitcion</a>):</p>
<ul>
<li>a cryptocurrency and worldwide payment system</li>
<li>without a central bank or single administrator</li>
<li>the network is peer-to-peer</li>
<li>transactions are verified by network nodes through the use of cryptography and recorded in a public distributed ledger called a blockchain</li>
</ul>
<p>区块链的另外一项重要应用是商品溯源,如果一个商品制造出来时产生这样一个区块链,并且它在被运输和交易时利用区块链记录全过程,那么当最终消费者购买到这个商品后,他可以看到这个商品是如何一步步到自己手上的.因为商品与区块链是一一对应的,这样子就能从理论上杜绝假货.</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>智能时代</tag>
      </tags>
  </entry>
  <entry>
    <title>最小作用量原理</title>
    <url>/2019/03/28/%E6%9C%80%E5%B0%8F%E4%BD%9C%E7%94%A8%E9%87%8F%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>前言: 最近在读最佳可能的世界一书, 这里简单分享一下最小作用量原理. <span id="more"></span></p>
<hr />
<h3 id="hero-of-alexandria-自然将永远选择最短的路线">Hero of Alexandria: 自然将永远选择最短的路线</h3>
<p>Hero of Alexandria(希罗)是公元后10年-70年时期来自亚历山大的数学家和工程师, 他的作品被Marin Cureau de la Chambre(马林·库若)读到; 1657年, 库若写了一篇论文"关于光"并把它寄给了Pierre de Fermat(皮埃尔·德·费马), 这篇文章陈述了反射定律, 根据这个原理, "自然将永远选择最短的路线", 正如下图所示: <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Reflection_angles.svg/800px-Reflection_angles.svg.png" width="50%" /></p>
<hr />
<h3 id="pierre-de-fermat-光沿着最快的路线传播">Pierre de Fermat: 光沿着最快的路线传播</h3>
<p>费马在给库若的回信表示了对希罗的观点的认同, 并在对光的折射的研究中提出了新的描述: 光沿着最快的路线传播; 5年后的1662年, 费马给出了他的结果, 并成功导出了Descartes(笛卡尔)在1637年推导出的正弦定律, 只不过笛卡尔认为光在水中比在空气中的传播速度快, 而费马的想法与此相反.</p>
<hr />
<h3 id="calude-clerselier-自然没有意识-假设自然致力于减少某种转化时间不是一种科学解释">Calude Clerselier: 自然没有意识, 假设自然致力于减少某种转化时间不是一种科学解释</h3>
<p>同年1662年5月, 已经是笛卡尔学派的领袖的Calude Clerselier(克莱尔塞利埃)在给费马的回信中表达了如下反对: 自然不会在一些可能性中选择要走的路, 任何时候, 它只会发现一扇敞开的门, 并且穿过那扇门. 这种世界观被称为<strong>决定主义</strong>, 简言之, 在穿过第一扇门时, 整条路就已经决定了. 决定主义后来被Isaac Newton(艾萨克·牛顿)发现, Albert Einstein(爱因斯坦)也从未放弃这一观点: 上帝不玩骰子.</p>
<blockquote>
<p>费马回应: 光就想有那种渴望和方法一样, 而数学问题可能不是对现实的某些更深层次发生问题的确切描述, 至于它代表什么是应该留给哲学家考虑的问题.</p>
</blockquote>
<p>费马的回应也被Niels Bohr(尼尔斯·玻尔)用来反对爱因斯坦:</p>
<blockquote>
<p>我不知道; 我正在说的是, 使用量子力学和概率论, 我可以做出非常精确地预测.</p>
</blockquote>
<hr />
<h3 id="maupertuis-光选择了一条具有真正好处的路线-它走的路线作用量最小">Maupertuis: 光选择了一条具有真正好处的路线: 它走的路线作用量最小</h3>
<p>1744年, Maupertuis(莫培督)提出: 既然光不能同时选择最短的和最快的路线, 那它为什么选择这条路而不是那条路呢? 光实际上选择了最小化作用量<span class="math inline">\(lvm\)</span>的线路, 这里<span class="math inline">\(l\)</span>表示距离, <span class="math inline">\(v\)</span>表示速度, <span class="math inline">\(m\)</span>表示物体质量; 1751年, 一位叫Koenig(科尼格)的提到Gottfried Wilhelm Leibniz(弗里德·威廉·莱布尼兹)在1707年所写的信中提出过作用量的概念, 其应当是距离, 时间和质量的乘积<span class="math inline">\(ltm\)</span>, 科尼格因此却被莫培督控告造假; 1752年, 莫培督在他的论宇宙一书声称了一个物理学同形而上学甚至道德的统一: 由最小作用量原理统治的世界一定是最好的.</p>
<blockquote>
<p>Voltaire: "圣马洛本地人早就染上了一种慢性疾病, 有人把这种病称为是追命惴栗, 也有人把它称为喜好权利." 他病得很重, 他写文章攻击医药和上帝的存在的证明. 他最后达到认为它比前一世纪的某个巨人莱布尼兹更伟大.</p>
</blockquote>
<p>Voltaire(伏尔泰)的文章迅速获得成功, 莫培督在欧洲受尽嘲讽, 并于1759年落魄去世; 而伏尔泰于1759年出版的小说Candide(老实人)至今仍被人阅读.</p>
<hr />
<h3 id="william-rowan-hamilton-作用量不是尽可能的小或尽可能的大-而是稳定">William Rowan Hamilton: 作用量不是尽可能的小或尽可能的大, 而是稳定</h3>
<p>哈密顿为最小作用量原理找到了正确的数学框架, 至此, 关于最小作用量原理才有了一个完全正确的论述, 这也意味着最小作用量原理最后成为了形而上学的工具. 直到20世纪, 它的有用性才被人们逐渐发现.</p>
<hr />
<h3 id="写在最后">写在最后</h3>
<p>如今最小作用量原理早已不再被认为是自然界的法则, 而是被当成了探索世界的一个数学工具, 比如说最优化, 变分学. 事实上, 最佳可能的世界这本科普书也是上学期变分学老师推荐给我们看的, 这里我强烈推荐一下此书.</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>最小作用量原理</tag>
      </tags>
  </entry>
  <entry>
    <title>期望传播</title>
    <url>/2018/10/02/%E6%9C%9F%E6%9C%9B%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p>前言: 本文主要是关于期望传播, 这是一种用指数族分布去近似复杂目标分布的方法, 其本质上为变分推断逆过来, 主要参考了Thomas P Minka的博士论文A family of algorithms for approximate Bayesian inference、维基百科及PRML一书10.7节.</p>
<span id="more"></span>
<hr />
<h2 id="指数族分布">指数族分布</h2>
<p>首先来看一个重要概念<a href="https://en.wikipedia.org/wiki/Exponential_family">指数族分布</a>, 称<span class="math inline">\(X\)</span>为指数族分布, 若其密度函数:<span class="math display">\[f(x\mid \theta )=h(x)\exp \left(\eta (\theta )^TT(x)-A(\theta )\right), \theta\in\Theta\]</span>其中:</p>
<ul>
<li><span class="math inline">\(T(x)\)</span>为<span class="math inline">\(X\)</span>的充分统计量. 样本<span class="math inline">\(x=\{x_1,..,x_m\}\)</span>, <span class="math inline">\(x_i\sim f(x\mid \theta)\)</span>, 统计量(比方说)<span class="math display">\[T(x)=(\bar{x}, \frac{1}{m}\sum_{i=1}^n (x_i-x)^2)\]</span>可理解为对样本<span class="math inline">\(x\)</span>的粗加工, 其将对样本的信息量由<span class="math inline">\(m\)</span>维降至<span class="math inline">\(2\)</span>维; 而充分统计量<span class="math inline">\(T(x)\)</span>则是那些能够充分说明原样本的统计量, 定义为: 若<span class="math inline">\(x\mid T(x)\)</span>的条件分布与未知参数<span class="math inline">\(\theta\)</span>无关, 则称<span class="math inline">\(T(x)\)</span>为充分统计量. <span class="math inline">\((\bar{x}, \frac{1}{m}\sum_{i=1}^n (x_i-\bar{x})^2)\)</span>就是正态分布的一个充分统计量(充分统计量不唯一).</li>
<li>当<span class="math inline">\(\eta (\theta )=\theta\)</span>取实值, 此时指数族分布称为是自然形式的指数族分布.</li>
<li><span class="math inline">\(A(\theta)\)</span>称为是归一化因子<span class="math display">\[A(\theta )=\ln \left(\int h(x)\exp(\theta ^T T(x))\,\mathrm {d} x\right)\]</span>它的存在保证了这个函数是个密度函数. 有时这个归一化因子会写在外面, 因此指数族分布的密度函数有如下等价形式:<span class="math display">\[f(x\mid \theta )=h(x)g(\theta )\exp \left(\theta ^T T(x)\right)\]</span>此时<span class="math inline">\(g(\theta)\)</span>满足<span class="math inline">\(g(\theta)^{-1}=\int h(x)\exp \left(\theta ^T T(x)\right)dx\)</span>.</li>
<li>参数空间<span class="math inline">\(\Theta\)</span>中的每一个元<span class="math inline">\(\theta\)</span>都应当使得归一化因子为正数: <span class="math inline">\(A(\theta)\in(0,+\infty)\)</span>; 而所使得归一化因子为正数的<span class="math inline">\(\theta\)</span>构成的参数空间<span class="math inline">\(\Theta\)</span>称为是自然参数空间, 且自然形式下的指数族分布的自然参数空间为欧式空间中的凸集.</li>
</ul>
<p>指数族分布是一大类分布的集合, 包括正态分布, 指数分布, 二项分布, 泊松分布, Beta分布, Gamma分布, 狄利克雷分布等, 具体见<a href="https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">维基百科上的总结</a>, 有兴趣的读者不妨拿一个验证一下. 以密度函数的第二个形式为例, 归一化因子满足条件的等式的两边对参数<span class="math inline">\(\theta\)</span>求导可以得到自然形式指数族分布的一个重要性质, 能轻易表示出<span class="math inline">\(T(x)\)</span>的各阶矩: <span class="math display">\[g(\theta)^{-2}\frac{dg(\theta)}{d\theta} =  \int h(x)\exp \left(\theta ^T T(x)\right)T(x)dx \]</span>两边同乘<span class="math inline">\(g(\theta)\)</span>得:<span class="math display">\[-\nabla\ln g(\theta)=\mathbb{E}_{f(x\mid \theta )}T(x)\]</span>再次对<span class="math inline">\(\theta\)</span>求导可得:<span class="math display">\[-\nabla^2 \ln g(\theta)=\mathbb{E}_{f(x\mid \theta )}T^2(x)\]</span>当然如果此时密度函数取第一个形式, 形式会更简单: <span class="math display">\[\nabla A(\theta)=\mathbb{E}_{f(x\mid \theta )}T(x), \nabla^2 A(\theta)=\mathbb{E}_{f(x\mid \theta )}T^2(x)\]</span>关于更多指数族分布或是统计量可参考数理统计的教材, 比如韦博成的参数统计教程, 陈希孺的高等数理统计学.</p>
<hr />
<h2 id="期望传播">期望传播</h2>
<p>下面来看期望传播:</p>
<ul>
<li>首先简单回顾变分推断, 变分推断做的是通过最小化KL散度(以EM算法中E步为例, 详见<a href="https://msgsxj.cn/2018/09/05/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95/">贝叶斯方法</a>的3.1节.):<span class="math display">\[\mathcal{KL}(q(T)\Vert P(T\mid X, \theta))\]</span> <span class="math inline">\(\mathcal{KL}\)</span>散度的定义见<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">维基百科</a>, 这里的<span class="math inline">\(X\)</span>为机器学习中的样本矩阵, 已给定, 不同于变量<span class="math inline">\(x\)</span>; <span class="math inline">\(\theta\)</span>为未知参数; <span class="math inline">\(T\)</span>本为隐变量, 这里为了不和上文<span class="math inline">\(T\)</span>重名, 简记为<span class="math display">\[\mathcal{KL}(q(x)\Vert P(x\mid X, \theta))\]</span>这里做的事情就是从一个相对简单的单分布族<span class="math inline">\(Q\)</span>中找到一个<span class="math inline">\(q(x)\)</span>去逼近复杂分布<span class="math inline">\(P(x\mid X, \theta)\)</span>. 和下文的期望传播以示区别, 这里我们称之为<span class="math inline">\(q(x)\)</span>从左逼近<span class="math inline">\(P(x\mid X, \theta)\)</span>, 这种从左逼近的方法的最大的好处就是不必去考虑关于<span class="math inline">\(X, \theta\)</span>的分布, 即不必计算<span class="math inline">\(P(X, \theta)\)</span>.</li>
<li>期望传播(Expectation Propagation: EP)做的则是最小化逆过来的KL散度:<span class="math display">\[\mathcal{KL}( P(x\mid X, \theta)\Vert q(x))\]</span>这里我们称之为<span class="math inline">\(q(x)\)</span>从右逼近<span class="math inline">\(P(x\mid X, \theta)\)</span>, 这种从右逼近的方法的好处就是, 若这里的<span class="math inline">\(q(x)\)</span>限定在指数族分布的范围内, 即<span class="math inline">\(q(x\mid\tilde{\theta})=h(x)g(\tilde{\theta} )\exp \left(\tilde{\theta} ^T T(x)\right)\)</span>(<span class="math inline">\(\tilde{\theta}\)</span>与<span class="math inline">\(\theta\)</span>是不同的未知参数), 此时最小化上述<span class="math inline">\(\mathcal{KL}\)</span>散度就等价于充分统计量<span class="math inline">\(T(x)\)</span>关于两个分布<span class="math inline">\(q(x)\)</span>和<span class="math inline">\(P(x\mid X, \theta)\)</span>的期望相等: <span class="math display">\[\begin{aligned} \mathcal{KL}( P(x\mid X, \theta)\Vert q(x)) &amp; =  \int P(x\mid X, \theta)\ln\frac{P(x\mid X, \theta)}{q(x)}dx\\\
&amp; =  \int P(x\mid X, \theta)\ln\frac{P(x\mid X, \theta)}{h(x)g(\tilde{\theta} )\exp \left(\tilde{\theta} ^T T(x)\right)}dx\\\
(let \, P=P(x\mid X, \theta)) \,&amp; = \int P\ln Pdx - \int P\ln h(x)dx-\int P\ln g(\tilde{\theta})dx- \int P\tilde{\theta} ^T T(x)dx\\\
&amp; = \int P\ln Pdx - \int P\ln h(x)dx-\ln g(\tilde{\theta})- \tilde{\theta} ^T \mathbb{E}_{P}T(x) \end{aligned}\]</span> 此时令关于未知参数 <span class="math inline">\(\tilde{\theta}\)</span> 的梯度为<span class="math inline">\(0\)</span>就可以得到<span class="math display">\[-\nabla\ln g(\tilde{\theta})=\mathbb{E}_{P(x\mid X, \theta)}T(x)\]</span>再利用上节中能轻易表示出<span class="math inline">\(T(x)\)</span>各接矩的性质, 这里仅用一阶矩: <span class="math display">\[-\nabla\ln g(\tilde{\theta})=\mathbb{E}_{q(x\mid\tilde{\theta})}T(x)\]</span>得到结论: <span class="math display">\[\mathbb{E}_{P(x\mid X, \theta)}T(x)=\mathbb{E}_{q(x\mid\tilde{\theta})}T(x)\]</span>即<span class="math inline">\(T(x)\)</span>关于目标分布<span class="math inline">\(P(x\mid X, \theta)\)</span>期望的信息能够传递下去. 以正态分布为例, <span class="math inline">\(q(x\mid \mu, \Sigma)=N(\mu, \Sigma)\)</span>:
<span class="math display">\[\begin{aligned} q(x\mid \mu, \Sigma) &amp; =  {\frac {\exp \left(-{\frac {1}{2}}(x- \mu )^{T }\Sigma ^{-1}(x-\mu)\right)}{\sqrt {(2\pi )^{k}|\Sigma|}}}\\\
&amp; =  {\frac {\exp \left(-{\frac {1}{2}}x^{ T }\Sigma ^{-1}x+x^{ T }\Sigma ^{-1}\mu-{\frac {1}{2}}\mu^{ T }\Sigma ^{-1}\mu\right)}{\sqrt {(2\pi )^{k}|\Sigma|}}} \end{aligned}\]</span>
&amp; = {} \end{aligned}只需考虑<span class="math inline">\(x\)</span>与参数<span class="math inline">\(\mu, \Sigma\)</span>分不开的那些项, 得<span class="math inline">\(T(x)=(x, xx^T)\)</span>(<code>p.s.</code>这里笔者并不是很明白怎么去算, 希望知道多元正态分布充分统计量详细推导的朋友的朋友能给我解惑, 联系方式见关于我 :) )必然是其中一个充分统计量, 用上述结论即为<span class="math display">\[\begin{aligned} \mathbb{E}_{P(x\mid X, \theta)}x &amp; =  \mathbb{E}_{q(x\mid \mu, \Sigma)}x=\mu\\\
\mathbb{E}_{P(x\mid X, \theta)}xx^T&amp; =  \mathbb{E}_{q(x\mid \mu, \Sigma)}xx^T=\Sigma + \mu\mu^T\end{aligned}\]</span>上述思路其实是Assumed Density Filtering(ADF)(或者说在线贝叶斯学习online bayesian leaning, 矩匹配moment matching)的主要思路, 至于在线学习则体现在初始化<span class="math inline">\(q=1\)</span>后对每个样本序列地用上述式子去更新, 详细可见Thomas P Minka的博士论文A family of algorithms for approximate Bayesian inference(百度学术, 谷歌学术均可达)的3.1节.</li>
</ul>
<hr />
<p><code>p.s.</code>这篇论文其实是期望方法传播的提出之作, 其可视为ADF的拓展, 或者说ADF是期望传播的online版本, 两者相同点在于每次都只对一个样本进行更新; 区别则在于逼近目标为batch还是依次来样本; 且期望方法会一直更新直到收敛, 因此很可能每个样本点更新多次, 而ADF对每个样本点只更新一次. 有兴趣的朋友可见Thomas P Minka的那篇论文, 或者见PRML一书p510, 但本质上两者的核心更新式子还是出于期望的传播这样一个想法.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>KL散度</tag>
        <tag>期望传播</tag>
        <tag>指数族分布</tag>
        <tag>充分统计量</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习中的贝叶斯思想</title>
    <url>/2018/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%80%9D%E6%83%B3/</url>
    <content><![CDATA[<p>前言:本文主要是关于机器学习中的贝叶斯思想的一些简单认识, 主要参考了Coursera上国立高等经济大学Advanced Machine Learning系列课程Course3: Bayesian Methods for Machine Learning Week1. <span id="more"></span></p>
<hr />
<h2 id="引言">引言</h2>
<p>我们先来直观感受一下贝叶斯思想, 首先来看一道选择题, 在灭霸还没有诞生的时间节点, 纽约街头有一人在跑步, 你觉得?</p>
<ol type="1">
<li>他看到了灭霸.</li>
<li>他想跑步就跑了.</li>
<li>他一直喜欢跑步.</li>
</ol>
<p>答案是B. 事实上, 使用先验信息可以排除A, 而C做了额外的假设. A.B.C.分别对应贝叶斯三条原则:</p>
<ul>
<li>使用先验信息.</li>
<li>选择最能解释现象的答案.</li>
<li>避免做出额外的假设.</li>
</ul>
<p>希望你们喜欢这道选择题,下面进入正题, 本文的主角是机器学习, 所以符号的选取会尽可能的采用机器学习中的惯用符号.</p>
<hr />
<h2 id="贝叶斯学派与频率学派">贝叶斯学派与频率学派</h2>
<p>贝叶斯学派与频率学派是统计学的两大学派, 他们主要有如下两大不同:</p>
<ul>
<li>贝叶斯学派将参数<span class="math inline">\(\theta\)</span>视为变量, 数据<span class="math inline">\(X\)</span>固定; 频率学派将参数<span class="math inline">\(\theta\)</span>固定, 数据<span class="math inline">\(X\)</span>视为变量.</li>
<li>贝叶斯学派: 得到关于参数的分布<span class="math inline">\(P(\theta\mid X)\)</span>; 频率学派: 得到参数值<span class="math inline">\(\hat{\theta}=\arg\max_{\theta}P(X\mid \theta)\)</span></li>
</ul>
<p>贝叶斯思想最大的特点就是把参数<span class="math inline">\(\theta\)</span>视为变量, 所以得到的结果也是关于参数<span class="math inline">\(\theta\)</span>的分布, 更加一般化. 若要向得到参数值, 最简单的可以对后验分布求概率值最大的点(maximum a posterior):<span class="math display">\[\theta_{MAP}=\arg\max_{\theta}P(\theta\mid X)\]</span></p>
<hr />
<h2 id="贝叶斯公式">贝叶斯公式</h2>
<p>贝叶斯公式想必大家都熟悉<span class="math display">\[P(\theta\mid X)=\frac{P(X,\theta)}{P(X)}={\frac {P(X\mid \theta)\,P(\theta)}{P(X)}}\]</span>其中<span class="math inline">\(P(\theta)\)</span>称为先验分布, <span class="math inline">\(P(X\mid \theta)\)</span>称为似然函数, 或者理解为模型, <span class="math inline">\(X\)</span>为所得到的数据集, 或者说evidence, <span class="math inline">\(P(\theta\mid X)\)</span>称为后验分布.</p>
<p>简单举两个例子说明贝叶斯公式在机器学习中的应用:</p>
<ul>
<li>在线学习做的就是用数据<span class="math inline">\(X\)</span>对<span class="math inline">\(P(\theta)\)</span>进行更新得到<span class="math inline">\(P(\theta\mid X)\)</span>, 因为不知道新的样本到达时间, 所以基本上就是每来一个样本<span class="math inline">\(x_k\)</span>就对参数进行一次更新: <span class="math display">\[P_k(\theta)=P(\theta\mid x_k)=\frac {P(x_k\mid \theta)\,P_{k-1}(\theta)}{P(x_k)}\]</span>比方说假定模型满足正态分布<span class="math inline">\(P(x\mid \theta)= N(\theta, 1)= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-\theta)^2}{2}}\)</span>, 假定先验<span class="math inline">\(P_0(\theta)=N(0,1)\)</span>, 将数据<span class="math inline">\(x_1=1\)</span>喂给它, 容易得到<span class="math display">\[P_1(\theta)=\frac{1}{2\pi} e^{-\frac{(1-\theta)^2}{2}} e^{-\frac{\theta^2}{2}}/P(x_k)\propto N(\frac{1}{2},\frac{1}{2}) \]</span>注意到<span class="math inline">\(P(x_k)\)</span>与<span class="math inline">\(\theta\)</span>无关所以不在乎,另外容易看到<span class="math inline">\(\theta\)</span>的后验的期望向1迈出了一步.</li>
<li>将贝叶斯公式应用到监督学习中,记参数为<span class="math inline">\(\omega\)</span>: <span class="math display">\[P(\omega\mid X, y)=\frac {P(\omega, y\mid X)}{P(y\mid X)}=\frac {P(y\mid X, \omega)\,P(\omega)}{P(y\mid X)}\]</span>其中<span class="math inline">\(P(y\mid X, \omega)\)</span>为模型的基本结构,比方说<span class="math inline">\(P(y\mid X, \omega)=N(X\omega, \sigma^2 I)\)</span>, 协方差阵固定位单位阵的倍数, 这实际上假定了样本之间的独立性, 若还假定参数<span class="math inline">\(\omega\)</span>满足正态分布, 则在线性模型中可以得到<strong>最小二乘法加L2正则项</strong>的结果(下文会加以详细说明). 有了模型<span class="math inline">\(P(y\mid X, \omega)\)</span>和参数<span class="math inline">\(P(\omega\mid X, y)\)</span>, 我们可以通过下式在测试集上进行预测:<span class="math display">\[P(y_{test}\mid X_{test}, X ,y)=\int P(y_{test}\mid X_{test}, \omega)P(\omega\mid X, y) d\omega\]</span></li>
</ul>
<p>有了上述两个例子, 想必大家都有了自己的理解, 在机器学习中, 模型<span class="math inline">\(P(y\mid X, \omega)\)</span>的选取与先验<span class="math inline">\(P(\omega)\)</span>成为了两个最关键的要素, 而模型<span class="math inline">\(P(y\mid X, \omega)\)</span>的选取是我们首先要考虑的,下面来看:</p>
<hr />
<h2 id="如何定义一个模型">如何定义一个模型</h2>
一般的监督学习任务, 我们所有的是一堆互相相关的特征, 目标是试图从中找出一些因果关系, 一个自然地想法就是试图用一些箭头来表示这种因果关系, 图片来自<a href="https://en.wikipedia.org/wiki/Bayesian_network">贝叶斯网络</a>的维基百科:
<div data-align="center">
<img src="/pictures/bayes in ml/SimpleBayesNepng.png" width="80%" height="50%" />
</div>
<p>这里我们关心这样一个问题,看到草坪湿了, 想知道有没有下过雨. 由常识容易知道:下雨能导致草坪湿掉, 喷洒器也能导致草坪湿掉, 且下雨天喷洒器往往不会运作, 有了这张图, 我们也就能轻易的表示出<span class="math inline">\(P(Rain\mid Grass)\)</span>, 这样的有向非循环的图模型被称为是贝叶斯网络.</p>
再来看一个贝叶斯网络的简单例子:贝叶斯分类器, 图片来自周志华的机器学习:
<div data-align="center">
<img src="/pictures/bayes in ml/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.png" width="80%" height="50%" />
</div>
<p>(a)是朴素贝叶斯, 想法也非常朴素, 特征<span class="math inline">\(x_i\)</span>仅仅由<span class="math inline">\(y\)</span>所决定, (b)、(c)则是考虑了特征之间的依赖关系.</p>
这里提一下板表示法, 板表示法(plate notation)是一种表示在图形模型中重复的变量的方法, 比方说朴素贝叶斯分类器(a)可以简画为:
<div data-align="center">
<img src="/pictures/bayes in ml/plate_notation.png" width="50%" height="50%" />
</div>
<p>图中的d表示将方形区域重复d遍, 也就是(a)中图形的样子. 模型的选取先说这么多, 下面考虑先验<span class="math inline">\(P(\omega)\)</span>的选取, 事实上, 为了计算简单, 一般选取共轭先验分布.</p>
<hr />
<h2 id="共轭先验">共轭先验</h2>
<p>关于贝叶斯公式,还有一个重要概念:<strong>共轭先验</strong>, 是指先验分布和后验分布同属某个分布的先验分布,这里举三个重要的共轭先验的例子:</p>
<ul>
<li>假定模型满足正态分布<span class="math inline">\(P(X\mid \theta)= N(\theta, \sigma^2)\)</span>, 其中<span class="math inline">\(\sigma^2\)</span>为给定值. 给定先验<span class="math inline">\(P(\theta)= N(0, 1)\)</span>满足标准正态分布. 此时后验分布:<span class="math display">\[ P(\theta\mid X) \propto  e^{-\frac{(x-\theta)^2}{2\sigma^2}} e^{-\frac{\theta^2}{2}}\propto N(\frac{x}{1+\sigma^2}, \frac{\sigma^2}{1+\sigma^2})\]</span>这个例子实际上在贝叶斯公式一节中讲到了,不难发现,若喂给它的数据是<span class="math inline">\(x=0\)</span>, 那么<span class="math inline">\(\theta\)</span>的后验分布第一个参数不变,第二根参数减小了些,也即期望值不变,方差减小了些.</li>
<li>假定模型满足正态分布<span class="math inline">\(P(X\mid \gamma)=N(\mu, \gamma^{-1})\)</span>, 其中<span class="math inline">\(\mu\)</span>为给定值,<span class="math inline">\(\gamma=\frac{1}{\sigma^2}\)</span>称为准确度,方差越小,准确度越高.给定先验<span class="math inline">\(P(\gamma)= \Gamma(a,b)\propto\gamma^{a-1}e^{-b\gamma}\)</span>满足参数为<span class="math inline">\(a,b\)</span>的Gamma分布.此时后验分布:<span class="math display">\[ P(\gamma\mid X) \propto \gamma^{\frac{1}{2}+a-1} e^{-(b+\frac{(x-\mu)^2}{2})\gamma} \propto \Gamma(\frac{1}{2}+a, b+\frac{(x-\mu)^2}{2})\]</span>不难发现,若喂给它的数据是<span class="math inline">\(x=\mu\)</span>, 那么<span class="math inline">\(\gamma\)</span>的后验分布第一个参数增大了<span class="math inline">\(\frac{1}{2}\)</span>,第二个参数不变,注意到Gamma分布的期望<span class="math inline">\(E[\Gamma(a,b)]=\frac{a}{b}\)</span>, 也即准确度<span class="math inline">\(\gamma\)</span>的期望变大了些, 若一直喂给它相同的数据<span class="math inline">\(x=\mu\)</span>, 可以看到准确度<span class="math inline">\(\gamma\)</span>的期望会一直上涨, 这也与我们的想象相符合.</li>
<li>假定模型满足二项分布<span class="math inline">\(P(X\mid \theta)= \theta^x(1-\theta)^{(1-x)}\)</span>. 给定先验<span class="math inline">\(P(\theta)=Be(a,b) \propto\theta^{a-1}(1-\theta)^{b-1}\)</span>满足参数为<span class="math inline">\(a,b\)</span>的Beta分布.此时后验分布:<span class="math display">\[ P(\theta\mid X) \propto \theta^{x+a-1}(1-\theta)^{1-x+b-1} \propto Be(x+a, 1-x+b)\]</span>若喂给它的数据是<span class="math inline">\(x=1\)</span>, 那么<span class="math inline">\(\theta\)</span>的后验分布第一个参数增大了<span class="math inline">\(1\)</span>,第二个参数不变,注意到Beta分布的期望<span class="math inline">\(E[Be(a,b)]=\frac{a}{a+b}\)</span>, 也即<span class="math inline">\(\theta\)</span>的期望向<span class="math inline">\(1\)</span>迈出了一小步, 这也与我们的想象相符.</li>
</ul>
<hr />
<h2 id="贝叶斯视角下的线性回归与最小二乘线性回归">贝叶斯视角下的线性回归与最小二乘线性回归</h2>
<p>考虑线性回归模型<span class="math inline">\(y=X\omega\)</span>, 这里的<span class="math inline">\(y\)</span>为<span class="math inline">\(m\)</span>维列向量, <span class="math inline">\(\omega\)</span>为<span class="math inline">\(n\)</span>维列向量, 假定模型满足正态分布<span class="math inline">\(P(y\mid X, \omega)=N(X\omega, \sigma^2I)\)</span>, 先验也满足正态分布<span class="math inline">\(P(\omega)=N(0, \gamma^2I)\)</span>, 容易得到:<span class="math display">\[P(\omega\mid X, y)=\frac {P(\omega, y\mid X)}{P(y\mid X)}=\frac {P(y\mid X, \omega)\,P(\omega)}{P(y\mid X)}\propto e^{-\frac{1}{2\sigma^2}(y-X\omega)^T(y-X\omega)}e^{-\frac{1}{2\gamma^2}\omega^T\omega}\]</span>可以通过PRML一书2.3.3节最后的公式给出后验的正态分布的参数, 这里仅求该后验分布的概率最大值点: <span class="math inline">\(\omega_{MAP}=\arg\max_{\omega}P(\omega\mid X, y)\)</span>, 易知取<span class="math inline">\(\log\)</span>不影响求最大值:<span class="math display">\[\log P(\omega\mid X, y)=-\frac{1}{2\sigma^2}(y-X\omega)^T(y-X\omega)-\frac{1}{2\gamma^2}\omega^T\omega=-\frac{1}{2\sigma^2}(\left\Vert y-X\omega\right\Vert^2+\frac{\sigma^2}{\gamma^2}\left\Vert \omega\right\Vert^2)\]</span>对其求最大值即为<strong>最小二乘方法加L2正则项</strong>的结果. 如果有兴趣, 不妨将上面对<span class="math inline">\(\omega\)</span>的先验分布替换成参数为<span class="math inline">\(0,b\)</span>的拉普拉斯分布<span class="math inline">\(P(\omega)=\frac{1}{(2b)^n} \prod_ {i=1}^n\exp\left(-\frac{|\omega_i|}{b}\right)\)</span>, 可以得到<strong>最小二乘方法加L1正则项</strong>的结果.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>共轭先验</tag>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>波束形成算法</title>
    <url>/2021/07/16/%E6%B3%A2%E6%9D%9F%E5%BD%A2%E6%88%90%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>前言: 本文根据张小飞的阵列信号处理理论与应用第2版一书整理了波束形成算法</p>
<span id="more"></span>
<p>自适应波束形成(Adaptive beamforming), 又称空域滤波, 它能够根据阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>的变化自适应的改变权重向量<span class="math inline">\(\mathbf{w}\)</span>, 输出对阵列输出加权求和<span class="math inline">\(y[n]\)</span>, 以达到增强期望信号, 抑制干扰的目的。<strong>p.s. </strong>粗体<span class="math inline">\(\mathbf{w}\)</span>表示向量/矩阵, 常规体<span class="math inline">\(d[t]\)</span>表示标量。 <span class="math display">\[
y[t]=\mathbf{w}^H\mathbf{x}[t]
\]</span> 几个要点:</p>
<ol type="1">
<li><p>假定任意时刻信号在各阵元上的振幅相同, <span class="math inline">\(\mathbf{w}\)</span>选用移相器<span class="math inline">\(\mathbf{a}(\theta)=[1,...,e^{-j(M-1)w\tau}]\)</span>, <span class="math inline">\(w=2\pi f=\frac{2\pi c}{\lambda}\)</span>, <span class="math inline">\(\tau=\frac{d\sin\theta}{c}\)</span>.</p></li>
<li><p>阵列信号由波达方向为<span class="math inline">\(\theta_d\)</span>的期望信号<span class="math inline">\(d[t]\)</span>、波达方向为<span class="math inline">\(\theta_j\)</span>的<span class="math inline">\(J\)</span>个干扰信号<span class="math inline">\(i_j[t]\)</span>和白噪声<span class="math inline">\(\mathbf{n}[t]\)</span>构成 <span class="math display">\[
\mathbf{x}[t]=\mathbf{a}(\theta_d)d[t]+\sum_{j=1}^J\mathbf{a}(\theta_j)i_j[t]+\mathbf{n}[t]\label{model}
\]</span> 期望能从<span class="math inline">\(\mathbf{x}[t]\)</span>恢复出<span class="math inline">\(d[t]\)</span>.</p></li>
<li><p>如果两个同频信号的空间方位角间隔大于阵列间隔的倒数时, 它们方可被分辨开, 这是瑞利限.</p></li>
</ol>
<h2 id="lcmv波束形成">LCMV波束形成</h2>
<p>波束形成器输出的平均功率为: <span class="math display">\[
P(\mathbf{w})=\mathbb{E}\left[\left\vert y[t]\right\vert^2\right]=\mathbb{E}\left[\left\vert d[t]\right\vert^2\right]\left\vert\mathbf{w}^H\mathbf{a}(\theta_d)\right\vert^2+\sum_{j=1}^J\mathbb{E}\left[\left\vert i_j[t]\right\vert^2\right]\left\vert\mathbf{w}^H\mathbf{a}(\theta_j)\right\vert^2+\sigma_n^2\Vert\mathbf{w}\Vert^2
\]</span> LCMV波束形成的优化问题如下: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w}\\
\mbox{s.t.}\quad
&amp;\mathbf{w}^H\mathbf{a}(\theta_d)=f
\end{alignat}
\]</span> 其中<span class="math inline">\(\mathbf{R}\in\mathbb{C}^{M\times M}\)</span>为阵列阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>的协方差矩阵. Lagrange乘子方法求得LCMV波束形成的最优权向量(Linearly Constrained Minimum Variance, LCMV): <span class="math display">\[
\mathbf{w}_{_{LCMV}}=\frac{f^*\mathbf{R}^{-1}\mathbf{a}(\theta_d)}{\mathbf{a}^H(\theta_d)\mathbf{R}^{-1}\mathbf{a}(\theta_d)}
\]</span> 而<span class="math inline">\(d[t]=\mathbf{w}_o^H\mathbf{x}[t]\)</span>, 下面不再赘述. 特别地, 当<span class="math inline">\(f=1\)</span>时称MVDR(Minimum Variance Distortionless Response)波束形成.</p>
<p>要点:</p>
<ol type="1">
<li>需要知道精确的期望信号导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>.</li>
</ol>
<h2 id="gsc波束形成">GSC波束形成</h2>
<p>GSC波束形成本质上只是在LCMV的基础上增加了约束的个数, GSC的优化问题如下: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w}\\
\mbox{s.t.}\quad
&amp;\mathbf{C}^H\mathbf{w}=\mathbf{f}
\end{alignat}
\]</span> 其中<span class="math inline">\(\mathbf{C}\)</span>为<span class="math inline">\(M\times(J+1)\)</span>维约束矩阵. 可由Lagrange乘子方法求得GSC波束形成的最优权向量(Generalized Sidelobe Canceller, GSC): <span class="math display">\[
\mathbf{w}_{_{GSC}}=\mathbf{R}^{-1}\mathbf{C}(\mathbf{C}^H\mathbf{R}^{-1}\mathbf{C})^{-1}\mathbf{f}\label{GSC1}
\]</span> 也可由正交子空间方法求得最优权向量: <span class="math display">\[
\mathbf{w}_{_{GSC}}=\mathbf{w}_{q}-\mathbf{B}\mathbf{w}_a=\mathbf{w}_{q}-\mathbf{B}(\mathbf{B}^H\mathbf{R}\mathbf{B})^{-1}\mathbf{B}^H\mathbf{R}\mathbf{w}_q\label{GSC2}
\]</span> 其中<span class="math inline">\(\mathbf{w}_q=(\mathbf{C\mathbf{C}^H})^{-1}\mathbf{C}\mathbf{f}\)</span>, 阻塞矩阵<span class="math inline">\(\mathbf{B}\)</span>的列向量位<span class="math inline">\(\mathbf{C}\)</span>的正交补空间中.</p>
<p>要点:</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{C}\)</span>, <span class="math inline">\(\mathbf{f}\)</span>需要人为确定, 且<span class="math inline">\(\mathbf{C}\)</span>包含<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>.</li>
<li>由于阵元增益存在幅度误差和相位误差<span class="math inline">\(\mathbf{G}\)</span>, 实际的期望信号导向向量<span class="math inline">\(\mathbf{G}\mathbf{a}(\theta_d)\)</span>不完全在<span class="math inline">\(\mathbf{C}\)</span>张成的空间中, 此时<span class="math inline">\(\mathbf{B}^H\mathbf{G}\mathbf{a}(\theta_d)d[t]\neq 0\)</span>, 就会出现期望相消的现象.</li>
<li>正交子空间方法要求<span class="math inline">\(J+1&lt;M\)</span>.</li>
</ol>
<h3 id="igsc波束形成">IGSC波束形成</h3>
<p>首先将协方差矩阵通过EVD分解为信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>: <span class="math display">\[
\mathbf{R}=\mathbf{U}_S\Sigma_S\mathbf{U}_S^H+\mathbf{U}_N\Sigma_N\mathbf{U}_N^H
\]</span> 接着将<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​投影到信号-干扰子空间中: <span class="math display">\[
\mathbf{a}&#39;(\theta_d)=\mathbf{U}_S\mathbf{U}_S^H\mathbf{a}(\theta_d)
\]</span> 最后只需带入公式<span class="math inline">\(\ref{GSC1}\)</span>或<span class="math inline">\(\ref{GSC2}\)</span>计算求得解(为了方便理解, 这里假定约束个数只有一个, 因此只需将<span class="math inline">\(\mathbf{a}&#39;(\theta_d)\)</span>带换<span class="math inline">\(\mathbf{C}\)</span>​​​​​) <span class="math display">\[
\mathbf{w}_{_{IGSC}}=\mathbf{R}^{-1}\mathbf{a}&#39;(\theta_d)(\mathbf{a}&#39;(\theta_d)^H\mathbf{R}^{-1}\mathbf{a}&#39;(\theta_d))^{-1}\mathbf{f}
\]</span> 要点:</p>
<ol type="1">
<li>在实际的导向向量<span class="math inline">\(\mathbf{G}\mathbf{a}(\theta_d)\)</span>属于信号-干扰子空间<span class="math inline">\(S\)</span>中的假设条件下, 投影后的导向向量<span class="math inline">\(\mathbf{U}_S\mathbf{U}_S^H\mathbf{a}(\theta_d)\)</span>会比<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>更加接近实际的导向向量<span class="math inline">\(\mathbf{G}\mathbf{a}(\theta_d)\)</span>, 从而能够带来更精确的解.</li>
<li>当白噪声信号的能量较大, 即SNR值较低, 此时EVD分解没法很好的区分信号-干扰子空间与噪声子空间​, 信号子空间会包含噪声, 波束形成图会有畸变.</li>
<li>要求<span class="math inline">\(J+1&lt;M\)</span>​, 且当<span class="math inline">\(J+1=M\)</span>​时, 投影为恒等变换.</li>
<li><span class="math inline">\(J\)</span>此时实际上也为信号-干扰子空间<span class="math inline">\(S\)</span>的维数, 必须恰当选取.</li>
</ol>
<h3 id="es-gsc波束形成">ES-GSC波束形成</h3>
<p>首先将协方差矩阵通过EVD分解为信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>: <span class="math display">\[
\mathbf{R}=\mathbf{U}_S\Sigma_S\mathbf{U}_S^H+\mathbf{U}_N\Sigma_N\mathbf{U}_N^H
\]</span> 接着用<span class="math inline">\(\mathbf{U}_S\)</span>​与期望信号的导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​构成广义信号-干扰子空间<span class="math inline">\(\mathbf{U}\)</span>​, 对<span class="math inline">\(\mathbf{U}\)</span>​再做SVD得到左奇异空间<span class="math inline">\(\mathbf{U}_{L}\)</span>​, 最后按照公式<span class="math inline">\(\ref{GSC1}\)</span>​或<span class="math inline">\(\ref{GSC2}\)</span>​计算求得的最优权向量<span class="math inline">\(\mathbf{w}_{_{GSC}}\)</span>​投影至左奇异向量构成的空间<span class="math inline">\(\mathbf{U}_L\)</span>​中: <span class="math display">\[
\mathbf{w}_{_{ES-GSC}}=\mathbf{U}_L\mathbf{U}_L^H\mathbf{w}_{_{GSC}}
\]</span> 要点:</p>
<ol type="1">
<li>带上期望信号导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>的信号子空间<span class="math inline">\(\mathbf{U}\)</span>再对<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>做投影失去意义, 因此直接投影最优权向量.</li>
<li>引入<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>, 这降低了信号子空间的噪声, 因此具有较好的噪声鲁棒性.</li>
<li>正交子空间方法要求<span class="math inline">\(J+1&lt;M\)</span>.</li>
</ol>
<h2 id="ebs波束形成">EBS波束形成</h2>
<p>首先将协方差矩阵通过EVD分解为信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>: <span class="math display">\[
\mathbf{R}=\mathbf{U}_S\Sigma_S\mathbf{U}_S^H+\mathbf{U}_N\Sigma_N\mathbf{U}_N^H
\]</span> 将SMI波束形成算法得到的最优权向量 <span class="math display">\[
\mathbf{w}_o=\mu\mathbf{R}^{-1}\mathbf{a}(\theta_d)
\]</span> (<strong>p.s.</strong>当<span class="math inline">\(\mu=\frac{f^*}{\mathbf{a}^H(\theta_d)\mathbf{R}^{-1}\mathbf{a}(\theta_d)}\)</span>即LCMV)投影至信号子空间<span class="math inline">\(\mathbf{U}_S\)</span>中: <span class="math display">\[
\mathbf{w}_{_{EBS}}=\mathbf{U}_S\mathbf{U}_S^H\mathbf{w}_{o}
\]</span> 要点:</p>
<ol type="1">
<li>可类比IGSC波束形成, 区别在于对<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>和<span class="math inline">\(\mathbf{w}_o\)</span>做投影.</li>
<li>信号子空间<span class="math inline">\(\mathbf{U}_S\)</span>必须包含期望信号导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>, 否则投影不是最优的.</li>
<li>当白噪声信号的能量较大, 信号子空间会包含噪声, 波束形成图会有畸变.</li>
<li>要求<span class="math inline">\(J+1&lt;M\)</span>, 且当<span class="math inline">\(J+1=M\)</span>时, 投影为恒等变换.</li>
</ol>
<h3 id="iebs波束形成">IEBS波束形成</h3>
<p>对协方差矩阵<span class="math inline">\(\mathbf{R}\in\mathbb{C}^{M\times M}\)</span>​​进行特征值分解, 用最大的<span class="math inline">\(J+1\)</span>​​或<span class="math inline">\(J\)</span>​​个特征值对应的特征向量与期望信号的导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​​构成信号子空间<span class="math inline">\(\mathbf{U}\)</span>​​, 对<span class="math inline">\(\mathbf{U}\)</span>​​再做奇异值分解, 将SMI波束形成算法得到的最优权向量<span class="math inline">\(\mathbf{w}_o\)</span>​​投影至左奇异向量构成的空间<span class="math inline">\(\mathbf{U}_S\)</span>​​中: <span class="math display">\[
\mathbf{w}_{_{IEBS}}=\mathbf{U}_S\mathbf{U}_S^H\mathbf{w}_{o}
\]</span> 要点:</p>
<ol type="1">
<li>可类比ES-GSC波束形成, 区别在于对<span class="math inline">\(\mathbf{w}_{_{GSC}}\)</span>和<span class="math inline">\(\mathbf{w}_o\)</span>做投影.</li>
<li>引入<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>, 这降低了信号子空间的噪声, 因此具有较好的噪声鲁棒性.</li>
<li>要求<span class="math inline">\(J+1&lt;M\)</span>.</li>
</ol>
<h2 id="基于斜投影的波束形成">基于斜投影的波束形成</h2>
<blockquote>
<p>关于投影:</p>
<ol type="1">
<li><span class="math inline">\(P\)</span> is a projection along null space <span class="math inline">\(V\)</span> onto range space <span class="math inline">\(U\)</span>.</li>
<li>In Hilbert space, we have the concept of inner <span class="math inline">\(\langle u_1, v_1 \rangle=u_1^Hv_1\)</span>, then the concept of orthogonality<span class="math inline">\(\langle u_1, v_1 \rangle=0\)</span>.</li>
<li>If <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal, call <span class="math inline">\(P\)</span> orthogonal projection; If <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are non-orthogonal, call <span class="math inline">\(P\)</span> oblique projection(斜投影);</li>
<li>In orthogonal projection, when <span class="math inline">\(U\)</span> are orthonormal basis(正交基), <span class="math inline">\(P=UU^H\)</span>; when <span class="math inline">\(U\)</span> are not orthonormal basis <span class="math inline">\(P=U(U^HU)^{-1}U^H\)</span>; for general inner product <span class="math inline">\(\langle u_1, v_1 \rangle=u_1^HDv_1\)</span>, <span class="math inline">\(P=U(U^HDU)^{-1}U^HD\)</span>.</li>
<li>In oblique projection, <span class="math inline">\(P=U(B^HU)^{-1}B^H\)</span>, where <span class="math inline">\(B\)</span> is the orthogonal complement of null space <span class="math inline">\(V\)</span>.</li>
</ol>
</blockquote>
<p>如下定义斜投影算子<span class="math inline">\(E_{AB}\)</span>为沿着<span class="math inline">\(B\)</span>到<span class="math inline">\(A\)</span>上的投影: <span class="math display">\[
E_{AB}=A\left(A^HP_B^\perp A\right)^{-1}A^HP_B^\perp
\]</span> 其中<span class="math inline">\(P_B^\perp=I-B(B^HB)^{-1}B^H\)</span>. 定义<span class="math inline">\(B=\left[\mathbf{a}(\theta_1),...,\mathbf{a}(\theta_J)\right]\)</span>, <span class="math inline">\(B\)</span>与<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>显然不正交. 将阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>沿着<span class="math inline">\(B\)</span>斜投影到<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>上: <span class="math display">\[
\mathbf{x}&#39;[t]=E_{\mathbf{a}(\theta_d)B}\mathbf{x}[t]=\mathbf{a}(\theta_d)\left(\mathbf{a}(\theta_d)^HP_B^\perp \mathbf{a}(\theta_d)\right)^{-1}\mathbf{a}(\theta_d)^HP_B^\perp\mathbf{x}[t]
\]</span> 接着进行空域滤波匹配, 得到期望信号: <span class="math display">\[
d[t]=\mathbf{a}(\theta_d)^H\mathbf{x}&#39;[t]
\]</span> 要点:</p>
<ol type="1">
<li>该算法不涉及权重向量<span class="math inline">\(\mathbf{w}\)</span>.</li>
<li><span class="math inline">\(P_B^\perp\)</span>难以计算, 一般对协方差矩阵<span class="math inline">\(\mathbf{R}\in\mathbb{C}^{M\times M}\)</span>进行特征值分解<span class="math inline">\(\mathbf{R}=U\Sigma U^H\)</span>, 用除前<span class="math inline">\(J+1\)</span>个特征值之外的特征值的均值作为白噪声方差<span class="math inline">\(\sigma^2\)</span>的估计值, 定义<span class="math inline">\(\mathbf{R}_A=\mathbf{R}-\sigma^2I\)</span>,用<span class="math inline">\(\mathbf{R}_A\)</span>的伪逆<span class="math inline">\(\mathbf{R}_A^+\)</span>取代<span class="math inline">\(P_B^\perp\)</span>.</li>
<li>要求<span class="math inline">\(J+1&lt;M\)</span>.</li>
</ol>
<h2 id="amv波束形成">AMV波束形成</h2>
<p>AMV波束形成的优化问题如下: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{\tilde{R}}\mathbf{w}\\
\mbox{s.t.}\quad
&amp;\mathbf{w}^H\mathbf{a}(\theta_d)=1
\end{alignat}
\]</span> 其中<span class="math inline">\(\mathbf{\tilde{R}}=\frac{1}{2\pi}\int_{0}^{2\pi}\mathbf{a}(\theta_d)\mathbf{a}(\theta_d)^H \mathrm{d}\theta\)</span>为阵列固有的协方差矩阵. Lagrange乘子方法求得AMV波束形成的最优权向量: <span class="math display">\[
\mathbf{w}_o=\frac{\mathbf{\tilde{R}}^{-1}\mathbf{a}(\theta_d)}{\mathbf{a}^H(\theta_d)\mathbf{\tilde{R}}^{-1}\mathbf{a}(\theta_d)}
\]</span> 要点</p>
<ol type="1">
<li>假设入射信号角度为<span class="math inline">\([0,2\pi]\)</span>上的独立均匀抽样, 阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>的协方差矩阵<span class="math inline">\(\mathbf{R}\)</span>会随着入射信号数的增大而逐渐趋向<span class="math inline">\(\mathbf{\tilde{R}}\)</span>, 因此AMV为入射信号无穷情形下的LCMV.</li>
<li>需要知道精确的期望信号导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>.</li>
</ol>
<h2 id="基于四阶累积量的波束形成">基于四阶累积量的波束形成</h2>
<blockquote>
<p>关于累积量:</p>
<ol type="1">
<li><p><span class="math inline">\(M\)</span>维随机变量<span class="math inline">\(\mathbf{x}[t]\)</span>的<span class="math inline">\(r=k_1+\cdots+k_M\)</span>阶累积量定义为 <span class="math display">\[
C_{k_1,...,k_M}=\frac{\partial^r\Psi(\omega_1,\dots,\omega_M)}{(\partial\omega_1)^{k_1}\cdots(\partial\omega_M)^{k_M}}\mid_{\omega_i=0}
\]</span> 其中<span class="math inline">\(\Psi(\omega_1,\dots,\omega_M)=\ln\left(\mathbb{E}\left[\omega_1x_1[t]+\cdots+\omega_M x_M[t]\right]\right)\)</span></p></li>
<li><p>零均值<span class="math inline">\(M\)</span>维随机变量<span class="math inline">\(\mathbf{x}[t]\)</span>的四阶累积量与矩有如下关系 <span class="math display">\[
cum\{X,Y,Z,W\}=\mathbb{E}(XYZW)-\mathbb{E}(XY)\mathbb{E}(ZW)-\mathbb{E}(XZ)\mathbb{E}(YW)-\mathbb{E}(XW)\mathbb{E}(YZ)
\]</span> 比如 <span class="math display">\[
cum\{x_1[t], x_1^*[t],x_1^*[t], x_m[t]\}=\mathbb{E}(x_1[t]x_1^*[t]x_1^*[t]x_m[t])-\mathbb{E}(x_1[t]x_1^*[t])\mathbb{E}(x_1^*[t]x_m[t])-\mathbb{E}(x_1[t]x_1^*[t])\mathbb{E}(x_1^*[t]x_m[t])-\mathbb{E}(x_1[t]x_m[t])\mathbb{E}(x_1^*[t]x_1^*[t])
\]</span></p></li>
<li><p>若<span class="math inline">\(d[t]\)</span>与<span class="math inline">\(i_j[t]\)</span>独立, 有 <span class="math display">\[
cum\{d[t]+i_j[t], d[t]+i_j[t]\}=cum\{d[t], d[t]\}+cum\{i_j[t], i_j[t]\}\label{cum1}
\]</span></p></li>
<li><p>若<span class="math inline">\(i_j[t]\)</span>服从高斯分布, 有 <span class="math display">\[
cum\{i_j[t], i_j[t], i_j[t],...\}=0\label{cum2}
\]</span> 即高斯信号三阶级以上的累积量为0.</p></li>
</ol>
</blockquote>
<p>定义阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>的四阶累积量为 <span class="math display">\[
C_{4m}=cum\{x_1[t], x_1^*[t],x_1^*[t], x_m[t]\}, m=1,\dots,M
\]</span> 令<span class="math inline">\(\mathbf{C}_4=[C_{41},\dots,C_{4M}]\)</span>, 可以证明, <span class="math inline">\(\mathbf{C}_4\)</span>只与<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>相差一个常数<span class="math inline">\(\beta\)</span>, 因此可以作为<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>的估计值. 最后可用LCMV波束形成的到最优权向量 <span class="math display">\[
\mathbf{w}_{_{cum1}}=\beta\mathbf{R}^{-1}\mathbf{C}_4
\]</span> 或是进行盲波束形成 <span class="math display">\[
\mathbf{w}_{_{cum2}}=\rho\{\mathbf{R}^{-1}\sigma^2\mathbf{C}_4\mathbf{C}_4^H\}
\]</span> 其中<span class="math inline">\(\rho\{\}\)</span>表示求最大特征值对应的特征向量, <span class="math inline">\(\sigma^2\)</span>为期望信号的功率.</p>
<p>要点:</p>
<ol type="1">
<li><p>要求期望信号<span class="math inline">\(d[t]\)</span>、<span class="math inline">\(J\)</span>个干扰信号<span class="math inline">\(i_j[t]\)</span>和白噪声<span class="math inline">\(\mathbf{n}[t]\)</span>均两两独立, 且假定期望信号<span class="math inline">\(d[t]\)</span>为非高斯信号, 而<span class="math inline">\(J\)</span>个干扰信号<span class="math inline">\(i_j[t]\)</span>和白噪声<span class="math inline">\(\mathbf{n}[t]\)</span>均为高斯信号.</p></li>
<li><p>结合<span class="math inline">\(\ref{model}\)</span>, <span class="math inline">\(\mathbf{C}_4=\beta\mathbf{a}(\theta_d)\)</span>的证明只需要用到性质<span class="math inline">\(\ref{cum1}\)</span>, <span class="math inline">\(\ref{cum2}\)</span> <span class="math display">\[
\begin{alignat}{2}
C_{4m}&amp; =cum\{a_1(\theta_d)d[t], a_1^*(\theta_d)d^*[t], a_1^*(\theta_d)d^*[t], a_m(\theta_d)d[t]\}\\
&amp;=\vert a_1(\theta_d)\vert^2a_1^*(\theta_d)cum\{d[t], d^*[t], d^*[t], d[t]\}a_m(\theta_d)\\
&amp;=\beta a_m(\theta_d)
\end{alignat}
\]</span> 因此<span class="math inline">\(\beta=\vert a_1(\theta_d)\vert^2a_1^*(\theta_d)cum\{d[t], d^*[t], d^*[t], d[t]\}\)</span>. 当干扰信号<span class="math inline">\(i_1[t]\)</span>为非高斯信号, 此时<span class="math inline">\(\mathbf{C}_4=\beta_1\mathbf{a}(\theta_d)+\beta_2\mathbf{a}(\theta_1)\)</span>, 且<span class="math inline">\(\beta_2=\vert a_1(\theta_1)\vert^2a_1^*(\theta_1)cum\{i_1[t], i_1^*[t], i_1^*[t], i_1[t]\}\)</span>.</p></li>
</ol>
<h2 id="cab波束形成">CAB波束形成</h2>
<blockquote>
<p>关于周期平稳信号:</p>
<ol type="1">
<li><p>对于阵列接收信号矢量<span class="math inline">\(\mathbf{x}[k]\)</span>​​, 其周期平稳相关矩阵定义为 <span class="math display">\[
\phi_{\mathbf{x}\mathbf{x}}(n_0,\alpha)=\overline{[\mathbf{x}[k]\mathbf{x}^H[k]e^{-j2\pi\alpha k}]_{\infty}}=\lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^N\mathbf{x}[k]\mathbf{x}^H[k]e^{-j2\pi\alpha k}
\]</span> 类似的, 周期共轭平稳相关矩阵定义为 <span class="math display">\[
\phi_{\mathbf{x}\mathbf{x}}(n_0,\alpha)=\overline{[\mathbf{x}[k]\mathbf{x}^T[k]e^{-j2\pi\alpha k}]_{\infty}}=\lim_{N\to\infty}\frac{1}{N}\sum_{k=1}^N\mathbf{x}[k]\mathbf{x}^T[k]e^{-j2\pi\alpha k}
\]</span> 如果信号在时延<span class="math inline">\(n_0\)</span>​和频率偏<span class="math inline">\(\alpha\)</span>时非零, 则此信号称为周期平稳信号.</p></li>
<li><p>实际计算过程中均采用有限采样长度<span class="math inline">\(N\)</span>的时间平均, 并可将周期平稳相关矩阵和周期共轭平稳相关矩阵统一记号, 记作 <span class="math display">\[
\hat{R}_{\mathbf{x}\mathbf{u}}=\left\{
\begin{aligned}
\hat{\phi}_{\mathbf{x}\mathbf{x}}(n_0,\alpha) &amp; =  \overline{[\mathbf{x}[k]\mathbf{x}^H[k]e^{-j2\pi\alpha k}]_{N}} \\
\hat{\phi}_{\mathbf{x}\mathbf{x}^*}(n_0,\alpha) &amp; =  \overline{[\mathbf{x}[k]\mathbf{x}^T[k]e^{-j2\pi\alpha k}]_{N}}
\end{aligned}
\right.
\]</span> 这里<span class="math inline">\(\mathbf{u}[k]=\mathbf{x}[k+n_0]e^{j2\pi\alpha k}\)</span>​​表示时频位移矢量.</p></li>
</ol>
</blockquote>
<p>CAB波束形成的优化问题如下: <span class="math display">\[
\begin{alignat}{2}
\max_{\mathbf{w}, \mathbf{c}} \quad &amp;  \mathbf{w}^H\hat{R}_{\mathbf{x}\mathbf{u}}cc^H\hat{R}_{\mathbf{x}\mathbf{u}}^{H}\mathbf{w}\\
\mbox{s.t.}\quad
&amp;\mathbf{w}^H\mathbf{w}=\mathbf{c}^H\mathbf{c}=1
\end{alignat}
\]</span> 假定这里的<span class="math inline">\(\mathbf{w}, \mathbf{c}\)</span>​能使得标量信号<span class="math inline">\(\mathbf{w}^H\mathbf{x}[t]\)</span>​和<span class="math inline">\(\mathbf{c}^H\mathbf{u}[t]\)</span>​有极高的相关值, 即最大化时间平均量<span class="math inline">\(\left\vert[\mathbf{w}^H\mathbf{x}[t]\mathbf{c}\mathbf{u}^H[t]]_N\right\vert^2\)</span>​,这也就顺便获得了期望信号. 由拉格朗日乘子法得<span class="math inline">\(\mathbf{w}, \mathbf{c}\)</span>​分别为<span class="math inline">\(\hat{R}_{\mathbf{x}\mathbf{u}}\)</span>​的最大奇异值对应的左右奇异向量, 记<span class="math inline">\(\mathbf{w}_{_{CAB}}\)</span>​.</p>
<p>要点:</p>
<ol type="1">
<li>假定了期望信号为周期平稳信号.</li>
<li>假定了期望信号与干扰信号不相关.</li>
</ol>
<h3 id="c-cab波束形成">C-CAB波束形成</h3>
<p>C-CAB波束形成在CAB的基础上应用MVDR, 其优化问题如下: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w}\\
\mbox{s.t.}\quad
&amp;\mathbf{w}^H\mathbf{w}_{_{CAB}}=1
\end{alignat}
\]</span> 这里<span class="math inline">\(\mathbf{R}\in\mathbb{C}^{M\times M}\)</span>​为阵列阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>​的协方差矩阵.由拉格朗日乘子法得 <span class="math display">\[
\mathbf{w}_{_{C-CAB}}=\mathbf{R}^{-1}\mathbf{w}_{_{CAB}}
\]</span> 要点:</p>
<ol type="1">
<li>继承了MVDR的问题.</li>
</ol>
<h3 id="r-cab波束形成">R-CAB波束形成</h3>
<p>R-CAB在C-CAB的基础上采用传统的对角线加载技术 <span class="math display">\[
\mathbf{w}_{_{R-CAB}}=(\mathbf{R}-\mathbf{R}_s+\gamma I)^{-1}\mathbf{w}_{_{CAB}}
\]</span> 这里的<span class="math inline">\(\mathbf{R}_s\)</span>​为期望信号部分的自相关矩阵, 单个期望信号的情况下有<span class="math inline">\(\mathbf{R}_s=\sigma_s^2 \mathbf{w}_{_{CAB}}\mathbf{w}_{_{CAB}}^H\)</span>​​​​, <span class="math inline">\(\sigma_s^2\)</span>​​为期望信号的方差.</p>
<p>要点:</p>
<ol type="1">
<li>对角线加载技术通过增大小特征值信号(噪声信号)的特征值的方式来减小噪声信号特征值的扰动, 从而减弱了小特征值信号在快拍数有限时的影响.</li>
</ol>
<h2 id="基于恒模的盲波束形成">基于恒模的盲波束形成</h2>
<p>基于恒模的盲波束形成的优化问题如下 <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \mathbb{E}\left[\left\vert\left\vert \mathbf{w}^H\mathbf{x}[t]\right\vert^p -1\right\vert^q\right]
\end{alignat}
\]</span> 可直接用随机梯度下降方法(最新一个样本)或高斯牛顿方法(最新K个样本)解之, 对应随机梯度恒模算法和LS-CMA.</p>
<p>要点:</p>
<ol type="1">
<li>基于恒模的方法仅仅对期望信号的模做了约束.</li>
</ol>
<h2 id="smi波束形成">SMI波束形成</h2>
<p>SMI波束形成是LCMV波束形成的一种一般化 <span class="math display">\[
\mathbf{w}_o=\mu\mathbf{R}^{-1}\mathbf{a}(\theta_d)
\]</span> 这里<span class="math inline">\(\mathbf{R}\in\mathbb{C}^{M\times M}\)</span>​为阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>​的协方差矩阵. 当<span class="math inline">\(\mu=\frac{f^*}{\mathbf{a}^H(\theta_d)\mathbf{R}^{-1}\mathbf{a}(\theta_d)}\)</span>​即LCMV.</p>
<p>要点:</p>
<ol type="1">
<li><p>假定阵列接收信号由<span class="math inline">\(P\)</span>​个信号构成, 体现在协方差矩阵<span class="math inline">\(\mathbf{R}\)</span>​​由特征值较大的信号和特征值较小的噪声两部分组成 <span class="math display">\[
\mathbf{R}=\sum_{i=1}^P\lambda_i\mathbf{u}_i\mathbf{u}_i^H+\sigma_n^2\sum_{i=P+1}^M\mathbf{u}_i\mathbf{u}_i^H=\sum_{i=1}^P(\lambda_i-\sigma_n^2)\mathbf{u}_i\mathbf{u}_i^H+\sigma_n^2\mathbf{I}
\]</span> 其中<span class="math inline">\(\lambda_1\ge \lambda_2\ge\dots\ge\lambda_P&gt;\lambda_{P+1}=\dots=\lambda_M=\sigma_n^2\)</span>​, 继而由矩阵求逆引理得 <span class="math display">\[
\mathbf{w}_o=\mu\mathbf{R}^{-1}\mathbf{a}(\theta_d)=\frac{\mu}{\sigma_n^2}\left[\mathbf{a}(\theta_d)-\sum_{i=1}^P\frac{\lambda_i-\sigma_n^2}{\lambda_i}\mathbf{u}_i\mathbf{u}_i^H\mathbf{a}(\theta_d)\right]
\]</span></p></li>
<li><p>实际计算由<span class="math inline">\(K\)</span>​次采样信号来得到<span class="math inline">\(\mathbf{R}\)</span>​的估计值<span class="math inline">\(\hat{\mathbf{R}}=\frac{1}{K}\sum_{i=1}^K\mathbf{x(t_i)}\mathbf{x(t_i)}^H\)</span>​, 此时<span class="math inline">\(\hat{\mathbf{R}}\)</span>​的特征值很有可能不满足后<span class="math inline">\(M\)</span>​​个都一样, 因此就没法完全消除噪声信号带来的影响 <span class="math display">\[
\hat{\mathbf{w}}_o=\mu\hat{\mathbf{R}}^{-1}\mathbf{a}(\theta_d)=\frac{\mu}{\hat{\lambda}_M}\left[\mathbf{a}(\theta_d)-\sum_{i=1}^M\frac{\hat{\lambda}_i-\hat{\lambda}_M}{\hat{\lambda}_i}\mathbf{u}_i\mathbf{u}_i^H\mathbf{a}(\theta_d)\right]
\]</span> 我们需要一些技术来降低噪声信号带来的影响.</p></li>
</ol>
<h3 id="adl-smi波束形成">ADL-SMI波束形成</h3>
<p>ADL-SMI是在SMI的基础上加上了自适应的加载量 <span class="math display">\[
\mathbf{w}_o=\mu(\mathbf{R}+L\mathbf{I})^{-1}\mathbf{a}(\theta_d)
\]</span> 此时 <span class="math display">\[
\hat{\mathbf{w}}_o=\mu\hat{\mathbf{R}}^{-1}\mathbf{a}(\theta_d)=\frac{\mu}{\hat{\lambda}_M}\left[\mathbf{a}(\theta_d)-\sum_{i=1}^M\frac{\hat{\lambda}_i-\hat{\lambda}_M}{\hat{\lambda}_i+L}\mathbf{u}_i\mathbf{u}_i^H\mathbf{a}(\theta_d)\right]
\]</span> ADL-SMI波束形成的加载量<span class="math inline">\(L\)</span>可由如下方式确定</p>
<ol type="1">
<li>计算<span class="math inline">\(\xi=\frac{1}{M-P}\sum_{i=P+1}^M\lambda_i\)</span>​​​​​</li>
<li>给定常数字<span class="math inline">\(d\)</span>​​, <span class="math inline">\(\xi&gt;d\)</span>​​视为信噪比较低的情况, 取<span class="math inline">\(L=0\)</span>​​; <span class="math inline">\(\xi\le d\)</span>​​视为信噪比较高的情况, 取<span class="math inline">\(L\)</span>​逐渐变大.</li>
</ol>
<p>要点:</p>
<ol type="1">
<li>加载量<span class="math inline">\(L\)</span>过大, 同时也会抑制信号,导致SINR降低; 加载量<span class="math inline">\(L\)</span>过小, 对噪声的抑制就不明显.</li>
<li>加载量<span class="math inline">\(L\)</span>的确定可通过画出方向图<span class="math inline">\(G(\mathbf{w},\theta)=\mathbf{w}^H\mathbf{a}(\theta)\)</span>来判断, 信号一般具有明显的方向性, 而噪声一般不具备. 适中的<span class="math inline">\(L\)</span>能够较好的保留信号同时抑制噪声.</li>
</ol>
<h2 id="基于空间频域的波束形成">基于空间频域的波束形成</h2>
<h3 id="flms-abf">FLMS-ABF</h3>
<ol type="1">
<li>首先对阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>​进行FFT</li>
</ol>
<p><span class="math display">\[
\mathbf{r}[t]=\mathbf{W}\mathbf{x}[t]
\]</span></p>
<p>这里<span class="math inline">\(\mathbf{W}\)</span>​​​为<span class="math inline">\(M\times M\)</span>​​​​的对称矩阵、酉矩阵 <span class="math display">\[
\mathbf{W}[u,v]=\frac{1}{\sqrt{M}}\exp(\frac{-j2\pi u v}{M})\quad u,v\in[0,M-1]
\]</span></p>
<ol start="2" type="1">
<li>LMS求解 <span class="math display">\[
\min_{V} \mathbb{E}\left[(d[t]-V^T[t]\mathbf{r}[t])^2\right]
\]</span> 其中<span class="math inline">\(d[t]\)</span>​为期望信号, 算法输出值为 <span class="math display">\[
y[t]=V^T[t]\mathbf{r}[t]
\]</span></li>
</ol>
<p>要点:</p>
<ol type="1">
<li><p>LMS本质上为随机梯度方法, 在目标函数强凸(这里的二次函数显然满足)且步长满足<span class="math inline">\(O(1/k)\)</span>​​的衰减速率的前提下, 随机梯度方法能以<span class="math inline">\(O(\kappa/k)\)</span>​​的速率收敛到其最小值点. 这里的<span class="math inline">\(\kappa\)</span>​​指带问题的条件数, 特别地, 这里二次问题的条件数<span class="math inline">\(\kappa\)</span>​​恰为矩阵<span class="math inline">\(\mathbf{r}[t]\)</span>​​的最大/最小奇异值的模之比<span class="math inline">\(\kappa (\mathbf{r}[t])={\frac {\sigma _{\text{max}}(\mathbf{r}[t])}{\sigma _{\text{min}}(\mathbf{r}[t])}}\)</span>​​, 也即<span class="math inline">\(\mathbf{r}[t]\)</span>​​​的自相关矩阵的最大最小特征值之比再开根号.</p></li>
<li><p>对于一般的矩阵范数, 矩阵的条件数定义为<span class="math inline">\(\kappa(A)=\Vert A\Vert\Vert A^{-1}\Vert\)</span>​​​, 由 <span class="math display">\[
\kappa(AB)=\|AB\|\,\|(AB)^{-1} \| \le \|A\|\,\|B\|\,\|B^{-1}\|\,\|A^{-1} \| =\kappa(A)\kappa(B)
\]</span> 以及<a href="https://en.wikipedia.org/wiki/Condition_number">酉矩阵的条件数为1</a>知, 对阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>进行FFT至少不会增加条件数.</p></li>
<li><p>也可在FFT之后再做一步带通滤波, 即 <span class="math display">\[
\mathbf{r}[t]=\mathbf{W}_B\mathbf{W}\mathbf{x}[t]
\]</span></p></li>
</ol>
<h3 id="小波域的波束形成">小波域的波束形成</h3>
<ol type="1">
<li>首先对阵列信号<span class="math inline">\(\mathbf{x}[t]\)</span>​​进行小波变换</li>
</ol>
<p><span class="math display">\[
\mathbf{r}[t]=\mathbf{W}\mathbf{x}[t]
\]</span></p>
<p>这里<span class="math inline">\(\mathbf{W}\)</span>.</p>
<ol start="2" type="1">
<li>LMS求解 <span class="math display">\[
\min_{V} \mathbb{E}\left[(d[t]-V^T[t]\mathbf{r}[t])^2\right]
\]</span> 其中<span class="math inline">\(d[t]\)</span>​为期望信号, 算法输出值为 <span class="math display">\[
y[t]=V^T[t]\mathbf{r}[t]
\]</span></li>
</ol>
<p>要点:</p>
<ol type="1">
<li>小波变换后的信号自相关性下降, 这进一步改善了问题的条件数.</li>
</ol>
<h2 id="鲁棒的波束形成">鲁棒的波束形成</h2>
<p>LCMV方法的最大特点为需要知道<strong>精确</strong>的期望信号导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​, 而实际情况中的期望信号的导向向量往往不是精确的, 为了提高LCMV波束形成对导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​​的鲁棒性, 我们有以下几种改进方法.</p>
<h3 id="基于二次正则的方法">基于二次正则的方法</h3>
<p>基于二次正则的方法为在MVDR(即LCMV中<span class="math inline">\(f=1\)</span>​​)优化目标函数的基础上加了一个二次正则: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w}+\xi\mathbf{w}^H\mathbf{w} \\
\mbox{s.t.}\quad
&amp;\mathbf{w}^H\mathbf{a}(\theta_d)=1
\end{alignat}
\]</span> Lagrange乘子方法求得解为(Diagonal Loading-Sample Matrix Inversion, DL-SMI): <span class="math display">\[
\mathbf{w}_{_{DL-SMI}}=\frac{(\mathbf{R}+\xi \mathbf{I})^{-1}\mathbf{a}(\theta_d)}{\mathbf{a}^H(\theta_d)(\mathbf{R}+\xi \mathbf{I})^{-1}\mathbf{a}(\theta_d)}
\]</span> 要点:</p>
<ol type="1">
<li>加二次正则等价于对协方差矩阵的特征值都加上了一个常数, 这显著改善问题的条件数(自相关矩阵的最大最小特征值之比再开根号), 增强了解的鲁棒性.</li>
<li>对角加载系数<span class="math inline">\(\xi\)</span>​​​需要人为确定.</li>
</ol>
<h3 id="基于子空间的方法">基于子空间的方法</h3>
<p>首先将协方差矩阵通过EVD分解为信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>: <span class="math display">\[
\mathbf{R}=\mathbf{U}_S\Sigma_S\mathbf{U}_S^H+\mathbf{U}_N\Sigma_N\mathbf{U}_N^H
\]</span> 再将导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>投影至信号-干扰子空间<span class="math inline">\(S\)</span>. 此时最优权向量为(Sub Space-Sample Matrix Inversion, SS-SMI), SS): <span class="math display">\[
\mathbf{w}_{_{SS-SMI}}=\mathbf{R}^{-1}\mathbf{U}_S\mathbf{U}_S^H\mathbf{a}(\theta_d)
\]</span> 要点:</p>
<ol type="1">
<li>仅适用SNR值较高的情形. 当SNR值较高, 信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>可通过特征值分解鲁棒地区分开来; 当SNR值较低, 信号-干扰子空间<span class="math inline">\(S\)</span>与噪声子空间<span class="math inline">\(N\)</span>​就没法通过特征值分解鲁棒地区分开来.</li>
<li>在实际的导向向量<span class="math inline">\(\mathbf{G}\mathbf{a}(\theta_d)\)</span>属于信号-干扰子空间<span class="math inline">\(S\)</span>中的假设条件下, 投影后的导向向量<span class="math inline">\(\mathbf{U}_S\mathbf{U}_S^H\mathbf{a}(\theta_d)\)</span>会比<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>更加接近实际的导向向量<span class="math inline">\(\mathbf{G}\mathbf{a}(\theta_d)\)</span>​, 从而能够带来更精确的解.</li>
<li>从数学上子空间投影的角度看, 投影操作本身就能够减小导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​​的误差, 从而增强了鲁棒性.</li>
<li>信号子空间<span class="math inline">\(S\)</span>​的维数需要人为确定.</li>
</ol>
<h3 id="基于贝叶斯的方法">基于贝叶斯的方法</h3>
<p>基于贝叶斯的方法的波束形成的最优权向量为(Bayes-Diagonal Loading-Sample Matrix Inversion, B-DL-SMI): <span class="math display">\[
\mathbf{w}_{_{B-DL-SMI}}=\sum_{i=1}^Lp(\theta_i\mid \mathbf{X})\mathbf{w}_{_{DL-SMI}}(\theta_i)
\]</span> 其中<span class="math inline">\(\mathbf{X}\)</span>​为<span class="math inline">\(K\)</span>​个快拍采样的阵列信号; <span class="math inline">\(\mathbf{w}_{_{DL-SMI}}(\theta_i)\)</span>​为DL-SMI波束形成以<span class="math inline">\(\theta_i\)</span>​作为期望方向的解.</p>
<p>要点:</p>
<ol type="1">
<li>在DL-SMI的基础上, 结合了加权平均, 这增强了鲁棒性.</li>
<li>需要根据<span class="math inline">\(\mathbf{X}\)</span>给出DOA角度的概率估计<span class="math inline">\(p(\theta_i\mid \mathbf{X})\)</span>.</li>
<li>DL-SMI波束形部分的对角加载系数<span class="math inline">\(\xi\)</span>​​需要人为确定.</li>
</ol>
<h3 id="基于最坏情况的方法">基于最坏情况的方法</h3>
<p>这种方法给导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​​的留下了一些误差余地<span class="math inline">\(\varepsilon\)</span>: <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w} \\
\mbox{s.t.}\quad &amp;\left\vert\mathbf{w}^H\left(\mathbf{a}(\theta_d)+\delta\right)\right\vert\ge 1, \quad \forall \Vert\delta\Vert\le\varepsilon\\
\end{alignat}
\]</span> 其中<span class="math inline">\(\delta\)</span>为导向向量的真实误差, <span class="math inline">\(\mathbf{a}(\theta_d)+\delta\)</span>表示真实的导向向量. 可用Lagrange乘子方法求得最优权向量(Worst-Sample Matrix Inversion, W-SMI): <span class="math display">\[
\mathbf{w}_{_{W-SMI}}=\frac{(\mathbf{R}+\lambda\varepsilon^2\mathbf{I})^{-1}\mathbf{a}(\theta_d)}{\lambda\mathbf{a}^H(\theta_d)(\mathbf{R}+\lambda\varepsilon^2 \mathbf{I})^{-1}\mathbf{a}(\theta_d)-1}
\]</span> 要点:</p>
<ol type="1">
<li>给导向向量<span class="math inline">\(\mathbf{a}(\theta_d)\)</span>​​留下的了一些误差余地带来了鲁棒性.</li>
<li>Lagrange乘子<span class="math inline">\(\lambda\)</span>​​​​​需要将最优权向量入原问题后用Newton迭代得到, 计算量较大.</li>
<li>误差余地<span class="math inline">\(\varepsilon\)</span>需要人为确定.</li>
</ol>
<h3 id="基于概率约束的方法">基于概率约束的方法</h3>
<p>当导向向量误差<span class="math inline">\(\delta\)</span>的分布已知, 可仅约束发生概率充分大的那些真实导向向量(Probability Constraint-Sample Matrix Inversion, PC-SMI) <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w} \\
\mbox{s.t.}\quad &amp;\mbox{Pr}\left\{\left\vert\mathbf{w}^H\left(\mathbf{a}(\theta_d)+\delta\right)\right\vert\ge 1\right\}\ge p\\
\end{alignat}
\]</span> 比如假定导向向量误差<span class="math inline">\(\delta\)</span>​满足零均值复高斯分布, 上述问题等价于 <span class="math display">\[
\begin{alignat}{2}
\min_{\mathbf{w}} \quad &amp; \mathbf{w}^H\mathbf{R}\mathbf{w} \\
\mbox{s.t.}\quad &amp;\sqrt{2}\mbox{erf}^{-1}(\sqrt{p})\Vert \mathbf{C}_{\delta}^{1/2}\mathbf{w}\Vert\le\mathbf{w}^H\mathbf{a}(\theta_d)-1\\
\end{alignat}
\]</span> 其中<span class="math inline">\(\mbox{erf}^{-1}(z)=\frac{2}{\sqrt{\pi}}\int_{0}^z e^{-x^2}\mbox{d}x\)</span>, <span class="math inline">\(\mathbf{C}_{\delta}\)</span>为导向向量误差<span class="math inline">\(\delta\)</span>的二阶统计. 当<span class="math inline">\(\mathbf{C}_{\delta}=\sigma_{\delta}^2\mathbf{I}\)</span>和<span class="math inline">\(\varepsilon=\sigma_{\delta}\sqrt{2}\mbox{erf}^{-1}(\sqrt{p})\)</span>, 基于概率约束的方法就等价于基于最坏情况的方法.</p>
<p>要点:</p>
<ol type="1">
<li>概率值<span class="math inline">\(p\)</span>需要人为确定.</li>
</ol>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>信号处理</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习中的几个概率不等式</title>
    <url>/2019/10/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A6%82%E7%8E%87%E4%B8%8D%E7%AD%89%E5%BC%8F/</url>
    <content><![CDATA[<p>前言: 福特的实习总算快结束了, 忙里偷闲写博客. 本文的初衷用简明扼要的语言说一说统计学习理论中的常见的概率不等式, 证明过程略, 只为了方便快速回忆. 主要参考了Shai Shalev-Shwartz的Understanding machine learning一书、<a href="https://murongxixi.github.io/2013/10/26/%E5%87%A0%E4%B8%AA%E6%A6%82%E7%8E%87%E4%B8%8D%E7%AD%89%E5%BC%8F/">这篇博客</a>以及维基百科.</p>
<span id="more"></span>
<p>设<span class="math inline">\(Z_i, i=1,...,m\)</span>是一列独立同分布的随机变量序列, 且在本文讨论的范围统计学习理论中, 这里的<span class="math inline">\(Z_i\)</span>可以直接理解为每个样本分类的正确与否, 因此有如下几个事实:</p>
<ul>
<li><span class="math inline">\(Z_i\)</span>有界, 且<span class="math inline">\(0\le Z_i\le1\)</span></li>
<li><span class="math inline">\(\frac{1}{m}\sum_{i=1}^{m}Z_i\)</span>即为大小为<span class="math inline">\(m\)</span>的训练集的分类正确率, 且<span class="math inline">\(0\le \frac{1}{m}\sum_{i=1}^{m}Z_i\le1\)</span></li>
<li>假定优化过程中训练集的分类错误率不断减小, 因此<span class="math inline">\(1-\frac{1}{m}\sum_{i=1}^{m}Z_i\)</span>为<a href="https://en.wikipedia.org/wiki/Martingale_(probability_theory)">上鞅</a>的假设视为合理.</li>
</ul>
<p>上述假设均可直接带入下述不等式去理解.</p>
<h2 id="moment-based-bound">Moment Based Bound</h2>
<p>基于矩的界主要是<a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov Inequality</a>和<a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">Chebyshev Inequality</a>.</p>
<h3 id="markov-inequality">Markov Inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z\)</span>为非负随机变量, <span class="math inline">\(\forall a &gt;0\)</span> <span class="math display">\[\mathbb{P}(Z\ge a)\le\frac{\mathbb{E}(Z)}{a}\]</span></p>
</blockquote>
<h3 id="chebyshev-inequality">Chebyshev Inequality</h3>
<blockquote>
<p><span class="math inline">\(\forall a &gt;0\)</span> <span class="math display">\[\mathbb{P}(\left|Z-\mathbb{E}(Z)\right|\ge a)\le\frac{Var(Z)}{a^2}\]</span></p>
</blockquote>
<h2 id="chernoff-bound">Chernoff Bound</h2>
<p>Chernoff Bound一般指用指数函数的单调性及Markov Inequality得到的界. 笔者按自己的理解再细分成了两类:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Chernoff_bound#Example">Chernoff Inequality</a>、<a href="https://en.wikipedia.org/wiki/Bennett%27s_inequality">Bennett Inequality</a> and <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein Inequality</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding Inequality</a> 、<a href="https://en.wikipedia.org/wiki/Doob_martingale#McDiarmid&#39;s_inequality">McDiarmid inequality</a> and <a href="https://en.wikipedia.org/wiki/Azuma%27s_inequality">Azuma–Hoeffding inequality</a></li>
</ul>
<h3 id="chernoff-inequality">Chernoff Inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i \sim B(1,p_i), i=1,...,m\)</span>为独立的伯努利随机变量序列, 那么对于<span class="math inline">\(\forall a &gt;0\)</span> <span class="math display">\[
\begin{aligned}\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i&gt;(1+a)\frac{1}{m}\sum_{i=1}^m p_i\right) &amp; \le \exp\left(- \frac{a^2}{2+2a/3}\sum_{i=1}^m p_i\right)\\\
\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i&lt;(1-a)\frac{1}{m}\sum_{i=1}^m p_i\right) &amp; \le \exp\left(- \frac{a^2}{2+2a/3}\sum_{i=1}^m p_i\right)\end{aligned}
\]</span></p>
</blockquote>
<h3 id="bennett-inequality">Bennett Inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i , i=1,...,m\)</span>为0均值独立随机变量序列, 且几乎处处有<span class="math inline">\(Z_i\le c\)</span>. 那么对于<span class="math inline">\(\forall a &gt;0\)</span>,<span class="math inline">\(\forall \sigma^2\ge\frac{1}{m}\sum_{i=1}^{m}\mathbb{E}\left[Z_i^2\right]\)</span> <span class="math display">\[
\begin{aligned}\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i&gt;a \right) &amp; \le \exp\left(- \frac{m\sigma^2}{c^2}h\left(\frac{ac}{\sigma^2}\right)\right)\end{aligned}
\]</span>其中<span class="math inline">\(h(x)=(1+x)\log(1+x)-x\)</span></p>
</blockquote>
<h3 id="bernstein-inequality">Bernstein Inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i , i=1,...,m\)</span>为0均值独立随机变量序列, 且几乎处处有<span class="math inline">\(\left\vert Z_i\right\vert\le c\)</span>. 那么对于<span class="math inline">\(\forall a &gt;0\)</span> <span class="math display">\[
\begin{aligned}\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i&gt;a \right) &amp; \le \exp\left(- \frac{m^2 a^2}{2\sum_{j=1}^m\mathbb{E}[Z_j^2]+2acm/3}\right)\end{aligned}
\]</span></p>
</blockquote>
<h3 id="hoeffding-inequality">Hoeffding Inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i , i=1,...,m\)</span>为独立随机变量序列, 且几乎处处有<span class="math inline">\(l_i\le Z_i\le u_i\)</span>. 那么对于<span class="math inline">\(\forall a &gt;0\)</span> <span class="math display">\[
\begin{aligned}\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i - \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m} Z_i \right] \ge a\right) &amp; \le\exp\left(- \frac{2m^2a^2}{\sum_{i=1}^m(u_i-l_i)^2}\right)\\\
\mathbb{P}\left(\frac{1}{m}\sum_{i=1}^{m} Z_i - \mathbb{E}\left[\frac{1}{m}\sum_{i=1}^{m} Z_i \right] \le -a\right) &amp; \le\exp\left(- \frac{2m^2a^2}{\sum_{i=1}^m(u_i-l_i)^2}\right)\end{aligned}
\]</span></p>
</blockquote>
<h3 id="mcdiarmid-inequality">McDiarmid inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i , i=1,...,m\)</span>为独立随机变量序列, <span class="math inline">\(f(Z_1,...,Z_m)\)</span>为一映射, 且几乎处处有<span class="math display">\[\sup_{z_1,...,z_i,z_i&#39;,...,z_m}|f(z_1,...,z_i,...,z_m)-f(z_1,...,z_i&#39;,...,z_m)|\le c_i\]</span>成立. 那么对于<span class="math inline">\(\forall a&gt;0\)</span> <span class="math display">\[\begin{aligned}\mathbb{P}\left(f(Z_1,...,Z_m)- \mathbb{E}\left[f(Z_1,...,Z_m) \right] \ge a\right) &amp; \le\exp\left(- \frac{2a^2}{\sum_{i=1}^mc_i^2}\right) \\\       \mathbb{P}\left(f(Z_1,...,Z_m)- \mathbb{E}\left[f(Z_1,...,Z_m) \right] \le -a\right) &amp; \le\exp\left(- \frac{2a^2}{\sum_{i=1}^mc_i^2}\right)\end{aligned}\]</span></p>
</blockquote>
<p><code>p.s.</code> 该不等式可视为Hoeffding’s Inequality的推广, <span class="math inline">\(f\)</span>取为均值映射就是Hoeffding’s Inequality.</p>
<h3 id="azumahoeffding-inequality">Azuma–Hoeffding inequality</h3>
<blockquote>
<p>设<span class="math inline">\(Z_i , i=0,1,...,m\)</span>为一个鞅(上鞅), 且几乎处处有<span class="math inline">\(|Z_k-Z_{k-1}|\le c_k\)</span>成立. 那么对于<span class="math inline">\(\forall a&gt;0\)</span> <span class="math display">\[\begin{aligned}\mathbb{P}\left(Z_m-Z_0  \ge a\right) &amp; \le\exp\left(- \frac{a^2}{2\sum_{i=1}^mc_i^2}\right) \\\       \mathbb{P}\left(Z_m-Z_0  \le -a\right) &amp; \le\exp\left(- \frac{a^2}{2\sum_{i=1}^mc_i^2}\right)\end{aligned}\]</span></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>统计学习</tag>
        <tag>概率论</tag>
        <tag>不等式</tag>
      </tags>
  </entry>
  <entry>
    <title>自然梯度</title>
    <url>/2019/07/06/%E8%87%AA%E7%84%B6%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<p>前言: 本文介绍自然梯度算法, 这种方法考虑到许多问题的参数空间具有某种结构, 因此考虑放在非欧式空间中优化(但实际优化计算过程仍然在欧式空间), 有兴趣的同学可以读一读原论文<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf">Natural Gradient</a>. <span id="more"></span></p>
<h2 id="欧式空间与黎曼空间">欧式空间与黎曼空间</h2>
<p>什么是欧式空间?</p>
<blockquote>
<p>A finite dimensional real vector space equipped with an inner product <span class="math inline">\(\left&lt; x,y \right&gt;\)</span>, is called a <strong>Eulidean space</strong>, if it is endowed with the norm <span class="math inline">\(\left\Vert x\right\Vert=\sqrt{\left&lt; x,x \right&gt;}\)</span>, which is referred to as <strong>Eulidean norm</strong>.</p>
</blockquote>
<p>通俗的讲, 首先得有一个向量空间<span class="math inline">\(\mathbb{R}^n\)</span>, 接着在这个空间上定义内积(inner product), 这个内积可以是常见的内积点乘, 也可以是其它内积:</p>
<ul>
<li>dot product: <span class="math inline">\(\left&lt; x,y \right&gt;=\sum_{i=1}^nx_iy_i\)</span>, 其中<span class="math inline">\(x_i\)</span>为<span class="math inline">\(x\)</span>的第<span class="math inline">\(i\)</span>个分量.</li>
<li><span class="math inline">\(Q\)</span> inner: <span class="math inline">\(\left&lt; x,y \right&gt;=x^TQy\)</span>, 其<span class="math inline">\(Q\)</span>为<span class="math inline">\(n\)</span>阶正定阵.</li>
</ul>
<p>然后由内积可以诱导出对应的范数:</p>
<ul>
<li><span class="math inline">\(l_2\)</span> norm: <span class="math inline">\(\left\Vert x\right\Vert=\sqrt{\left&lt; x,x \right&gt;}=\sqrt{ \sum_{i=1}^n x_i^2 }\)</span>, 一般记为<span class="math inline">\(\left\Vert x\right\Vert_2\)</span>.</li>
<li><span class="math inline">\(Q\)</span> norm: <span class="math inline">\(\left\Vert x\right\Vert=\sqrt{\left&lt; x,x \right&gt;}=\sqrt{x^TQx}\)</span>, 一般记为<span class="math inline">\(\left\Vert x\right\Vert_Q\)</span>.</li>
</ul>
<p>有了范数, 我们就可以度量两个点<span class="math inline">\(x,x+dx\)</span>之间的距离(点乘内积): <span class="math display">\[\Vert x + dx-x\Vert=\Vert dx\Vert=\sqrt{ \sum_{i=1}^n dx_i^2 }.\]</span></p>
<p>小结一下:</p>
<ul>
<li>欧式空间是一个向量空间, 且带有内积和范数;</li>
<li>最重要的是, 该范数一定是要由内积根据上式诱导出来的.</li>
</ul>
<p>什么是黎曼空间?</p>
<blockquote>
<p>A differentiable manifold provided with a <strong>Riemannian metric</strong>.</p>
</blockquote>
<p>微分流形(differentiable manifold又名光滑流形, 其最简单的例子就是三维空间中的球面, 在这里可简单理解为向量空间; 黎曼度量(Riemannian metric.)又名<a href="https://zh.wikipedia.org/wiki/%E5%BA%A6%E9%87%8F%E5%BC%A0%E9%87%8F">度量张量</a>, 黎曼度量张量或是度规张量, 这里可简单理解为某<span class="math inline">\(n\)</span>阶方阵<span class="math inline">\(G\)</span>或是<span class="math inline">\((g_{ij})\)</span>. 有了它我们就可以用它来度量空间中两点<span class="math inline">\(x,x+dx\)</span>之间的距离: <span class="math display">\[dx^2=\sum_{ij}g_{ij}dx_i dx_j=dx^TGdx\]</span>其中<span class="math inline">\(dx_i\)</span>为<span class="math inline">\(dx\)</span>的第<span class="math inline">\(i\)</span>个分量. 特别的, 若取<span class="math inline">\(G\)</span>为<span class="math inline">\(n\)</span>阶单位阵(对角线上元素为1, 其余为0), 此时<span class="math inline">\(dx^2=\sum_{i}^ndx_i^2\)</span>. 这就与带有正交坐标系的欧式空间一致了(点乘内积). 小结一下:</p>
<ul>
<li>黎曼空间可视为欧式空间的推广, 但一般情况下, 黎曼空间为非欧空间.</li>
<li>确定了<span class="math inline">\(G\)</span>就相当于确定了一个黎曼空间, 另外形式上类似欧式空间中的<span class="math inline">\(Q\)</span> norm但没有正定性的要求.</li>
</ul>
<h2 id="自然梯度">自然梯度</h2>
<p>带正交坐标系的欧式空间中, 我们知道负梯度方向<span class="math inline">\(-\nabla L(w)\)</span>为<span class="math inline">\(L(w)\)</span>的最速下降方向; 类似的, 黎曼空间中的<span class="math inline">\(L(w)\)</span>最速下降方向为<span class="math inline">\(-G^{-1}(w)\nabla L(w)\)</span>, 这被称为是自然梯度, 证明可见<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf">Natural Gradient Works Efficiently in Learning</a>的定理1. 下面举个例子加以说明:</p>
<ul>
<li>现有一族带参的分布族<span class="math inline">\(\{p(z\mid w)\}\)</span>, 目标是找到最优参数<span class="math inline">\(w=w^*\)</span>使得<span class="math inline">\(p(z\mid w)\)</span>尽可能的接近未知的密度函数<span class="math inline">\(q(z)\)</span>.</li>
<li>事实上, 很多机器学习任务可归结为上述问题, 比方说<span class="math inline">\(q(z)=q(x,y)\)</span>表示某监督学习任务的样本真实分布, <span class="math inline">\(w\)</span>表示某个NN的参数.</li>
<li>一个常用的损失函数就是最小化交叉熵的负数:<span class="math display">\[L(w)=-\mathbb{E}_{q(z)}[\log p(z\mid w)]=\mathcal{KL}(q(z)\Vert p(z\mid w))-\mathbb{E}_{q(z)}[\log q(z)]\]</span>其中<span class="math inline">\(\mathcal{KL}(q(z)\Vert p(z\mid w))=\mathbb{E}_{q(z)}[\log\frac{q(z)}{p(z\mid w)}]=\int q(z)\log\frac{q(z)}{p(z\mid w)}dz\)</span>, 且第二项与<span class="math inline">\(w\)</span>无关.</li>
<li>Information geometry (Amari, 1985)给出了利用<a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher信息阵</a>定义<span class="math inline">\(G\)</span>(概率密度函数族可以看做与参数空间同胚的黎曼流形, Fisher信息矩阵可以看做是该流形上的黎曼度量, 可以证明这一度量是外围欧式空间在该流形上的诱导度量): <span class="math display">\[G_{ij}(w)=I_{ij}(w)=-\mathbb{E}_{p(z\mid w)}\left [\frac{\partial \log p(z\mid w)}{\partial w_i}\frac{\partial \log p(z\mid w)}{\partial w_j}\right]\]</span></li>
<li>最后假设有一堆独立同分布样本<span class="math inline">\(z_i \sim p(z)\)</span>, 就可用stochastic或batch的思路去优化. 此时<span class="math inline">\(L(w)\)</span>的自然梯度方向即为<span class="math inline">\(-I^{-1}(w)\nabla L(w)\)</span>, 这又被称为是<strong>Fisher scoring algorithm</strong>.</li>
</ul>
<p>回过头来看, 对<span class="math inline">\(q(z)\)</span>的参数化近似<span class="math inline">\(p(z,w)\)</span>必定赋予了它某种结构(比方说对<span class="math inline">\(w\)</span>迈出一大步未必能导致<span class="math inline">\(p(z,w)\)</span>的大的变化), 而要想利用这种结构, 最大的难点就是如何找到对应的黎曼空间, 或者说<span class="math inline">\(G\)</span>.</p>
<ul>
<li>第6节给出了<strong>感知机模型</strong>的参数空间的相对应<span class="math inline">\(G, G^{-1}\)</span>的详细求解过程, 同时还有<strong>多层感知机模型</strong>的<span class="math inline">\(G\)</span>.</li>
<li>第7节给出了<span class="math inline">\(m\)</span>阶非奇异方阵构成的空间中自然梯度的求法; 第8节给出了系统空间中的自然梯度.</li>
</ul>
<h2 id="自然梯度与牛顿类法">自然梯度与牛顿类法</h2>
<p>形式上, 自然梯度法类似于牛顿法: <span class="math display">\[G(w)=-\mathbb{E}_{p(z\mid w)}\left[ \frac{\partial^2\log p(z\mid w)}{\partial w^2}\right], \nabla ^2L (w)=-\mathbb{E}_{q(z)}\left[ \frac{\partial^2\log p(z\mid w)}{\partial w^2}\right]\]</span>且随着优化算法的进行, <span class="math inline">\(p(z\mid w)\)</span>会越来越接近<span class="math inline">\(q(z)\)</span>, 此时两者的表现会相近.当然, 实际任务中因为实际分布一般不知道因此再多一层近似, 即用batch或是stochastic近似上述右式. 最后, 由于<a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher信息阵</a>的一个特殊性质, 在满足<a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound#Regularity_conditions">CR条件</a>的前提下, 有<span class="math display">\[\mathbb{E}_{p(z\mid w)}\left[ \frac{\partial^2\log p(z\mid w)}{\partial w^2}\right]=\mathbb{E}_{p(z\mid w)}\left[ (\frac{\partial\log p(z\mid w)}{\partial w})(\frac{\partial\log p(z\mid w)}{\partial w})^T\right]\]</span>若我把上式中的<span class="math inline">\(p(z\mid w)\)</span>换成<span class="math inline">\(q(z)\)</span>, <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm">Gauss-Newton法</a>就跃然纸上.</p>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>黎曼空间</tag>
        <tag>欧式空间</tag>
        <tag>Fisher信息</tag>
        <tag>自然梯度</tag>
      </tags>
  </entry>
  <entry>
    <title>近端梯度方法</title>
    <url>/2021/01/19/%E8%BF%91%E7%AB%AF%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>前言: 本文的主角是近端梯度方法(Proximal Gradient Method), 这是一个在数学优化领域广为人知，在机器学习领域却很少看到的数学优化方法，它在非光滑优化领域发挥着巨大的作用，下面我们从一个机器学习问题开始了解它能够解决什么问题。 <span id="more"></span></p>
<h3 id="正则化损失最小化">正则化损失最小化</h3>
<p>正则化损失最小化(Regularized Loss Minimization, RLM)是机 器学习领域中一种联合最小化经验⻛险和正则化函数的学习范式，其形式如下: <span class="math display">\[
\min_{\omega}L_S(\omega)+\alpha\mathcal{R}(\omega)
\]</span></p>
<p>这里假定<span class="math inline">\(L_S(\omega)\)</span>表示关于训练样本集<span class="math inline">\(S\)</span>和模型系数<span class="math inline">\(\omega\)</span>的经验⻛险项，代表模型对训练 样本集<span class="math inline">\(S\)</span>拟合的好坏程度;<span class="math inline">\(\mathcal{R}(\omega)\)</span>表示关于模型系数<span class="math inline">\(\omega\)</span>的正则项，代表我们 对模型的额外偏好;正则项系数<span class="math inline">\(\alpha\)</span>用于控制正则ß程度。以回归为例，当经验⻛险项为最小二乘损 失<span class="math inline">\(L_S(\omega)=(X\omega-Y)^T(X\omega-Y)\)</span>时，若正则项为<span class="math inline">\(\ell_2\)</span>正则<span class="math inline">\(\mathcal{R} (\omega)=\Vert\omega\Vert_2^2\)</span>，此时RLM问题为 <span class="math display">\[
\min_{\omega}(X\omega-Y)^T(X\omega-Y)+\alpha\Vert\omega\Vert_2^2
\]</span></p>
<p>其有闭式解<span class="math inline">\(\omega_*=(X^TX+\alpha I)^{-1}X^TY\)</span>; 若正则项为<span class="math inline">\(\ell_1\)</span>正则<span class="math inline">\(\mathcal{R}(\omega)=\Vert\omega\Vert_1\)</span>，此时RLM问题为</p>
<p><span class="math display">\[\min_{\omega}(X\omega-Y)^T(X\omega-Y)+\alpha\Vert\omega\Vert_1\]</span> 上述问题又名Lasso问题，可以采用坐标下降方法、近端梯度方法(Proximal Gradient Method, PGM)去解决，且相比<span class="math inline">\(\ell_2\)</span>正则，<span class="math inline">\(\ell_1\)</span>正则得到的解更为稀疏。</p>
<h3 id="近端梯度方法解决ell_1正则化问题">近端梯度方法解决<span class="math inline">\(\ell_1\)</span>正则化问题</h3>
<p>一般的，考虑如下问题带正则项的机器学习问题</p>
<p><span class="math display">\[\min_{\omega}L_S(\omega)+\alpha\mathcal{R}(\omega)\]</span> 其中<span class="math inline">\(L_S(\omega)\)</span>具有L-smooth性质，即<span class="math inline">\(\Vert\nabla_\omega L_S(x)-\nabla_\omega L_S(y)\Vert_2\le L\Vert x-y\Vert_2\)</span>。近端梯度方法处理的方式为迭代地进行如下计算</p>
<ol type="1">
<li>计算</li>
</ol>
<p><span class="math display">\[
z_k=\omega_k-\frac{1}{L}\nabla L_S(\omega_k)
\]</span></p>
<ol start="2" type="1">
<li>求解</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\omega_{k+1} &amp;= \arg\min_\omega\ \frac{L}{2}\Vert \omega-z_k\Vert_2^2+\alpha \mathcal{R}(\omega) \\
&amp;=\arg\min_\omega\ \frac{1}{2}\Vert \omega-z_k\Vert_2^2+\frac{\alpha}{L} \mathcal{R}(\omega)\\
\end{aligned}
\]</span></p>
<p>第一步中的 <span class="math inline">\(\nabla L_S(\omega_k)\)</span> 表示<span class="math inline">\(L_S(\omega)\)</span>在<span class="math inline">\(\omega=\omega_k\)</span>处的梯度；第二步的计算复杂度则依赖于<span class="math inline">\(\mathcal{R}(\omega)\)</span>的选取。接下来我们从第二步的计算中抽象出近端算子<span class="math inline">\(\operatorname{prox}_{h}(x)\)</span>的概念</p>
<p><span class="math display">\[
\operatorname{prox}_{h}(z)=\underset{\omega \in \mathbb{R}^{n}}{\arg \min }\ \frac{1}{2}\|\omega-z\|_{2}^{2}+h(\omega)
\]</span></p>
<p>当<span class="math inline">\(h(\omega)=\lambda\|\omega\|_{1}\)</span>，<span class="math inline">\(\lambda&gt;0\)</span>，<span class="math inline">\(\operatorname{prox}_{h}(x)\)</span>恰为软阈值算子(Soft Thresholding)[1]</p>
<p><span class="math display">\[
\left[\operatorname{prox}_{h}(z)\right]_{i}=\left\{\begin{array}{ll}
0 &amp; \text { if }\left|z_{i}\right| \leq \lambda \\
z_{i}-\lambda &amp; \text { if } z_{i}&gt;\lambda \\
z_{i}+\lambda &amp; \text { if } z_{i}&lt;-\lambda
\end{array} \quad i=1, \ldots, n\right.;
\]</span></p>
<p>上述闭式解意味着对于正则项为<span class="math inline">\(\ell_1\)</span>的RLM问题，近端梯度方法能够高效的解决。</p>
<h3 id="近端梯度方法解决其它正则问题">近端梯度方法解决其它正则问题</h3>
<p>当<span class="math inline">\(h(\omega)=\sum_{i=1}^{n}\left(\lambda^{2}-\left(\left|\omega_{i}\right|-\lambda\right)^{2} \mathbb{I}\left(\left|\omega_{i}\right|&lt;\lambda\right)\right)\)</span>, <span class="math inline">\(\operatorname{prox}_{h}(x)\)</span>恰为硬阈值算子(Hard Thresholding)[2]; 当<span class="math inline">\(h(\omega)=\|\omega\|_{1}-\lambda\|\omega\|_{2}\)</span>, [3]给出了闭式解。因此，对于上述两种正则的RLM问题而言，近端梯度方法也能够高效的解决。</p>
<h3 id="近端梯度方法的一种直观理解">近端梯度方法的一种直观理解</h3>
<p>回到近端梯度方法处理的问题上来</p>
<p><span class="math display">\[
\min_{\omega}L_S(\omega)+\alpha\mathcal{R}(\omega)
\]</span></p>
<p>近端梯度方法做的事可以理解为分步最优化原目标函数的一个上界 <span class="math display">\[
\begin{aligned}
L_S(\omega)+\alpha\mathcal{R}(\omega) &amp; \leq L_S(\omega_k)+\left\langle\nabla L_S(\omega_k), \omega-\omega_k\right\rangle+\frac{L}{2}\left\|\omega-\omega_k\right\|_{2}^{2}+\alpha\mathcal{R}(\omega) \\
&amp;=\frac{L}{2}\left\|\omega-\left(\omega_k-\frac{1}{L} \nabla L_S(\omega_k)\right)\right\|_{2}^{2}+\alpha\mathcal{R}(\omega) \\
\end{aligned}
\]</span> 第一个不等式可由L-smooth性质得到，而近端梯度方法等价于最小化上式。</p>
<hr />
<p>[1] Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.</p>
<p>[2] Antoniadis, Anestis. "Wavelets in statistics: a review." Journal of the Italian Statistical Society 6.2 (1997): 97.</p>
<p>[3] Lou, Yifei, and Ming Yan. "Fast L1–L2 minimization via a proximal operator." Journal of Scientific Computing 74.2 (2018): 767-785.</p>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>近端</tag>
        <tag>近端梯度</tag>
        <tag>正则</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习的基本定理</title>
    <url>/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/</url>
    <content><![CDATA[<p>本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:</p>
<ol type="1">
<li>统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理.</li>
<li>三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency).</li>
<li>三种学习范式:经验风险最小化(ERM)、结构风险最小化(SRM)及最小描述长度(MDL).</li>
</ol>
<p>最后要强调的是这块理论是针对监督学习中的二分类问题而言的,对于多分类问题,可将VC维拓展为<a href="https://link.springer.com/content/pdf/10.1007%2FBF00114804.pdf">Natarajan维</a>.</p>
<span id="more"></span>
<h1 id="统计学习理论的基本定理">统计学习理论的基本定理</h1>
<p>在给出最核心的定理之前, 应当接触以下几个概念:</p>
<ul>
<li>样本空间: 对于监督学习而言为带标签的样本<span class="math inline">\((x,y)\)</span>的取值空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>, 学习的目标是映射<span class="math inline">\(f: \mathcal{X}\to\mathcal{Y}\)</span>, 以用于产生对新样本<span class="math inline">\(x_{new}\)</span>的标签的可靠预测<span class="math inline">\(f(x_{new})\)</span>. <code>p.s.</code>对无监督学习而言为不带标签的样本<span class="math inline">\(x\)</span>的取值空间<span class="math inline">\(\mathcal{X}\)</span>, 学习目标为对<span class="math inline">\(\mathcal{X}\)</span>的可靠分类.</li>
<li>目标函数: 通常记为<span class="math inline">\(f\)</span>, 即上述监督学习中所说的映射<span class="math inline">\(f\)</span>. 对于分类问题就是我们希望得到的真实的分类函数.</li>
<li>训练集: 通常记为<span class="math inline">\(S\)</span>, 由定义在样本空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>上的概率分布<span class="math inline">\(\mathcal{D}\)</span>抽样得到 <span class="math display">\[S=\{(x_{1},y_{1}),....,(x_{m},y_{m})\}\]</span></li>
<li>学习算法: 通常记为<span class="math inline">\(A\)</span>, 即我们构建的、能从数据中学到东西的学习算法, 期望其能从假设类中返回良好的假设.</li>
<li>假设类: 通常记为<span class="math inline">\(\mathcal{H}\)</span>, 对目标函数<span class="math inline">\(f\)</span>的假设构成的空间, 学习器从中选择.</li>
<li>预测器(predictor): 或者说分类器(classifier)、假设(hypothesis), 通常记为<span class="math inline">\(h\)</span>, 即学习器输出的一个函数:<span class="math inline">\(h:\mathcal{X}\to\mathcal{Y}\)</span>, 且应当和目标函数<span class="math inline">\(f\)</span>有相近的函数值.</li>
<li>损失函数: 通常记为<span class="math inline">\(l(h, (x, y))\)</span>, 其定义在预测器<span class="math inline">\(h\)</span>和单个样本<span class="math inline">\((x, y)\)</span>上. 对于二分类问题, 一个常用的损失函数为<span class="math inline">\(l(h, (x, y))=I\{y\neq h(x)\}\)</span>, 即分对了就为0, 分错了就为1.</li>
<li>训练误差: 通常记为<span class="math inline">\(L_{S}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在训练集<span class="math inline">\(S\)</span>上的平均损失<span class="math display">\[L_{S}(h)=\frac{1}{m}\sum_{i=1}^m l(h, (x_i, y_i))\]</span></li>
<li>泛化误差: 通常记为<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在<span class="math inline">\((\mathcal{X}, \mathcal{Y})\)</span>上的平均损失<span class="math display">\[\mathbb{E}_{(x, y)\sim (\mathcal{X}, \mathcal{Y})}l(h, (x, y))\]</span></li>
</ul>
<p>这里给出的概念及记号借鉴了深入理解机器学习一书中的写法. 下面的内容适合有一定基础的同学阅读, 这里给出最核心的定理:</p>
<blockquote>
<p>令<span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>的映射函数构成的假设空间, 且令损失函数为0-1损失. 那么,下述陈述等价:</p>
<ol type="1">
<li><span class="math inline">\(\mathcal{H}\)</span>有一致收敛性.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</li>
</ol>
</blockquote>
<p>下面我会一一解释上述出现的名词并证明该定理, 证明分两块</p>
<ul>
<li>1推2推3推4推6推1</li>
<li>1推2推5推6推1</li>
</ul>
<p>其中6推1为难点.</p>
<h2 id="一致收敛性">一致收敛性</h2>
<h3 id="varepsilon-代表性样本"><span class="math inline">\(\varepsilon\)</span>-代表性样本</h3>
<p><span class="math inline">\(\varepsilon\)</span>-代表性样本(<span class="math inline">\(\varepsilon\)</span>-representative sample):</p>
<blockquote>
<p>如果满足下列不等式:<span class="math inline">\(\forall h \in\mathcal{H},|L_{S}(h)-L_{\mathcal{D}}(h)|\le\varepsilon\)</span>,训练集<span class="math inline">\(S\)</span>就称作<span class="math inline">\(\varepsilon\)</span>-代表性样本.</p>
</blockquote>
<p>泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>为<span class="math inline">\(h\)</span>在概率分布<span class="math inline">\(\mathcal{D}\)</span>上的表现, 定义为<span class="math display">\[L_{\mathcal{D}}(h)=\mathbb{P}_{(x,y)\in \mathcal{D}}[h(x)\neq y]\]</span>它是能反应假设<span class="math inline">\(h\)</span>的性能的最真实的误差. 但一般情况下, 我们无法知道<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(f\)</span>是什么样子的, 所以无法直接获知真实的误差. 一种思路是转而去关心训练误差和泛化误差界:</p>
<ul>
<li>训练误差<span class="math inline">\(L_{S}(h)\)</span>为<span class="math inline">\(h\)</span>在训练样本<span class="math inline">\(S\)</span>上的表现, 定义为<span class="math display">\[L_{S}(h)=\frac{1}{|S|}\sum_{(x,y)\in S}I\{h(x)\neq y\}\]</span>由于这里的<span class="math inline">\(S\)</span>已知, 因此能够计算得到训练误差.</li>
<li>泛化误差界定义为<span class="math display">\[\sup_{h\in\mathcal{H}}|L_{S}(h)-L_{\mathcal{D}}(h)|\]</span>一般来说, 有关泛化误差界的一些结果如<a href="https://msgsxj.cn/2018/06/14/VC-bound/">VC界</a>等总是在以<span class="math inline">\(1-\delta\)</span>概率成立的意义下, 显然这个界会与假设类<span class="math inline">\(\mathcal{H}\)</span>的选择有关.</li>
</ul>
<p>控制好了这两个量, 就可以认为泛化误差也被控制住了.</p>
<p><code>p.s.</code> <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本即说明哪怕<span class="math inline">\(h\)</span>是闭着眼睛瞎选的, 它在<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(S\)</span>上的表现都会差不多, 这说明此时训练集<span class="math inline">\(S\)</span>是足以去代表<span class="math inline">\(\mathcal{D}\)</span>的.</p>
<h3 id="一致收敛性-1">一致收敛性</h3>
<p>一致收敛性(Uniform Convergence, UC)这一概念从学习成功所需训练集大小的角度刻画了假设类的复杂度:</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>具有一致收敛性, 若存在一个函数<span class="math display">\[m^{UC}_{\mathcal{H}}:(0,1)\times (0,1)\to\mathbb{N}\]</span> 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>、<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>、从<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到的大小为<span class="math inline">\(m\)</span>的训练集<span class="math inline">\(S\)</span>, 只要<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\)</span>, 就有至少在概率<span class="math inline">\(1-\delta\)</span>下, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性的.</p>
</blockquote>
<p>这里的一致指的是对假设类<span class="math inline">\(\mathcal{H}\)</span>中的假设<span class="math inline">\(h\)</span>和<span class="math inline">\(Z\)</span>上任意概率分布<span class="math inline">\(\mathcal{D}\)</span>的一致性, 满足一致收敛的类也叫做Glivenko-Cantelli类.</p>
<p><code>p.s.</code> <span class="math inline">\(\varepsilon\)</span>-代表性可以理解为训练集足够好, 好到泛化误差界<span class="math display">\[\sup_{h\in\mathcal{H}}|L_{S}(h)-L_{\mathcal{D}}(h)|\]</span>能被<span class="math inline">\(\varepsilon\)</span>控制住. 若假设类<span class="math inline">\(\mathcal{H}\)</span>具有一致收敛性, 那么不管是什么样的要求, 只要训练集的样本数<span class="math inline">\(m\)</span>足够大且是独立抽样得到, 那我就能在一定概率下保证泛化误差能被<span class="math inline">\(\varepsilon\)</span>控制住. 假想这么一种情况, 假设类<span class="math inline">\(\mathcal{H}\)</span>足够复杂, 因此对于任意的<span class="math inline">\(S\)</span>, 遍历<span class="math inline">\(h\in\mathcal{H}\)</span>就能让<span class="math inline">\(L_{S}(h)\)</span>遍历<span class="math inline">\([0,1]\)</span>, 继而我总能找到一个<span class="math inline">\(h\)</span>, 使得<span class="math display">\[|L_{S}(h)-L_{\mathcal{D}}(h)|&gt;\varepsilon\]</span>事实上, 一致收敛性中对<span class="math inline">\(h\in\mathcal{H}\)</span>的一致性的要求本质上要求了假设类<span class="math inline">\(\mathcal{H}\)</span>不能过于复杂.</p>
<h3 id="经验风险最小化erm">经验风险最小化(ERM)</h3>
<p>经验风险最小化(ERM:Empirical Risk Minimization):</p>
<blockquote>
<p>对于学习器来说, 训练样本是真实世界的一个缩影, 一种很自然的想法就是通过最小化训练误差<span class="math inline">\(L_{S}(h)\)</span>来寻找<span class="math inline">\(h\)</span>, 这种最小化<span class="math inline">\(L_{S}(h)\)</span>的学习范式称为是经验风险最小化.</p>
</blockquote>
<p><code>p.s.</code> 按照同时控制训练误差和泛化误差界这两个量的思路, 我们只需要设计好算法来最小化训练误差, 同时恰当选择假设类以控制好泛化误差界, 就能控制住泛化误差, 即所谓的学习成功.</p>
<h2 id="pac可学习">PAC可学习</h2>
<p>PAC(Probably Approximately Correct)可学习的概念主要包括不可知PAC可学习(Agnostic PAC Learnability), PAC可学习(PAC Learnability)及广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions)这三个概念.</p>
<h3 id="不可知pac可学习">不可知PAC可学习</h3>
<p>不可知PAC可学习(Agnostic PAC Learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 若存在一个函数<span class="math display">\[m_{\mathcal{H}}:(0,1)\times(0,1)\to\mathbb{N}\]</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>、<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>、从<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到的大小为<span class="math inline">\(m\)</span>的训练集<span class="math inline">\(S\)</span>, 只要<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon, \delta)\)</span>, 算法<span class="math inline">\(A\)</span>通过最小化训练误差<span class="math inline">\(L_{S}(h)\)</span>将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<p>倘若此时假设类<span class="math inline">\(\mathcal{H}\)</span>只包含贝叶斯最优分类器, 即以概率1满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>, 此时我只需将贝叶斯最优分类器的函数值取其对立面(将函数输出结果的01对调), 就能将概率1降为概率0.5. 从这个层面看, 不可知PAC可学习也要求了假设类不能过于复杂.</p>
<p><code>p.s.</code> 与一致收敛性不同的是, 不可知PAC可学习通过考察<strong>假设类中的最优假设</strong>和<strong>最优秀的算法返回的假设</strong>之间的泛化误差差距来控制假设类的复杂度, 因而需要剔除训练误差优秀、泛化误差垃圾的假设; 而一致收敛性要求假设类中的一致性同样要求将训练误差优秀、泛化误差垃圾的假设剔除.</p>
<p><code>p.s.</code>回头来看定理的第一第二条. <span class="math inline">\(\mathcal{H}\)</span>有一致收敛性等价于任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器, 也就是说不可知PAC学习器再加上一个不错的方法(ERM规则), 就能等同于一致收敛性.</p>
<h3 id="pac可学习-1">PAC可学习</h3>
<p>PAC可学习(PAC Learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 若存在一个函数<span class="math display">\[m_{\mathcal{H}}:(0,1)\times(0,1)\to\mathbb{N}\]</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 任意的<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>, 如果在<span class="math inline">\(\mathcal{H},\mathcal{D},f\)</span>下满足可实现的假设, 那么当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到并由<span class="math inline">\(f\)</span>标记, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \varepsilon\)</span>.</p>
</blockquote>
<p>不可知PAC可学习实际上是PAC可学习的范化, 这里假定样本的标签由<span class="math inline">\(f(x)\)</span>给出, 这实际上假设了不出现样本相同标签不同的情况. 另外由<span class="math inline">\(L_{\mathcal{D}}(f)=0\)</span>可知, PAC可学习还要求了假设类<span class="math inline">\(\mathcal{H}\)</span>必然包含<span class="math inline">\(f\)</span>.</p>
<p>如果假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 有很多函数<span class="math inline">\(m_{\mathcal{H}}\)</span>满足PAC可学习的定义给出的条件, 我们定义采样复杂度为满足条件的最小函数, 下面是有限假设类的一个例子:</p>
<blockquote>
<p>任意有限假设类是PAC可学习的,其采样复杂度满足:<span class="math inline">\(m_{\mathcal{H}}(\varepsilon,\delta)\le [\frac{log(|\mathcal{H}|)/\delta}{\varepsilon}]\)</span>.</p>
</blockquote>
<p>该定理用霍夫丁不等式(<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding's inequality</a>)可证, 霍夫丁不等式的证明可见链接或是深入理解机器学习一书附录B.4, 该定理的详细证明可见深入理解机器学习一书2.3节.</p>
<p><code>p.s.</code> 霍夫丁不等式(Hoeffding's inequality):</p>
<blockquote>
<p><span class="math inline">\(\theta_{1},...,\theta_{m}\)</span>是一个独立同分布的r.v.序列, 假设对所有的<span class="math inline">\(i\)</span>, <span class="math inline">\(E[\theta_{i}]=\mu\)</span>,且<span class="math inline">\(\mathbb{P}[a\le\theta_{i}\le b]=1\)</span>, 则对于<span class="math inline">\(\forall\varepsilon&gt;0\)</span> <span class="math display">\[\mathbb{P}[|\frac{1}{m}\sum^{m}_{i=1}\theta_{i}-\mu|&gt;\varepsilon]\le2\exp(-2m\varepsilon^{2}/(b-a)^{2})\]</span></p>
</blockquote>
<h3 id="广义损失函数下的不可知pac可学习">广义损失函数下的不可知PAC可学习</h3>
<p>为了能处理其他学习任务比方说多分类, 回归问题, 我们将损失函数进行如下范化: 广义损失函数:</p>
<ul>
<li>0-1损失, 这个损失函数用在二分类或者多分类问题中. <span class="math display">\[ l_{0-1}(h,(x,y))=0,where\{h(x)=y\};1,where \{h(x)\neq y\}\]</span></li>
<li>平方损失,这个损失函数用在回归问题中. <span class="math display">\[l_{sq}(h,(x,y))=(h(x)-y)^{2}\]</span></li>
</ul>
<p>广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions):</p>
<blockquote>
<p>对于集合<span class="math inline">\(Z\)</span>和损失函数<span class="math inline">\(l:\mathcal{H}×Z\to \mathbb{R}_{+}\)</span>, 若存在一个函数<span class="math inline">\(m_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<h3 id="对不可知的探讨">对不可知的探讨</h3>
<p>上文提到不可知PAC可学习是对PAC可学习的范化, 因为假设类<span class="math inline">\(\mathcal{H}\)</span>中未必有一个完美的分类器<span class="math inline">\(h\)</span>, 而这又有两种可能的深层原因:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span>取得不够大, 没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>取得够大了, 但<span class="math inline">\(\mathcal{H}\)</span>仍没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>, 一种合理的推测就是根本不存在能将西瓜完美分类的<span class="math inline">\(f\)</span>. 比方说因为<span class="math inline">\(\mathcal{X}\)</span>中特征选取不是太好, 某好瓜和某坏瓜在选取的特征上的数据完全一致, 此时<span class="math inline">\(f\)</span>当然没法把他们区分开来.</li>
</ul>
<p>前者相对好办, 只要扩大<span class="math inline">\(\mathcal{H}\)</span>, 而后者就需要去寻找或者构造新的特征以区分好坏瓜.</p>
<h2 id="vc维">VC维</h2>
<p>由上节PAC可学习我们已经知道, 有限假设类假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的充分条件. 但是否为必要条件呢? 答案是否定的. 事实上, 无限假设类也能是PAC可学习的, 只要它的VC维有限, 下面来介绍VC维.</p>
<p>我们先来看一个著名的定理: 没有免费的午餐定理(No-Free-Lunch), 这个定理说明了如果不对假设类加以限制, 任何学习算法总有很差表现的时候.</p>
<h3 id="没有免费的午餐定理">没有免费的午餐定理</h3>
<p>没有免费的午餐定理(No-Free-Lunch):</p>
<blockquote>
<p>对实例空间<span class="math inline">\(\mathcal{X}\)</span>上的0-1损失的二分任务, <span class="math inline">\(A\)</span>表示任意的学习算法. 样本大小<span class="math inline">\(m\)</span>为小于<span class="math inline">\(|\mathcal{X}|/2\)</span>的任意数.则在<span class="math inline">\(\mathcal{X}×\{0,1\}\)</span>上存在一个分布<span class="math inline">\(\mathcal{D}\)</span>, 使得:</p>
<ol type="1">
<li>存在一个函数<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(f)=0\)</span>.</li>
<li>样本量为<span class="math inline">\(m\)</span>的样本集<span class="math inline">\(S\)</span>, 由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 以至少<span class="math inline">\(\frac{1}{7}\)</span>的概率满足:<span class="math inline">\(L_{\mathcal{D}}(A(S))\ge \frac{1}{8}\)</span>.</li>
</ol>
</blockquote>
<p>这个定理陈述的是对每个学习器<span class="math inline">\(A\)</span>, 都存在一个学习任务(分布<span class="math inline">\(\mathcal{D}\)</span>)使其失败, 即使这个任务存能被另一个学习器成功学习(比方说关于假设类<span class="math inline">\(\mathcal{H}=\{f\}\)</span>的一个ERM学习器).</p>
<p>下面是没有免费的午餐定理的证明: 一个直观的看法如下, 令<span class="math inline">\(C\)</span>是大小为<span class="math inline">\(2m\)</span>的集合<span class="math inline">\(\mathcal{X}\)</span>的子集, 任何只观测到空间<span class="math inline">\(C\)</span>中一半样本的学习算法<span class="math inline">\(A\)</span>, 都不具有信息来反映<span class="math inline">\(C\)</span>中剩余样本. 因此存在一个可能性, 在<span class="math inline">\(C\)</span>中未观测到的样本上, 目标函数<span class="math inline">\(f\)</span>与<span class="math inline">\(A(S)\)</span>预测的完全不同(详细证明见深入理解机器学习一书5.1节):</p>
<ol type="1">
<li>从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>有<span class="math inline">\(T=2^{|\mathcal{X}|}\)</span>个函数,记为<span class="math inline">\(f_{1},...,f_{T}\)</span>. 对每个<span class="math inline">\(f_{i}\)</span>取分布<span class="math inline">\(\mathcal{D_{i}}\)</span>满足<span class="math inline">\(f_{i}\)</span>, 即如果<span class="math inline">\(y\neq f(x)\)</span>, 分布<span class="math inline">\(\mathcal{D_{i}}\)</span>抽取样本<span class="math inline">\(x×y\)</span>的概率置为0.</li>
<li>事实上, 此时<span class="math inline">\(|\mathcal{X}|\)</span>可直接取为<span class="math inline">\(2m\)</span>, 假定对<span class="math inline">\(|\mathcal{X}|=2m\)</span>定理得证, 那么此时任意加入新样本,只需将新样本的标签取成<span class="math inline">\(|1-A(S)|\)</span>, 就能保证<span class="math inline">\(L_{\mathcal{D}}(A(S))\)</span>仍大于<span class="math inline">\(\frac{1}{8}\)</span>, 而<span class="math inline">\(f\)</span>及<span class="math inline">\(\mathcal{D}\)</span>只需要做相应修改.</li>
<li>对任意学习算法<span class="math inline">\(A\)</span>从1.中的<span class="math inline">\(\{\mathcal{D}_{i}\}\)</span>找一个<span class="math inline">\(\mathcal{D}_{i}\)</span>使得<span class="math inline">\(E[L_{\mathcal{D_{i}}}(A(S))]\ge \frac{1}{4}\)</span>.</li>
<li><span class="math inline">\(P[L_{\mathcal{D}_{i}}(A(S))\ge\frac{1}{8}]\ge\frac{1}{7}\)</span></li>
</ol>
<p>注意到定理中并没有对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 假设类<span class="math inline">\(\mathcal{H}\)</span>的规模是随着样本量大小而指数型增长的, 从而导致学习失败. 于是乎如果我们要想学习成功, 就必须对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 那么如何去限制呢？</p>
<ul>
<li>我们已经知道任何有限假设类都是可学习的(Hoeffding测度集中不等式可证).</li>
<li>那么无限类呢?事实上, 有一个叫VC维的东西可以刻画出PAC可学习的假设类的增长速度如何, 直观的去看, VC维有限的假设类就是那些限制在集合<span class="math inline">\(C\)</span>上并且只能随着<span class="math inline">\(|C|\)</span>的增长而多项式增长而不是指数型增长的这类假设类, 这也被称为是小的有效规模.</li>
</ul>
<h3 id="打散">打散</h3>
<p>打散(Shattering):</p>
<blockquote>
<p>如果限制<span class="math inline">\(\mathcal{H}\)</span>在<span class="math inline">\(C\)</span>上是从<span class="math inline">\(C\)</span>到<span class="math inline">\(\{0,1\}\)</span>的所有函数的集合, 那么我们称<span class="math inline">\(\mathcal{H}\)</span>打散了集合<span class="math inline">\(C\)</span>, 此时<span class="math inline">\(|\mathcal{H}_{C}|=2^{|C|}\)</span>.</p>
</blockquote>
<p>这里<span class="math inline">\(\mathcal{H}\)</span>的状态也就是上文说的不加以限制的状态.</p>
<h3 id="vc维-1">VC维</h3>
<p>VC维(VC-dimension):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维, 记为<span class="math inline">\(VCdim(\mathcal{H})\)</span>可以打散的最大的集合的大小. 如果<span class="math inline">\(\mathcal{H}\)</span>可以打散任何集合大小,我们说<span class="math inline">\(\mathcal{H}\)</span>的VC维是无穷的.</p>
</blockquote>
<p>回头再来看没有免费午餐定理,我们易得下述推论:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{X}\)</span>为一个无限定义域集, <span class="math inline">\(\mathcal{H}\)</span>为从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>上的所有映射集, 即<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>,则<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的.</p>
</blockquote>
<h3 id="偏差与复杂性权衡与偏差方差权衡">偏差与复杂性权衡与偏差方差权衡</h3>
<p>在PAC可学习框架下, 我们所用的方式是: 对于给定的假设类<span class="math inline">\(\mathcal{H}\)</span>(先验知识), 采用ERM范式(最小化训练误差)从中选取分类器<span class="math inline">\(h\)</span>.</p>
<p>我们将ERM范式返回的假设<span class="math inline">\(h_{ERM}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})\)</span>分解为两部分, 第一部分叫偏差, 由假设类具有的最小风险<span class="math inline">\(\min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)\)</span>所决定, 它反映了先验知识的质量; 第二部分叫复杂性<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})-\min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)\)</span>, 是由过拟合引起的误差, 取决于假设类的大小或是复杂度, 又称为估计误差. 随着假设类变得越复杂, 偏差会变小, 但是复杂性会变大.</p>
<p>另一种更常见的分解方式是将ERM范式返回的假设<span class="math inline">\(h_{ERM}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})\)</span>分解为训练误差<span class="math inline">\(\min_{h\in\mathcal{H}}L_{S}(h)\)</span>和方差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})-\min_{h\in\mathcal{H}}L_{S}(h)\)</span>, 随着假设类变得越复杂, 训练误差会越来越小, 而泛化误差会有一个先增后减的过程. <img src="/pictures/vc2.svg" alt="机器学习3" /></p>
<h2 id="定理的证明">定理的证明</h2>
<p>定义说完了, 下面来看证明:</p>
<h3 id="推2">1推2</h3>
<p>一句话证明1推2:<span class="math inline">\(\mathcal{H}\)</span>一致收敛也就是说样本量足够时, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本, 现用<span class="math inline">\(\varepsilon/2\)</span>取代<span class="math inline">\(\varepsilon\)</span>, <span class="math inline">\(\varepsilon\)</span>-代表性样本的定义及ERM的返回规则得:对<span class="math inline">\(\forall h\in \mathcal{H}\)</span>,<span class="math display">\[L_{\mathcal{D}}(h_{S})\le L_{\mathcal{S}}(h_{S})+\varepsilon/2\le L_{\mathcal{S}}(h)+\varepsilon/2 \le L_{\mathcal{D}}(h)+\varepsilon/2+\varepsilon/2=L_{\mathcal{D}}(h)+\varepsilon\]</span>即由ERM(S)返回<span class="math inline">\(h_{S}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{S})\)</span>能保证学习成功.</p>
<h3 id="推3">2推3</h3>
<p>2推3就非常显然, 2是对于假设类<span class="math inline">\(\mathcal{H}\)</span>采用了ERM学习算法的学习器在<span class="math inline">\(\mathcal{H}\)</span>能成功, 要证明3只要证明存在一个学习算法能成功, 显然存在(ERM).</p>
<h3 id="推4">3推4</h3>
<p>3推4更显然, 3是4的泛化, 所以4的条件成立时3必然能用, 再加上<span class="math inline">\(f\)</span>满足可实现的假设, 即存在完美的分类器使得泛化误差为0, 得证.</p>
<h3 id="推5">2推5</h3>
<p>2推5同上可证.</p>
<h3 id="推6">4推6</h3>
<p>4推6用到了没有免费的午餐定理, 这个定理可以叙述为如果<span class="math inline">\(\mathcal{H}\)</span>的VC维无限, 那么<span class="math inline">\(\mathcal{H}\)</span>就不是PAC可学习的, 来看这个命题的逆否命题, 即如果<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 那么<span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</p>
<h3 id="推6-1">5推6</h3>
<p>5推6的话同样用到了免费的午餐定理, 假设<span class="math inline">\(\mathcal{H}\)</span>的VC维无穷, 那么<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的, 然后铁定不是采取了ERM之后的PAC可学习的了.</p>
<h3 id="推1">6推1</h3>
<p>这里只叙述证明思路:</p>
<ul>
<li>如果<span class="math inline">\(VCdim(\mathcal{H})=d&lt;+\infty\)</span>, 那么即使<span class="math inline">\(\mathcal{H}\)</span>无限, 当将其限制在一有限集合<span class="math inline">\(C\)</span>时, 其有效规模<span class="math inline">\(|\mathcal{H}_{C}|\)</span>只有<span class="math inline">\(O(|C|^{d})\)</span>.</li>
<li>假设类有一个小的有效规模时其一致收敛成立, 小的有效规模指的是<span class="math inline">\(|\mathcal{H}_{C}|\)</span>随<span class="math inline">\(|C|\)</span>按多项式方式增长.</li>
</ul>
<p>其中第一步实际上是将<span class="math inline">\(\mathcal{H}\)</span>的VC维有限时, <span class="math inline">\(\mathcal{H}\)</span>的增长速度为多项式增长这一事实明确下来, 证明用到了Sauer引理, 详细见深入理解机器学习一书6.5.1节; 2.的证明见深入理解机器学习一书6.5.2节.<span class="math inline">\(□\)</span></p>
<h1 id="统计学习的基本定理-定量形式">统计学习的基本定理-定量形式</h1>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to \{0,1\}\)</span>的映射函数构成的假设类, 且令损失函数为0-1损失. 假定<span class="math inline">\(VCdim(\mathcal{H})&lt;\infty\)</span>, 那么, 存在绝对常数<span class="math inline">\(C_{1},C_{2}\)</span>使得:</p>
</blockquote>
<blockquote>
<ol type="1">
<li><span class="math inline">\(\mathcal{H}\)</span>一致收敛, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{dlog(1/\varepsilon)+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
</ol>
</blockquote>
<p><code>p.s.</code>由上述定理可以看出, 一致收敛与不可知PAC可学习的需要的样本量一致, 而PAC可学习的需要的样本量少一些. 该定理的证明在深入理解机器学习一书第28章给出.</p>
<h1 id="不一致可学习">不一致可学习</h1>
<p>上文所讨论的PAC可学习的概念是考虑精度和置信参数来决定样本数量, 且样本标签分布与内在的样本数据分布是一致的. 因此, PAC可学习等价于VC维有限. 下面要讨论的是不一致可学习这个概念, 这个概念是不可知PAC可学习的严格松弛, 它允许样本数量依赖假设空间<span class="math inline">\(\mathcal{H}\)</span>, 下面来看其定义:</p>
<p>不一致可学习(Nonuniform learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的若存在一个学习算法<span class="math inline">\(A\)</span>和一个函数<span class="math display">\[m^{NUL}_{\mathcal{H}}:(0,1)\times(0,1)×\mathcal{H}\to\mathbb{N}\]</span>使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和<span class="math inline">\(h\in\mathcal{H}\)</span>, 如果样本数量<span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\)</span>, 那么对每个分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>, <span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>.</p>
</blockquote>
<p>这个概念要求输出假设与假设类中其他假设相比具有<span class="math inline">\((\varepsilon,\delta)\)</span>的竞争力, 同时从定义也很容易得到不可知PAC可学习一定能推得不一致可学习, 且不一致可学习是不可知PAC可学习的严格松弛. 下面举个例子说明两者的严格松弛关系:</p>
<ul>
<li>考虑二分问题, 假设<span class="math inline">\(\mathcal{H}_{n}\)</span>是<span class="math inline">\(n\)</span>次多项式构成的假设类, 即<span class="math inline">\(\mathcal{H}_{n}\)</span>为<span class="math inline">\(h(x)=sign(p(x))\)</span>的分类器的集合.这里<span class="math inline">\(p(x)\)</span>是<span class="math inline">\(\mathbb{R}\to\mathbb{R}\)</span>的<span class="math inline">\(n\)</span>次多项式.令<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 容易得到<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>, 但<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</li>
</ul>
<p>事实上, 有一个定理清晰的描述了两者的关系:</p>
<blockquote>
<p>两分类器的假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 当且仅当它是不可知PAC可学习的可数并.</p>
</blockquote>
<p>下面来看这个定理的证明:</p>
<ul>
<li><p>必要性: 假定<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 对于每个<span class="math inline">\(n\in\mathbb{N}\)</span>,令<span class="math display">\[\mathcal{H}_{n}=\{h\in\mathcal{H}:m^{NUL}_{\mathcal{H}}(\frac{1}{8},\frac{1}{7},h)\le n\}\]</span>显然<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 此外, 由<span class="math inline">\(m^{NUL}_{\mathcal{H}}\)</span>定义知, 对于任何关于<span class="math inline">\(\mathcal{H}_{n}\)</span>满足可实现性假设的分布<span class="math inline">\(\mathcal{D}\)</span>, <span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 选择样本<span class="math inline">\(S\)</span>的概率大于等于<span class="math inline">\(\frac{6}{7}\)</span>. 则<span class="math inline">\(L_{\mathcal{D}}(A(S))\le\frac{1}{8}\)</span>. 由统计学习基本定理知, <span class="math inline">\(VCdim(\mathcal{H}_{n})&lt;+\infty\)</span>, 因此<span class="math inline">\(\mathcal{H}_{n}\)</span>是不可知PAC可学习的.</p></li>
<li><p>充分性: 充分性的证明依赖于下述引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote></li>
</ul>
<p>该引理证明留给下小节, 那么由统计学习基本定理知, 不可知PAC可学习与一致收敛性等价,即得证<span class="math inline">\(□\)</span></p>
<h2 id="结构风险最小化srm">结构风险最小化SRM</h2>
<p>本节主要是关于结构风险最小化(SRM:Structural Risk Minimization)以及利用结构风险最小化算法来证明上节未证明的引理.</p>
<p>目前为止, 我们都是通过具体化一个假设类<span class="math inline">\(\mathcal{H}\)</span>来利用先验知识, 并且相信这样一个假设类中包含完成当前任务的有效预测器. 而另一种表达先验知识的方法则是将假设类<span class="math inline">\(\mathcal{H}\)</span>上的偏好具体化.</p>
<p>在结构风险最小化范式中, 我们首先假定假设类<span class="math inline">\(\mathcal{H}\)</span>能够写成<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛, 且样本复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\)</span>, 然后具体化一个权重函数<span class="math display">\[\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\]</span>这个权重函数可也反映每个假设类的重要性, 或是假设类复杂性的度量.例如<span class="math inline">\(\mathcal{H}\)</span>是所有多项式分类器构成的类, <span class="math inline">\(\mathcal{H}_{n}\)</span>表示<span class="math inline">\(n\)</span>次多项式分类器构成的类.定义下式函数<span class="math inline">\(\varepsilon_{n}\)</span>:<span class="math inline">\(\mathbb{N}×(0,1)\to(0,1)\)</span> <span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\]</span>由<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛性, <span class="math inline">\(\forall m\in\mathbb{N}\)</span>,<span class="math inline">\(\forall\delta\in(0,1)\)</span>, 大小为<span class="math inline">\(m\)</span>的样本<span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span> <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta)\]</span>这就是我们所关心的训练误差与泛化误差之间的差值. 此时权重函数可取成<span class="math inline">\(\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>或是<span class="math inline">\(\omega(n)=2^{-n}\)</span>.</p>
<p>下面将上述例子写成定理的形式,并由此引出结构风险最小化(SRM):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>. 令<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>为一权重函数. 令<span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\]</span> 则对<span class="math inline">\(\forall\delta\in(0,1)\)</span>,任意的分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>其中<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>.</p>
</blockquote>
<p>证明: 对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>, 定义<span class="math inline">\(\delta_{n}=\omega(n)\delta\)</span>, 固定<span class="math inline">\(n\)</span>, 由一致收敛性, 在选择样本<span class="math inline">\(S\)</span>的概率不低于<span class="math inline">\(1-\delta\)</span>的条件下, <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta_{n})\]</span>应用<span class="math inline">\(n=1,2,...,\)</span>的联合界, 我们得到上述结论以不低于<span class="math display">\[1-\sum_{n}\delta_{n}=1-\delta\sum_{n}\omega(n)\ge1-\delta\]</span>的概率对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>都成立, 最后令<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>自然也成立.<span class="math inline">\(□\)</span></p>
<p>结构风险最小化(SRM)则是寻找<span class="math inline">\(h\)</span>来最小化这个上界, 以保证<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>不是那么大, 下面是结构风险最小化(SRM)的定义:</p>
<ol type="1">
<li>先验:<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>; 权重函数<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>.</li>
<li>定义:<span class="math inline">\(\varepsilon_{n}(m,\delta)=min\{\varepsilon\in(0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\)</span>; <span class="math inline">\(n(h)=min\{n:h\in\mathcal{H}_{n}\}\)</span>.</li>
<li>输入:训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>.</li>
<li>输出:<span class="math inline">\(h\in argmin_{h\in\mathcal{H}}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\)</span>.</li>
</ol>
<p><code>p.s.</code>结构风险最小化(SRM)与经验风险最小化(ERM)最大的不同就是, 经验风险最小化(ERM)只关心训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>, 而结构风险最小化(SRM)兼顾了训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>和训练误差与泛化误差之间的差值<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\)</span>.</p>
<p>下面用SRM来证明上节未能证明的一个引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote>
<p>事实上利用SRM算法我们能得到下述更进一步的结论:</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>.如果<span class="math inline">\(\omega\)</span>:<span class="math inline">\(\mathbb{N}\to[0,1]\)</span>, <span class="math inline">\(s.t.\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 且满足有<span class="math display">\[m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\le m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\frac{6\delta}{(\pi n(h))^{2}})\]</span></p>
</blockquote>
<p>证明: 假定<span class="math inline">\(A\)</span>是考虑权重函数<span class="math inline">\(\omega\)</span>的结构风险最小化算法, 对于<span class="math inline">\(\forall h\in\mathcal{H}_{n},\forall\varepsilon,\delta\in(0,1)\)</span>, 令<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\varepsilon,\omega(n(h))·\delta)\)</span>,应用上述定理, 可以得到<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>这个定理对于由结构风险规则返回的假设<span class="math inline">\(A(S)\)</span>成立. 通过结构风险最小化的定义可得,<span class="math display">\[L_{\mathcal{D}}(A(S))\le\min_{h}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>如果<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\omega(n(h))·\delta)\)</span>, 那么<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\le\frac{\varepsilon}{2}\)</span>. 此外, 从每个<span class="math inline">\(\mathcal{H}_{n}\)</span>的一致收敛性, 我们可得到下式成立的概率大于<span class="math inline">\(1-\delta\)</span>,<span class="math display">\[L_{S}(h)\le L_{\mathcal{D}}(h)+\frac{\varepsilon}{2}\]</span>综上所述, 可得<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>, 定理得证.<span class="math inline">\(□\)</span></p>
<p>单独比较不一致可学习和不可知PAC可学习是非常有意思的, 算法会在全空间<span class="math inline">\(\mathcal{H}\)</span>上搜索一个模型, 而不是在特定的<span class="math inline">\(\mathcal{H}_{n}\)</span>上搜索一个模型, 利用先验知识的缺陷所带来的成本就是增加复杂度与特定的<span class="math inline">\(h\in\mathcal{H}_{n}\)</span>相竞争.假定对于所有的<span class="math inline">\(n\)</span>, <span class="math inline">\(VCdim(\mathcal{H}_{n})=n\)</span>, 由统计学习的基本定理-定量形式知<span class="math display">\[m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le C\frac{n+log(1/\delta)}{\varepsilon^{2}}\]</span> 一个直接的计算表明<span class="math display">\[m^{NUL}_{\mathcal{H}_{n}}(\varepsilon,\delta,h)-m^{UC}_{\mathcal{H}_{n}}(\frac{\varepsilon}{2},\delta)\le 4C\frac{2\log(2n)}{\varepsilon^{2}}\]</span>代价增加了类的索引, 可以解释为反映已知的假设类<span class="math inline">\(\mathcal{H}\)</span>的好的先验知识的排序值.</p>
<h3 id="不一致可学习的没有免费午餐定理">不一致可学习的没有免费午餐定理</h3>
<p>在不一致可学习框架下,没有免费的午餐定理(No-Free-Lunch for Nonuniform Learnability)也是成立的,也就是说, 当样本域无限时, 不存在关于所有确定性二分类器所构成的类的不一致学习器(尽管对于每一个分类器存在一个尝试算法能够学习包含这些分类器假设的结构风险最小化).</p>
<h2 id="最小描述长度mdl">最小描述长度MDL</h2>
<p><span class="math inline">\(\mathcal{H}\)</span>为假设类, 将<span class="math inline">\(\mathcal{H}\)</span>写成单个类的可数并, 即<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\{h_{n}\}\)</span>.</p>
<p>由Hoeffding不等式知, 每一个单类有一致收敛性, 收敛速率<span class="math inline">\(m^{UC}(\varepsilon,\delta)=\frac{\log(2/ \delta)}{2\varepsilon^{2}}\)</span>, 该结果的证明见深入理解机器学习一书page26. 因此, <span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}(\varepsilon,\delta)\le m\}\]</span>所给出的函数<span class="math inline">\(\varepsilon_{n}\)</span>变成了<span class="math display">\[\varepsilon_{n}(m,\delta)=\sqrt{\frac{\log(2/\delta)}{2m}}\]</span>且结构风险最小化SRM变成了:<span class="math display">\[argmin_{h_{n}\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(n))+\log(2/\delta)}{2m}}\]</span>等价的, 我们可以认为<span class="math inline">\(\omega\)</span>是从<span class="math inline">\(\mathcal{H}\to[0,1]\)</span>的函数, 然后结构结构风险最小化SRM变成了:<span class="math display">\[argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(h))+\log(2/\delta)}{2m}}\]</span>这节讨论一种特别方便的方式定义权重函数<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>, 这个函数与假设的描述长度有关. 下面介绍其背景:</p>
<p>对于每个假设类<span class="math inline">\(\mathcal{H}\)</span>, 我们想知道如何描述和表示每一个类的假设<span class="math inline">\(h\)</span>, 英语, 汇编语言, 数学公式等等形式, 当我们选定某种语言形式, 那么一个假设<span class="math inline">\(h\)</span>肯定能被一些特定字母<span class="math inline">\(\Sigma\)</span>组成的有限字符串所描述, <span class="math inline">\(\Sigma\)</span>我们称之为字母表. 例如, 令<span class="math inline">\(\Sigma=\{0,1\}\)</span>, <span class="math inline">\(\sigma=(0,1,1,1,0)\)</span>为一字符串且字符串的长度<span class="math inline">\(|\sigma|=5\)</span>, 所有有限长度的字符串用<span class="math inline">\(\Sigma^{+}\)</span>表示, 对假设类<span class="math inline">\(\mathcal{H}\)</span>的描述可用一个函数<span class="math inline">\(d:\mathcal{H}\to\Sigma^{+}\)</span>表示, <span class="math inline">\(d(h)\)</span>将<span class="math inline">\(\mathcal{H}\)</span>中每个假设<span class="math inline">\(h\)</span>映射为一个字符串<span class="math inline">\(d(h)\)</span>, <span class="math inline">\(d(h)\)</span>称为<span class="math inline">\(h\)</span>的描述长度, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度. 我们要求描述语言<span class="math inline">\(d(h)\)</span>无前缀, 即不同的<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(d(h)\)</span>不是<span class="math inline">\(d(h&#39;)\)</span>的前缀. 无前缀的字符串满足如下组合性质:</p>
<p>Kraft不等式:</p>
<blockquote>
<p>如果<span class="math inline">\(S\subset\{0,1\}^{+}\)</span>是一个无前缀的字符串集合,则<span class="math inline">\(\sum_{\sigma\in S}\frac{1}{2^{|\sigma|}}\le1\)</span>.</p>
</blockquote>
<p>根据Kraft不等式, 任何无前缀的语言都能给出假设类<span class="math inline">\(\mathcal{H}\)</span>的权重函数<span class="math inline">\(\omega\)</span>, 我们可简单的设置为<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>,于是便有了下述定理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一假设类, <span class="math inline">\(d:\mathcal{H}\to\{0,1\}^{+}\)</span>是<span class="math inline">\(\mathcal{H}\)</span>的一无前缀描述语言,对于样本量<span class="math inline">\(m\)</span>, 置信参数<span class="math inline">\(\delta&gt;0\)</span>和概率分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率大于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H},L_{\mathcal{D}}(h)\le L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\]</span>这里<span class="math inline">\(|h|\)</span>是指<span class="math inline">\(d(h)\)</span>的长度.</p>
</blockquote>
<p>类似结构风险最小化(SRM), 最小描述长度(MDL)这种考虑了训练误差和减小描述长度, 这就得到了减小描述长度的学习范式. 最小描述长度(MDL:Minimum Description Length):</p>
<ol type="1">
<li>先验: 假设类<span class="math inline">\(\mathcal{H}\)</span>由定义在<span class="math inline">\(\{0,1\}\)</span>上的无前缀语言<span class="math inline">\(d\)</span>描述;对于任何一个<span class="math inline">\(h\in\mathcal{H}\)</span>, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度.</li>
<li>输入: 训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>.</li>
<li>输出: <span class="math inline">\(h\in argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\)</span></li>
</ol>
<h3 id="奥卡姆剃刀原理">奥卡姆剃刀原理</h3>
<p>上届的定理指出, 对于经验风险相同的两个假设, 描述长度较小的假设, 其真实风险的误差界更小, 因此这个结果表达了一个哲学理念, 这也是著名的奥卡姆剃刀原理(Occam’s Razor):</p>
<blockquote>
<p>短的解析更有效.</p>
</blockquote>
<p><code>p.s.</code>假象下列情形, 两个假设<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>在数学语言下的描述长度分别为10,100; 但在中文下的长度分别为100,10. 因此这里笔者理解为对大多数语言的来说短的解析, 或是期望意义下的最短的解析.</p>
<h1 id="一致性">一致性</h1>
<p>最后给出一个比不一致可学习更松弛的可学习概念: 一致性, 这个概念允许所欲要的样本不仅依赖于<span class="math inline">\(\varepsilon,\delta\)</span>和<span class="math inline">\(h\)</span>,而且依赖于数据所依托的分布<span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>一致性(Consistency):</p>
<blockquote>
<p><span class="math inline">\(\mathcal{P}\)</span>表示<span class="math inline">\(Z\)</span>上的概率分布, 称一个学习规则<span class="math inline">\(A\)</span>关于<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{P}\)</span>满足一致性, 若存在一个函数<span class="math display">\[m^{CON}_{\mathcal{H}}:(0,1)^{2}×\mathcal{H}×\mathcal{P}\to\mathbb{N}\]</span>使得对<span class="math inline">\(\forall h\in\mathcal{H}\)</span>,<span class="math inline">\(\forall\mathcal{D}\in\mathcal{P}\)</span>,<span class="math inline">\(\forall\varepsilon\)</span>,<span class="math inline">\(\delta\in(0,1)\)</span>, <span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h,\mathcal{D})\)</span>,下式成立的概率不小于1-δ<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>.</p>
</blockquote>
<h2 id="memorize算法">Memorize算法</h2>
<p>考虑如下定义的分类预测算法Memorize, 这个算法对新样本的预测永远都是某个固定的标签,比如说是训练集中出现次数最多的标签. 事实上, Memorize算法对于任何数据集上所有函数构成的类都满足一致性, 但这个算法很明显没有什么用.</p>
<p><code>写在最后</code>: 本文主要是写了写PAC可学习框架下有关VC维的内容, 这个VC维(Vapnik–Chervonenkis dimension)其实代表两位大牛:svm发明者<a href="https://en.wikipedia.org/wiki/Vladimir_Vapnik">Vapnik</a>和俄罗斯的数学家<a href="https://en.wikipedia.org/wiki/Alexey_Chervonenkis">Chervonenkis</a>. 进一步内容的Rademacher复杂度则是我下一篇博客的内容, 西瓜书将其放在12.5节, 深入理解机器学习一书则是将其放在第四部分高级理论26-27章, 针对多分类问题的Natarajan维在29章.</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PAC可学习</tag>
        <tag>一致收敛性</tag>
        <tag>没有免费午餐定理</tag>
        <tag>VC维</tag>
        <tag>不一致可学习</tag>
        <tag>一致性</tag>
        <tag>ERM</tag>
        <tag>SRM</tag>
        <tag>MDL</tag>
        <tag>奥卡姆剃刀</tag>
      </tags>
  </entry>
  <entry>
    <title>随机梯度法中的噪声减小方法</title>
    <url>/2019/10/28/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%AD%E7%9A%84%E5%99%AA%E5%A3%B0%E5%87%8F%E5%B0%8F%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>前言: 本文从优化的角度整理了随机梯度法中的常用的一类算法: 噪声缩减方法, 这里的噪声指的是对随机样本计算梯度带来的噪声. 主要参考了Léon Bottou的综述<a href="https://arxiv.org/pdf/1606.04838.pdf">Optimization Methods for Large-Scale Machine Learning</a>、Jorge Nocedal的Numerical Optimization(2nd edition)以及<a href="http://caoxiaoqing.github.io/2018/05/11/SVRG%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">这篇博客</a>. <span id="more"></span></p>
<h2 id="机器学习中的优化">机器学习中的优化</h2>
<h3 id="一些记号">一些记号</h3>
<p>首先给出两个基本记号, 这是机器学习中要考虑的两种优化基本函数, 前者是定义在<span class="math inline">\(n\)</span>个样本上的函数, 而后者是定义在样本的概率空间上的函数.</p>
<ul>
<li><span class="math inline">\(R_n(w)=\frac{1}{n}\sum_{i=1}^nf_i(w)\)</span>表示经验风险.</li>
<li><span class="math inline">\(R(w)=\mathbb{E}[f(w;\xi)]\)</span>表示期望风险.</li>
</ul>
<p>而关于最优解<span class="math inline">\(w\)</span>有三种:</p>
<ul>
<li>第一种是<span class="math inline">\(w_{*}\in \arg\min R(w)\)</span>.</li>
<li>第二种是<span class="math inline">\(w_{n}\in \arg\min R_n(w)\)</span>.</li>
<li>第三种是实际运行算法返回的最优解<span class="math inline">\(\tilde{w}_n\)</span>, 其受制于具体优化算法的计算.</li>
</ul>
<p><strong>注意</strong>: 下面用<span class="math inline">\(F(w)\)</span>表示<span class="math inline">\(R_n(w)\)</span>或者<span class="math inline">\(R(w)\)</span>, 意在区分只适用<span class="math inline">\(R_n(w)\)</span>和<span class="math inline">\(R_n(w), R(w)\)</span>均适用的算法</p>
<h3 id="梯度下降法">梯度下降法</h3>
<p>梯度下降法(gradient descent, GD), 只适用于<span class="math inline">\(R_n(w)\)</span>, 在强凸前提下其收敛速度为线性[Numerical Optimization TH3.4]:<span class="math display">\[R_n(w_k)-R_*\le O(\rho^k)\]</span>其中<span class="math inline">\(\rho\in(0,1)\)</span>. 因此GD的计算复杂度为:<span class="math display">\[O(n\kappa\log(1/\epsilon))\]</span>因为<span class="math inline">\(O(\rho^k)=\epsilon \Rightarrow k=O(\log(1/\epsilon))\)</span>且每次迭代要计算<span class="math inline">\(n\)</span>个样本的梯度.</p>
<p><strong>注意</strong>: <span class="math inline">\(\kappa=L/\mu\)</span>为优化中常用的常数, 类比于二次情形的条件数. 可参见[leon bottou]中的Assumption4.1 4.3</p>
<h3 id="随机梯度">随机梯度</h3>
<p>随机梯度(stochastic gradient, SG), 为梯度下降法的随机版本, 但因为每次迭代不能保证下降, 因此没有descent的字眼, 它是适用于<span class="math inline">\(F(w)\)</span>的算法. 在强凸(步长满足<span class="math inline">\(\alpha_k=\frac{\beta}{\gamma+k}\)</span>)及期望的意义下其收敛速率为次线性[leon bottou TH4.7] :<span class="math display">\[\mathbb{E}[F(w_k)-F_*]\le \frac{\nu}{\gamma+k}\]</span>其中<span class="math inline">\(\beta, \nu\)</span>为常数. 因此SG的计算复杂度为:<span class="math display">\[O(\kappa/\epsilon)\]</span>因为<span class="math inline">\(\frac{\nu}{\gamma+k}=\epsilon \Rightarrow k=\frac{\nu}{\epsilon}-\gamma\)</span></p>
<h2 id="噪声减小方法">噪声减小方法</h2>
<h3 id="迭代平均方法">迭代平均方法</h3>
<p>先来看最容易想到的一类方法, 迭代平均方法, 它直接基于SG因此适用<span class="math inline">\(F(w)\)</span>. 考虑SG迭代序列<span class="math display">\[w_{k+1}\leftarrow w_k-\alpha_k g(w_k,\xi_k)\]</span>这里的<span class="math inline">\(\xi_k\)</span>为随机变量, 指代第<span class="math inline">\(k\)</span>步随机抽取的样本, <span class="math inline">\(g(w_k,\xi_k)\)</span>表示样本<span class="math inline">\(\xi_k\)</span>对应的梯度. 迭代平均法新建了一个序列:<span class="math display">\[\tilde{w}_{k+1}\leftarrow \frac{1}{k+1}\sum_{j=1}^{k+1}w_{j}\]</span>若步长衰减速度为<span class="math inline">\(O(1/k^a)\)</span>, <span class="math inline">\(a\in(\frac{1}{2},1)\)</span>. 在强凸的前提下有:<span class="math display">\[\mathbb{E}[\Vert w_k-w_+\Vert_2^2] = O(1/k^a)\]</span>且(注:这里的<span class="math inline">\(w_+\)</span>指<span class="math inline">\(w\)</span>的最优值点)<span class="math display">\[\mathbb{E}[\Vert \tilde{w}_k-w_+\Vert_2^2] = O(1/k)\]</span></p>
<h3 id="动态样本大小方法">动态样本大小方法</h3>
<p>动态样本大小方法同样直接基于SG因此适用<span class="math inline">\(F(w)\)</span>. 下面介绍一个核心的结果, 简言之就是, 若能把每步迭代的方向的方差控制住:<span class="math display">\[\mathbb{V}_{\xi_k}[g(w_k, \xi_k)]\le M \zeta^{k-1}\]</span>在强凸(步长固定)的前提下, 可得到线性收敛[leon bottou TH5.1]:<span class="math display">\[\mathbb{E}[F(w_k)-F_*]\le \omega\rho^{k-1}\]</span>其中<span class="math inline">\(\rho&lt;1, \omega\)</span>为常数. 再结合统计中的一个结果: <span class="math inline">\(n_k\)</span>个独立同分布样本的方差为单个样本的方差的<span class="math inline">\(1/n_k\)</span>. 因此只需要动态改变每次迭代所用样本个数, 使其控制在定理条件下<span class="math display">\[\mathbb{V}_{\xi_k}[g(w_k, \xi_k)]\le\frac{C}{n_k}\le M\zeta^{k-1}\Rightarrow n_k\ge\frac{C}{M}(\frac{1}{\zeta})^{k-1}\]</span>(这里假定所有样本的梯度的方差有一致的上界, 记为<span class="math inline">\(C\)</span>) 就能得到线性收敛的结果. 然而虽有线性收敛的速度, 但每次迭代所需样本也是指数级增长, 因此动态样本大小方法的计算复杂度为[leon bottou TH5.3]:<span class="math display">\[O(1/\epsilon)\]</span>由于样本集是有限的, 在某一步中必然会出现样本不够用的情况, 一种可行的做法是转而要求每步迭代的方向产生了足够多的下降, 而不管上述样本量指数级增长的要求.</p>
<h3 id="梯度聚合">梯度聚合</h3>
<p>这类方法着重于利用先前迭代中计算得到的样本的梯度信息, 其包括SVGR、SAG/SAGA等.</p>
<h4 id="svrg">SVRG</h4>
<p><img src="/pictures/SVRG.png" /></p>
<p>SVGA只适用于<span class="math inline">\(R_n(w)\)</span>. 如上图算法所示, 相比SG, SVGA多了抽取<span class="math inline">\(m\)</span>个样本的内层循环, 再用内层循环产生的子序列<span class="math inline">\(\{\tilde{w}_j\}\)</span>,<span class="math inline">\({j=1,...,m}\)</span>去生成高质量的<span class="math inline">\(w_{k+1}\)</span>, 方式比如说(a),(b),(c). 核心式子为:<span class="math display">\[\tilde{g}_j\leftarrow \nabla f_{i_{j}}(\tilde{w}_j)-(\nabla f_{i_{j}}(w_k)-\nabla R_n(w_k))\]</span>它可以从两个角度去理解:</p>
<ol type="1">
<li><span class="math inline">\((\nabla f_{i_{j}}(w_k)-\nabla R_n(w_k))\)</span>可视为从单个样本<span class="math inline">\(i_j\)</span>到<span class="math inline">\(n\)</span>个样本的修正, 因此<span class="math inline">\(\tilde{g}_j\)</span>可近似视为<span class="math inline">\(\nabla R_n(\tilde{w}_j)\)</span>.</li>
<li>若<span class="math inline">\(m\)</span>取的不是特别大, <span class="math inline">\(\nabla f_{i_{j}}(\tilde{w}_j)\)</span>显然与<span class="math inline">\(\nabla f_{i_{j}}(w_k)\)</span>强相关, 而<span class="math inline">\(\nabla f_{i_{j}}(w_k)\)</span>的期望恰为<span class="math inline">\(\nabla R_n(w_k)\)</span>, 此时上述形如<span class="math display">\[\tilde{g}_j\leftarrow X-(Y-\mathbb{E}[Y])\]</span>此时<span class="math inline">\(\mathbb{E}[\tilde{g}_j]=\mathbb{E}[X]\)</span>, <span class="math inline">\(\mathbb{V}[\tilde{g}_j]=\mathbb{V}[X]+\mathbb{V}[Y]-2Cov(X, Y)\)</span>. 再加上强相关的假设, 确实能取到Variance Reduced(VR)的效果, <a href="https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf">SVRG原论文</a>在Experiments中说明了这一点.</li>
</ol>
<p>在强凸前提下其收敛速度为线性[SVRG TH1]<span class="math display">\[\mathbb{E}[R_n(w_{k})-R_*]\le\omega\rho^{k-1}\]</span>其中<span class="math inline">\(\rho\in(0,1)\)</span>. SVRG的计算复杂度为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span></p>
<h4 id="sagasag">SAGA/SAG</h4>
<p><img src="/pictures/SAGA.png" /></p>
<p>SAGA只适用于<span class="math inline">\(R_n(w)\)</span>, 显然算法部分第一块为初始化所有样本梯度值并存储, 第二个循环才是算法循环. 其中<span class="math inline">\(\nabla f_j(w_{[j]})\)</span>表示第<span class="math inline">\(j\)</span>个样本最近一次存储着的梯度值. 比如说对于第<span class="math inline">\(1\)</span>个样本, 初始化时<span class="math inline">\(\nabla f_1(w_{[1]})\leftarrow \nabla f_1(w_{1})\)</span>, 剩下涉第<span class="math inline">\(1\)</span>个样本的梯度的更新就只有靠算法循环中的<code>Choose j uniformly in &#123;1,...,n&#125;</code>抽到再用<span class="math inline">\(\nabla f_1(w_k)\)</span>更新. 因此可以看出, SAGA本着<strong>能不计算梯度就不计算</strong>的原则(每步迭代只计算一个样本的梯度, 从这个角度看更接近SG), 而其核心式子为:<span class="math display">\[g_k\leftarrow \nabla f_{j}(w_k)-\nabla f_{j}(w_{[j]})+\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\]</span>它同样可以从两个角度去理解:</p>
<ol type="1">
<li><span class="math inline">\(\nabla f_{j}(w_{[j]})-\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\)</span>视为单样本到<span class="math inline">\(n\)</span>个样本的修正, <span class="math inline">\(g_k\)</span>可近似视为<span class="math inline">\(\nabla R_n(w_k)\)</span>.</li>
<li><span class="math inline">\(\nabla f_{j}(w_{[j]})\)</span>的期望为<span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\nabla f_i(w_{[i]})\)</span>, 但<span class="math inline">\(\nabla f_{j}(w_k)\)</span>与<span class="math inline">\(\nabla f_{j}(w_{[j]})\)</span>未必强相关.</li>
</ol>
<p>SAGA起源自SAG, 它们唯一的不同在于核心式子, SAG的核心式子如下:<span class="math display">\[g_k\leftarrow \frac{1}{n}(\nabla f_{j}(w_k)-\nabla f_{j}(w_{[j]})+\sum_{i=1}^n\nabla f_i(w_{[i]}))\]</span>与SAGA不同的是, 此时<span class="math inline">\(g_k\)</span>不是<span class="math inline">\(\nabla R_n(w_k)\)</span>的无偏估计.在强凸前提下, SAGA与SAG均能线性收敛, 且计算复杂度与SVRG一样均为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span></p>
<h3 id="其它">其它</h3>
<h4 id="sarah">SARAH</h4>
<p><img src="/pictures/SARAH.png" width = "488" height = "459" div align=center /></p>
<p><a href="https://arxiv.org/pdf/1703.00102.pdf">SARAH</a>只适用于<span class="math inline">\(R_n(w)\)</span>, 它与SVGR一样有内圈循环, 不同的是核心迭代式子中把<span class="math inline">\(\nabla R_n(w_k)\)</span>替换成了上一步的迭代方向. 且此时无偏性不再保持. 在强凸前提下, SARAH线性收敛 且计算复杂度与SVRG一样均为:<span class="math display">\[O((n+\kappa)\log(1/\epsilon))\]</span>注意到SARAH与SVRG中的<span class="math inline">\(m\)</span>均为超参, SARAH原论文中还提出了以<span class="math inline">\(\Vert v_{t-1}\Vert_2^2\le\gamma \Vert v_0\Vert_2^2\)</span>为内圈循环停止条件的实用版算法SARAH+. <img src="/pictures/SARAH+.png" width = "450" height = "421" div align=center /></p>
]]></content>
      <categories>
        <category>优化算法</category>
      </categories>
      <tags>
        <tag>SG</tag>
      </tags>
  </entry>
  <entry>
    <title>高效能人士的七个习惯-读书笔记</title>
    <url>/2018/10/23/%E9%AB%98%E6%95%88%E8%83%BD%E4%BA%BA%E5%A3%AB%E7%9A%84%E4%B8%83%E4%B8%AA%E4%B9%A0%E6%83%AF-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>前言: 大部分人在聆听时并不是想理解对方, 而是为了做出回应------习惯五 <span id="more"></span></p>
<p>本文是高效能人士的七个习惯一书的读书笔记, 对于书上提出的问题,</p>
<blockquote>
<p>部分的文字表示笔者个人的答案</p>
</blockquote>
<hr />
<h2 id="七个习惯">七个习惯</h2>
<p>习惯一: 积极主动(BE PROACTIVE) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">积极主动即采取主动, 为自己过去、现在及未来的行为负责, 并依据原则及价值观, 而非情绪或外在环境来下决定.</span><br></pre></td></tr></table></figure></p>
<p>习惯二: 以终为始(BEGIN WITH THE END IN MIND) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">所有的事物都经过两次创作: 先是在脑海里酝酿, 其次才是实质的创造.</span><br></pre></td></tr></table></figure></p>
<p>习惯三: 要事第一(PUT FIRST THINGS FIRST) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">要事第一即实质的创造, 是梦想(目标, 愿景, 价值观及要事处理顺序)的组织与实践.</span><br></pre></td></tr></table></figure></p>
<p>习惯四: 双赢思维(THINK WIN-WIN) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">双赢思维是一种基于互敬、寻求互惠的思考框架与心意, 目的是分享更多的机会、财富及资源.</span><br></pre></td></tr></table></figure></p>
<p>习惯五: 知彼解己(SEEK FIRST TO UNDERSTAND, THEN TO BE UNDERSTAND) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">当我们不再急切回答, 改以诚心去了解、聆听别人, 便能开启真正的沟通, 增进彼此关系.</span><br></pre></td></tr></table></figure></p>
<p>习惯六: 统合综效(SYNERGIZE) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">统合综效谈的是创造的第三种选择, 即非按照我的方式, 亦非遵循你的方式, 二是创造第三种更好的办法.</span><br></pre></td></tr></table></figure></p>
<p>习惯七: 不断更新(SHARPEN THE SAW) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">不断更新谈的是如何在四个生活的基本面(身体、精神、智力、社会\情感)中不断更新自己, 这个习惯提升了其他六个习惯的实施效率.</span><br></pre></td></tr></table></figure></p>
<hr />
<h2 id="思维定式">思维定式</h2>
<p>品德成功论提醒我们, 高效能的生活是有基本原则的.</p>
<h3 id="品德成功论与论个人魅力">品德成功论与论个人魅力</h3>
<p>自古以来, 品德(Character Ethic), 包括诚信、谦虚、忠诚、节欲、勇气、公正、耐心、勤勉、朴素等, 被视为成功之本; 一战后, 人们对成功的基本观念发生了改变, 个人魅力(Personality Ethic)逐渐取代品德, 认为成功与否更多取决于性格、社会形象、行为态度、人际关系, 基于这种想法主要有两大发展方向, 一是注重人际关系, 二是鼓吹盲目积极乐观.</p>
<p>若将成功具象化为某地的某物, 我们要做的便是根据地图, 通过努力取得成功. 这里的地图指的便是思维定式(Paradigm), 其最初用于指代哲学或科学的理论框架, 现多指我们看待世界的方式, 上述的两种成功论便是两种思维定式.</p>
<h3 id="以原则为中心的思维定式">以原则为中心的思维定式</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">演戏战舰在阴沉天气中航行数日, 浓重的雾气使得能见度极低.</span><br><span class="line">入夜后不久, 船桥一侧的瞭望员突然报告:&quot;右船舷位置有灯光.&quot;</span><br><span class="line">船长命令信号兵通知对方:&quot;我们正迎面驶来, 建议转向20度.&quot;</span><br><span class="line">对方说:&quot;建议你转向20度.&quot;</span><br><span class="line">船长:&quot;告诉他我是上校, 命令他转向20度.&quot;</span><br><span class="line">对方说:&quot;我是二等水手, 你最好转向20度.&quot;</span><br><span class="line">船长勃然大怒:&quot;告诉他,这里是战舰, 让他转向20度.&quot;</span><br><span class="line">对方传来信号:&quot;这里是灯塔.&quot;</span><br><span class="line">船长改变了航道.</span><br></pre></td></tr></table></figure>
<p>这位可怜的船长因为身处迷雾看不清事实, 化身为段子被人们讲了一遍又一遍, 只怪船长缺少一张绘制清晰、带有灯塔标识的谷歌地图; 原则如同灯塔, 是不容动摇的自然法则, 对个人而言, 同样需要一张绘制清晰、带有灯塔标识的地图, 或者说以原则为中心的思维定式.</p>
<h3 id="由内而外与由外而内">由内而外与由外而内</h3>
<p>由内而外的思维定式是指从自己的内心做起, 由外而内的思维定式是指将责任归咎于别人和环境. 综上所述, 就是这本书的主题: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">以原则为中心, 以品德为基础, 要求由内而外地实现个人效能与人际效能, 这是便是我们的目标.</span><br></pre></td></tr></table></figure></p>
<h3 id="检查你的思维定式">检查你的思维定式</h3>
<p>仔细想想如下几个问题的答案, 可以用笔写一写, 下面是笔者自己的答案: 你是否曾做出推测时发现自己的判断过于匆忙?请对这种经历加以描述.</p>
<blockquote>
<p>讨论班上, 每每讲完ppt, 到提问环节, 我总会等不及的等同学把完整的问题被叙述完就自顾自的开始回答, 以为问的问题应该是自己认为的问题, 事后想想不一定, 判断过于匆忙而想当然了.</p>
</blockquote>
<p>列出影响你生活的五个原则:</p>
<blockquote>
<p>完美主义. 保持好奇心. 不找理由找方法. 发挥自己的幽默感. 整体利益高于个人利益.</p>
</blockquote>
<p>各有其利弊:</p>
<blockquote>
<p>完美主义能让我交出优秀的成果, 但表现为总是拖到最后一刻 保持好奇心让我对什么都有些兴趣, 却又分摊了我的时间 不找理由找方法使我面对困难时能第一时间行动 发挥自己的幽默感能给别人留下不错的印象 践行整体利益高于个人利益的原则能赋予自己一种成就感</p>
</blockquote>
<hr />
<h2 id="七个习惯概论">七个习惯概论</h2>
<p>品德实际上是习惯的合成, 习惯在不知不觉中经年累月影响着我们的品德, 暴露着我们的本性, 左右着我们的成败.</p>
<h3 id="习惯">习惯</h3>
习惯定义为知识、技巧与意愿相互交织的结果. 知识指点做什么及为何做; 技巧告诉我们怎么做; 意愿代表了想要做; 要想养成一种习惯, 三者缺一不可.七个习惯并非零落、分散的心理法则, 它们符合成长规律, 提供了开发个人和人际效能的渐进、连续和高度整合的方法, 让我们经历由依赖到独立, 再到互赖.
<div data-align="center">
<img src="/pictures/timg.jpg" width="60%"/>
</div>
<hr />
<h2 id="习惯一-积极主动">习惯一: 积极主动</h2>
<p>我们应当充分认识到自己有责任创造条件.</p>
<h3 id="人性的本质是主动">人性的本质是主动</h3>
<p>下面请试着跳出自我的框框, 把意识转移到屋子的某个角落, 然后用心客观地审视自己, 你能站在旁观者的角度观察自己吗?再想一想你现在的精神状态如何?尝试去描述.</p>
<blockquote>
<p>尽可能的排除先验信息的话, 能看到一个奇怪坐姿的人类正对着电脑敲击着键盘, 抖着腿, 其它并不能观察出什么; 大脑处于一种高速思考、活跃的状态, 似乎特别想理清头绪.</p>
</blockquote>
<h3 id="刺激-回应理论与积极主动">刺激-回应理论与积极主动</h3>
<p>诺奖得主Pavlov通过实验得到, 狗会以某一特定方式方式回应某一特定刺激, 这被称为是刺激-回应理论; 对于人类而言, 在刺激与回应之间, 人有选择的能力, 原因就在于人类的四种天赋:</p>
<ul>
<li>自我意识(self-awareness), 即将自己与环境或是其它个体区分开来的能力</li>
<li>想象力(imagination), 即超越当前现实在头脑中进行创造的能力</li>
<li>良知(conscience), 即明辨是非, 坚持行为的原则, 判断思想, 言行与否的能力</li>
<li>独立意志(independent will), 即基于自我意识, 不受外力影响而自行其是的能力.</li>
</ul>
<p>由上述选择的能力发展出来的与生俱来的习惯为: 积极主动(be proactive), 它有时可能出于沉睡状态, 这时就需要我们去唤醒它.</p>
<h3 id="关注圈与影响圈">关注圈与影响圈</h3>
<p>每个人都有格外关注的问题, 比如健康、 事业、 战争等, 这些都可以被归纳入关注圈; 关注圈内的事物, 有些可以被掌控, 比如健康、事业, 有些则超出个人能力范围, 前者可被圈为一个较小的圈: 影响圈. 积极主动的人应当专注于影响圈, 若一个人一直把注意力放在超出其能力范围的事, 这会使得他慢慢变得怨天尤人, 积极主动也会慢慢陷入沉睡.</p>
<h3 id="培养积极主动的习惯">培养积极主动的习惯</h3>
<p>想想过去几周自己以消极方式作出回应的两三件事, 描述自己是怎么说的; 现在想想, 你应当如何作出积极应对:</p>
<blockquote>
<p>写论文是没用的(我应当去尝试一下).</p>
</blockquote>
<p>写下本周你所面临的各种挑战与问题, 他们分别归入哪些问题, 你的瞬间回应又是什么?</p>
<blockquote>
<p>周一晚上的变分法听不懂, 这属于影响圈内的问题, 我的回应是放弃治疗; 朋友想跳槽问我一些建议, 这属于影响圈内的问题, 我没能给出具体建议.</p>
</blockquote>
<p>试行积极主动原则30天, 写下自己的影响圈有何变化.</p>
<blockquote>
<p>有待补充</p>
</blockquote>
<hr />
<h2 id="习惯二-以终为始">习惯二: 以终为始</h2>
<p>我们务必紧盯着真正重要的愿景.</p>
<h3 id="主动设计还是被动接受">主动设计还是被动接受</h3>
<p>思考下述问题时, 请找个僻静的角落, 抛开一切杂念, 在你自己的葬礼上, 你希望人们对你的生活有什么样的评价?</p>
<blockquote>
<p>贴心的丈夫、孝顺的儿子、 教导有方的父亲、靠谱的同事、能谈心的挚友.</p>
</blockquote>
<p>以始为终的一个原则基础是任何事都是两次创造而成, 做任何事都是现在头脑中构思, 然后付诸实践; 但第一次的创造未必经过了有意识的设计, 一些人不愿主动设计自己的生活, 其生活轨迹就会屈从于家庭、同事、朋友及环境的压力. 生活的各个层面都存在第一次的创造, 每个人的人生都是第二次的创造, 或主动设计, 或被动接受. 自我意识、良知、想象力、独立意识这些天赋让我们能够审视第一次的创造, 换句话说, 习惯一明确了你是创造者, 而习惯二谈的是第一次的过程.</p>
<h3 id="自我领导">自我领导</h3>
<p>以始为终的另一个原则基础是自我领导. 领导不同于管理, 领导是第一次的创造, 必须先于管理; 管理是第二次的创造; 举个简单例子, 管理负责拟定政策, 引进技术等, 领导负责指明方向. 第一次创造的过程则可概括为, 利用想象力和良知两个天赋进行自我领导, 编写自己的人生剧本.</p>
<h3 id="个人使命宣言">个人使命宣言</h3>
<p>以始为终最有效的方法, 就是撰写一份个人使命宣言, 说明自己想成为怎么样的人, 这样的个人使命宣言能赋予你一种使命感. 有了个人使命宣言, 你就有了用以指导生活的愿景和价值观; 有了使命感, 你就有了积极主动的动力. 制定使命宣言必须从影响圈的核心开始, 基本的思维定式在这里, 它是我们观察世界的透镜:</p>
<blockquote>
<p>生活与工作有条不紊 诚恳待人 保持学习 保持幽默感</p>
</blockquote>
<p>利用良知作为罗盘来审视我们独特聪明才智和贡献手段; 利用想象力制定我们所渴求的人生目标 ,搜索使命宣言的素材. 这个核心还决定了安全感、人生方向、智慧与力量.</p>
<ul>
<li>安全感代表价值观、认同、情感的归属与自尊自重</li>
<li>人生方向是地图与内心的准则</li>
<li>智慧是人类对生命的认识、对平衡的感知和对事物联系的理解, 包括判断力、洞察力和理解力.</li>
<li>力量是指采取行动、达成目标的能力, 它是做出抉择的关键性力量.</li>
</ul>
<p>这四个因素是支撑一个人人生的关键因素, 若四者全面均衡, 便能培养出完美的个体</p>
<ul>
<li>安全感极度安全表示你对自己的真正价值有着清晰且深刻的认识</li>
<li>人生最佳方向为坚实的内在方向, 而不是以社会之镜为基础(p.s.社会之境代表了你周边的人透镜)</li>
<li>智慧的最高境界是所有事物和原则都适度关联的正确地图</li>
<li>力量的最高层次则是完全按照自己的价值观行事</li>
</ul>
<h3 id="以原则为中心">以原则为中心</h3>
<p>人人都有生活中心, 以原则为中心可以为上述四个支撑人生的因素安全感、人生方向、智慧与力量奠定坚实的基础. 然而, 大多数人并不是以原则为中心, 或是几种带有缺陷的中心的混合体, 其体现为上述四者不是那么完美均衡:</p>
<ul>
<li>以配偶为中心. 婚姻可以说是最亲密持久的人际关系了, 但容易出现情感过度依赖的问题. 如果我们一方面在情感上依赖对方, 另一方面又起了争执, 为了保护自己, 冷嘲热讽代替了真实的感受, 这么做似乎保证了安全感, 但失去了方向、智慧与力量.</li>
<li>以家庭为中心. 这样的父母在养育子女时往往缺乏以子女的最终幸福为目标的情感力量和自由, 比如他们只会关注子女一时的举止是否符合举止礼仪, 是否使自己失了面子.</li>
<li>以自我为中心. 这应该是时下最常见的了.</li>
<li>以金钱为中心. 若我主要从酬劳和薪水中获得安全感, 势必会寝食难安.</li>
<li>以工作为中心. 你的智慧力量会只限于工作领域.</li>
<li>以名利为中心. 你的方向会大大受限于社会之镜.</li>
<li>以享乐为中心.</li>
<li>以敌人或朋友为中心.</li>
<li>以宗教为中心.</li>
</ul>
<h3 id="培养以始为终的习惯">培养以始为终的习惯</h3>
<p>我现在的生活状况如何?它是否使我快乐?我是否有成就感?</p>
<blockquote>
<p>目前有条不紊的生活使我快乐, 同样带来了成就感.</p>
</blockquote>
<p>是什么一直在吸引我?它是否与我目前正在做的事情有关?</p>
<blockquote>
<p>知识与旅行一直在吸引我, 目前一直在学习知识.</p>
</blockquote>
<p>最让我的灵魂感到满足的是什么?</p>
<blockquote>
<p>我想可能是成就感.</p>
</blockquote>
<p>写下一个对你的生活有积极影响的人, 你最赞赏这个人的什么品质.</p>
<blockquote>
<p>王天浩, 细心.</p>
</blockquote>
<p>详细说明你想成为一个什么样的人.</p>
<blockquote>
<p>博学的人.</p>
</blockquote>
<hr />
<h2 id="习惯三-要事第一">习惯三: 要事第一</h2>
<p>自我领导决定了什么是重点后, 再靠自制力来掌握重点, 时刻把它放在第一位.</p>
<h3 id="第二次创造">第二次创造</h3>
<p>在你目前的生活中, 有哪些事情能够彻底使你的个人生活得到改观, 但你一直都没有去做.</p>
<blockquote>
<p>早睡早起</p>
</blockquote>
<p>简单梳理一下习惯一二三:</p>
<ul>
<li>习惯一告诉你, 你是创造者, 这个习惯的基础是人类特有的四大天赋: 想象力、良知、独立意志、自我意识, 这个习惯能让你大声宣布, 虽然这是社会中最常见的事, 但我应该改变它.</li>
<li>习惯二告是关于第一次创造的习惯, 其原则基础是想象力与良知, 这个习惯同我们的基本思维定式和对自己的最高期望值、价值观密切相关.</li>
<li>习惯三则是关于第二次创造的习惯, 是对前两个习惯的实现、执行和自然流露. 它要求我们运用独立意志努力实现一个目标, 即以原则为基础安排人生.</li>
</ul>
<h3 id="有效管理">有效管理</h3>
<p>本质上说, 领导是一种高效的右脑型活动, 常被人们称为一门艺术; 而有效的自我管理所涉及的都是左脑擅长的能力: 分解、分析、排序. 因此可以总结为左脑进行管理, 右脑进行领导. 有效的管理指的是要事第一, 先做重要的事. 领导者首先要决定的, 就是哪些事是重要的; 而作为管理者, 就是要将这些事优先安排. 从这个意义上讲, 自我管理的实质就是自律和条理, 是对计划的安排.</p>
<h3 id="四代时间管理论">四代时间管理论</h3>
<p>如何分辨轻重缓急与培养组织能力, 是时间管理的精髓.</p>
<ul>
<li>第一代理论着重利用便条与备忘录, 在忙碌中调配时间与精力.</li>
<li>第二代理论强调行事历与日程表, 反映出时间管理已注意到规划未来的重要性.</li>
<li>第三代是目前正流行、讲究优先顺序的观念. 也就是根据轻重缓急设定短、中、长期目标, 再逐日订定实现目标的计划. 将有限的时间精力加以分配, 争取最高的效率.</li>
<li>第四代理论主张关键不在时间管理, 而在于个人管理. 主张与其着重时间与事务的安排, 不如把重心放在维持产出与产能的平衡上.</li>
</ul>
<h3 id="第二类事务">第二类事务</h3>
<p>若按照紧集程度和重要程度进行分类, 所有事物可以分为如下四类:</p>
<ul>
<li>第一类事务: 紧集且重要</li>
<li>第二类事务: 不紧急但重要</li>
<li>第三类事务: 紧集但不重要</li>
<li>第四类事务: 不紧急且不重要</li>
</ul>
<p>第二类事务包括建立人际关系, 防范于未然等, 人人都知道很重要, 却因尚未迫在眉睫, 反而避重就轻. 高效能人士的思维定式是预防型的, 这使得他们的第一类事务不会太多, 且他们会把主要精力放在第二类事务上. 让第二类事务成为生活中心的有效工具必须满足以下六个重要标准:</p>
<ul>
<li>和谐一致: 个人的理想与使命、角色与目标、工作重点与计划、欲望与自制之间应和谐一致.</li>
<li>平衡功能: 管理方法应有助于生活平衡发展, 以免忽略了健康、家庭、个人发展等重要的人生层面.</li>
<li>围绕中心: 应就事务本身重要性来安排行事.</li>
<li>以人为本: 有效的个人管理偶尔需要牺牲效率, 不必为了进度落后而产生愧疚感.</li>
<li>灵活变通: emm</li>
<li>便于携带: 随时参考修改</li>
</ul>
<h3 id="培养要事第一的习惯">培养要事第一的习惯</h3>
<p>本周最优先的三项事务是什么?</p>
<blockquote>
<p>读高效能人士的七个习惯 出去玩一次 入门强化学习</p>
</blockquote>
<hr />
<h2 id="人际关系的本质">人际关系的本质</h2>
<p>情感账户里储存的是人际关系中不可或缺的信任. 能增加情感账户存款的, 是礼貌、诚实、仁慈与信用.</p>
<h3 id="情感账户的投资">情感账户的投资</h3>
<p>这里推荐七种主要的投资方式:</p>
<ul>
<li>理解他人. 首先要了解他人的实际需求, 而不是主观臆断他人的想法或需求.</li>
<li>注意小节.</li>
<li>信守承诺.</li>
<li>明确期望. 很多期望都是含蓄的, 这容易引起误解, 甚至冲突.</li>
<li>正直诚信. 提现这种品格的最好方法是避免背后攻击别人.</li>
<li>勇于致歉.</li>
<li>无条件的爱.</li>
</ul>
<h3 id="一对一的人际关系">一对一的人际关系</h3>
<p>相比为群体服务, 建立私人关系需要更多人格修养, 见习惯四五六.</p>
<hr />
<h2 id="习惯四-双赢思维">习惯四: 双赢思维</h2>
<p>世界之大, 人人都有足够的立足空间, 不必将他人之得视为自己之失.</p>
<p>人际交往常见的六种模式:</p>
<ul>
<li>利己利人(双赢)</li>
<li>两败俱伤(输/输)</li>
<li>损人利己(赢/输)</li>
<li>独善其身(赢)</li>
<li>舍己为人(输/赢)</li>
<li>好聚好散(无交易)</li>
</ul>
<h3 id="双赢思维的五个要领">双赢思维的五个要领</h3>
<ul>
<li>双赢品德是基础. 双赢品德有三个基本特征:
<ol type="1">
<li>诚信. 习惯一二三教育我们养成并保持诚信的品德.</li>
<li>成熟. 这是敢作敢为与善解人意之间的一种平衡状态.</li>
<li>知足. 相信资源充足, 人人有份.</li>
</ol></li>
<li>接着建立起双赢关系. 双赢的精髓就是信用, 即情感账户.</li>
<li>由此衍生出双赢协议. 它让纵向交往转为水平交往, 从属关系转为合作关系, 上级监督转为自我监督. 在双赢协议中, 对以下五要素应有明确规定:
<ol type="1">
<li>预期结果: 确认目标和时限.</li>
<li>指导方针: 确认时限目标的原则、方针和行为限度.</li>
<li>可用资源: 包括人力、财力、技术或组织资源.</li>
<li>任务考核: 建立绩效评估标准和时间.</li>
<li>奖惩制度: 根据任务考核确定奖罚.</li>
</ol></li>
<li>需要双赢体系作为培育环境. 包括双赢的管理培训、双赢绩效协议和双赢体系.</li>
<li>通过双赢的双赢过程来完成. 完成双赢过程的四个步骤:
<ol type="1">
<li>首先, 从对方的角度看问题.</li>
<li>其次, 认清主要问题和顾虑(而非立场).</li>
<li>再次, 确定大家都能接受的结果.</li>
<li>最后, 找到实现这种结果的各种可能途径.</li>
</ol></li>
</ul>
<h3 id="培养双赢思维的习惯">培养双赢思维的习惯</h3>
<p>敢作敢为与善解人意齐头并进.</p>
<hr />
<h2 id="习惯五-知彼解己">习惯五: 知彼解己</h2>
<p>首先寻求取了解对方, 然后在争取让对方了解自己.</p>
<h3 id="移情聆听">移情聆听</h3>
<p>然而, 大部分人在聆听时并不是想理解对方, 而是为了做出回应: "是的, 我知道你的感受, 我的经验是...". 移情聆听是指以理解目的的聆听, 要求听者站在说话者的角度理解他们的思维模式和感受, 表现为耳到, 眼到, 心到. 移情聆听同样是有风险的, 只有当你做好了被对方影响的准备, 才能深入移情聆听的阶段, 而这需要足够的安全感的, 因为这时候的你会变得很脆弱. 所以说, 习惯一二三是基础, 帮你保持核心不变, 即以原则为中心, 从而和平有力地应对坚定内心之外的脆弱.</p>
<h3 id="合理表达自己">合理表达自己</h3>
<p>我们在听别人讲话时总是会联系我们自己的经历, 因此自以为是的人往往会有四种自传式回应(autobiographical reponse)的倾向:</p>
<ul>
<li>价值判断: 对旁人的意见只有接受或不接受</li>
<li>追根究底: 依自己的的价值观探查别人的隐私</li>
<li>好为人师: 以自己的经验提供忠告</li>
<li>自以为是: 根据自己的行为与动机衡量别人的行为与动机</li>
</ul>
<p>古希腊人有一种经典的哲学观点, 即品德第一, 感情第二, 理性第三. 品德指的是你个人的可信度, 是你的情感账户; 感情指的是你的移情聆听能力, 是感性的; 理性是你的逻辑能力, 即合理表达自己的能力.</p>
<h3 id="培养知彼解己的习惯">培养知彼解己的习惯</h3>
<p>你什么时候最容易不专心听对方讲话?</p>
<blockquote>
<p>脑中思考问题时.</p>
</blockquote>
<hr />
<h2 id="习惯六-统合综效">习惯六: 统合综效</h2>
<p>与所见略同的人沟通, 益处不大; 要有分歧才有收获.</p>
<h3 id="判断与尊重差异">判断与尊重差异</h3>
<p>统合综效的精髓就是判断和尊重差异. 当有人有不同的观点时, 我们应该有自己的判断, 因此我们不一定要表示赞同, 但可以表示肯定, 并给与理解. 如果你坚持双赢模式, 那你应该寻找统合综效的第三种观点, 一般情况下它总是存在的.</p>
<h3 id="培养统合综效的习惯">培养统合综效的习惯</h3>
<p>尊重差异, 开辟第三种变通方案.</p>
<hr />
<h2 id="习惯七-不断更新">习惯七: 不断更新</h2>
<p>人生最值得投资的就是磨练自己.</p>
<h3 id="自我提升和完善的四个层面">自我提升和完善的四个层面</h3>
<p>自我提升和完善有四个层面:</p>
<ul>
<li>身体: 健康饮食、充足休息、定期锻炼</li>
<li>精神: 实现价值、忠诚、学习、冥想</li>
<li>智力: 阅读、想象、规划、写作</li>
<li>社会/情感: 服务、移情聆听、统合综效、内在安全感</li>
</ul>
<p><strong>p.s.</strong></p>
<ul>
<li>锻炼: 锻炼是第二类事务, 可以提高耐力, 韧性和力量.</li>
<li>写作: 通过不断地记录自己的想法、经历、见解和心得, 我们的思路就会更加明晰、准确和连贯.</li>
</ul>
<h3 id="平衡更新">平衡更新</h3>
<p>我们必须平衡好四个层面的更新, 只有平衡才能产生最佳的整体效果; 七个习惯也唯有在身心平衡的状态下运用效果最佳, 因为它们之间有密不可分的关系:</p>
<ul>
<li>越是积极主动(习惯一), 就越能在生活中有效地实施自我领导(习惯二)和管理(习惯三).</li>
<li>越是有效地管理, 就能从事越多的第二类事务的更新活动.</li>
<li>越能先理解别人(习惯五), 就越能找到统合综效的双赢解决方案(习惯四六).</li>
<li>越是在培养独立性方面加以改进(习惯一二三), 就越能在相互依赖的环境下提高效能(习惯四五六).</li>
<li>自我更新是强化这些习惯的过程.</li>
</ul>
<p>每天至少一小时身体、精神、智力层面的更新, 又称为个人领域的成功.</p>
<ul>
<li>身体层面的自我更新等同于强化个人愿景(习惯一), 它帮助我们增强积极性、自我意识和独立意志, 让我们知道自己是自由的, 不需要被动地承受他人的行为后果.</li>
<li>精神层面的自我更新等同于强化自我领导(习惯二), 它帮助我们更好地按照想象和良知行事, 深入理解个人思维和价值观, 确定核心的正确原则, 明确自己在生活中的独特使命, 改变思维和行为模式, 坚持正确原则并利用个人的资源优势.</li>
<li>智力层面的自我更新等同于强化自我管理(习惯三), 它帮助你在做计划时确定属于第二类事务的重要活动, 优先安排能够有效利用时间和精力的目标与活动, 然后围绕这些组织并开展活动.</li>
</ul>
<p>个人领域的成功又是公众领域成功的基础, 是更新社会/情感层面所需内在安全感的来源, 它能让你在相互依赖的环境中专注于自己的影响圈, 它也是实现真正的理解和统合综效的双赢解决方案.</p>
<h3 id="良知">良知</h3>
<p>自我提升和完善是一个螺旋式上升的过程, 而指导这个螺旋式上升过程的必须有人类的天赋: 良知. 良知帮助我们判断自己是否背离了正确的原则, 然后引导我们向这些原则靠拢.</p>
<h3 id="培养不断更新的习惯">培养不断更新的习惯</h3>
<p>如果你能选择五件有益于你灵魂的事, 你将选择哪些?</p>
<blockquote>
<p>阅读 写博客</p>
</blockquote>
<p>目前生活中我应该做什么事, 才能达到最积极的效果? 请深思熟虑.</p>
<blockquote>
<p>写博客</p>
</blockquote>
<p>生活现在需要我做什么?</p>
<blockquote>
<p>健身</p>
</blockquote>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>高效能人士的七个习惯</tag>
      </tags>
  </entry>
  <entry>
    <title>高斯过程的随机模拟(含R实现)</title>
    <url>/2018/09/24/%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%A8%A1%E6%8B%9F(%E5%90%ABR%E5%AE%9E%E7%8E%B0)/</url>
    <content><![CDATA[<p>前言: 读PRML一书的6.4节, 对图6.4中高斯过程的采用不同核函数模拟实现非常好奇, 同时也惊叹于高斯过程看上是几乎处处连续的性质, 于是尝试用R去模拟高斯过程. <span id="more"></span></p>
<hr />
<p>首先简单说明几点:</p>
<ul>
<li>经典的随机过程(stochastic process)的定义<span class="math inline">\(\{x_t, t\in T\}\)</span>可以简单理解为随机变量加上一个维度(比方说时间), 一般的, 时间视为不可数; 另外随机变量可以取实值, 复值, 向量值等, 但一般还是取实值.</li>
<li>由随机变量的概率空间及状态空间出发去定义随机过程概率空间却不是那么方便, 这当中用到了Kolmogorov定理, 详见钱敏平的随机过程论1.1及1.2小节.</li>
<li>另外一个重要定理可分修正表明, 任意随机过程都能修正为一个具有可分性的过程(即两者几乎处处相等), 而可分的随机过程其实带有一些连续性的味道, 可见钱敏平的随机过程论p11.</li>
<li>高斯过程则是任意有限维时间的联合分布(<span class="math inline">\(\{x_{t_1}, x_{t_2},...,x_{t_k}\}\)</span>, 其中<span class="math inline">\(t_1,..,t_k, k\)</span>均任取)为多元正态分布的随机过程, 可见<a href="https://en.wikipedia.org/wiki/Gaussian_process">维基百科</a>.</li>
<li>补充一点, 随机场(<a href="https://en.wikipedia.org/wiki/Random_field">random field</a>)则是可以视为随机过程的一般化, 即此时添加的可能不仅仅是一维的时间, 而是多维向量; 而高斯随机场(Gaussian random field)则是有限维联合分布为多元正态分布的随机场.</li>
<li>在现代数学中, 随机过程这个概念等同于随机场(参见随机场的维基百科).</li>
</ul>
<hr />
<p>容易知道, 多元正态分布仅仅依赖于它的期望<span class="math inline">\(\mu\)</span>与协方差矩阵<span class="math inline">\(\Sigma\)</span>, 因此如果我们能对任意有限维分布给出期望<span class="math inline">\(\mu\)</span>与协方差矩阵<span class="math inline">\(\Sigma\)</span>, 高斯过程也就给定了.</p>
<p>一种思路是假定<span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\Sigma=K\)</span>, 其中<span class="math inline">\(K_{ij}=k(t_i, t_j)\)</span>, 下面给出<span class="math inline">\(k(t_i, t_j)\)</span>的三种形式:</p>
<ul>
<li><span class="math inline">\(k(t_i, t_j)=\min\{t_i, t_j\}\)</span>, 此时高斯过程即为维纳过程, 或者说布朗运动, 高斯过程的R模拟图像如下:
<div data-align="center">
<img src="/pictures/R/R_brown_motion.png" width="80%" />
</div></li>
<li><span class="math inline">\(k(t_i, t_j)=\exp(-\Vert t_i-t_j\Vert/2\sigma^2)\)</span>, 这个函数又被称为高斯核, 取<span class="math inline">\(2\sigma^2=1\)</span>, 高斯过程的R模拟图像如下:
<div data-align="center">
<img src="/pictures/R/R_gauss_kernel.png" width="80%" />
</div>
取<span class="math inline">\(2\sigma^2=1/16\)</span>, 高斯过程的R模拟图像又如下:
<div data-align="center">
<img src="/pictures/R/R_gausskernel_2.png" width="80%" />
</div></li>
<li><span class="math inline">\(k(t_i, t_j)=\exp(-\theta\Vert t_i-t_j\Vert)\)</span>, 此时高斯过程即为<a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">Ornstein–Uhlenbeck过程</a>, 取<span class="math inline">\(\theta=1\)</span>, 高斯过程的R模拟图像如下:
<div data-align="center">
<img src="/pictures/R/R_OUprocess.png" width="80%" />
</div></li>
</ul>
<p>最后附上R代码, 其中核心部分代码即<code>gaussprocess</code>函数, 这个函数即先根据函数<span class="math inline">\(k(,)\)</span>去求对所有时间的协方差矩阵<span class="math inline">\(\Sigma\)</span>(计算机的计算必须离散化, 这里的模拟精度为<span class="math inline">\(t\in [0,1]\)</span>之间有1000个数), 再用<code>MASS</code>包中的<code>mvrnorm</code>函数去从多元正态分布中抽样: <figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">library(magrittr)</span><br><span class="line">library(tidyverse)</span><br><span class="line">gaussprocess &lt;- <span class="keyword">function</span>(from = <span class="number">0</span>,</span><br><span class="line">                         to = <span class="number">1</span>,</span><br><span class="line">                         K = <span class="keyword">function</span>(s, t) &#123;<span class="built_in">min</span>(s, t)&#125;,</span><br><span class="line">                         start = <span class="literal">NULL</span>,</span><br><span class="line">                         m = <span class="number">2000</span>) &#123;</span><br><span class="line">    t &lt;- seq(from = from, to = to, length.out = m)</span><br><span class="line">    Sigma &lt;- sapply(t, <span class="keyword">function</span>(s1) &#123;</span><br><span class="line">        sapply(t, <span class="keyword">function</span>(s2) &#123;</span><br><span class="line">            K(s1, s2)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    path &lt;- MASS::mvrnorm(mu = <span class="built_in">rep</span>(<span class="number">0</span>, times = m), Sigma = Sigma)</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">is.null</span>(start)) &#123;</span><br><span class="line">        path &lt;- path - path[<span class="number">1</span>] + start  <span class="comment"># Must always start at &quot;start&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">return</span>(data.frame(<span class="string">&quot;t&quot;</span> = t, <span class="string">&quot;xt&quot;</span> = path))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 维纳过程</span></span><br><span class="line">plot_1 &lt;- <span class="keyword">function</span>(times = <span class="number">5</span>) &#123;</span><br><span class="line">    data &lt;- gaussprocess()</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:(times-<span class="number">1</span>))&#123;</span><br><span class="line">        temp &lt;- gaussprocess()</span><br><span class="line">        temp[<span class="string">&#x27;id&#x27;</span>] = i</span><br><span class="line">        data &lt;- rbind(data, temp)</span><br><span class="line">    &#125;</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    ggplot(data = data, aes(x = t, y = xt, color = id, group = id)) + geom_line()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plot_1(times = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 高斯过程</span></span><br><span class="line">plot_2 &lt;- <span class="keyword">function</span>(times = <span class="number">5</span>) &#123;</span><br><span class="line">    data &lt;- gaussprocess(K = <span class="keyword">function</span>(s, t) &#123;<span class="built_in">exp</span>(-<span class="number">16</span> * (s - t) ^ <span class="number">2</span>)&#125;)</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:(times-<span class="number">1</span>))&#123;</span><br><span class="line">        temp &lt;- gaussprocess(K = <span class="keyword">function</span>(s, t) &#123;<span class="built_in">exp</span>(-<span class="number">16</span> * (s - t) ^ <span class="number">2</span>)&#125;)</span><br><span class="line">        temp[<span class="string">&#x27;id&#x27;</span>] = i</span><br><span class="line">        data &lt;- rbind(data, temp)</span><br><span class="line">    &#125;</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    ggplot(data = data, aes(x = t, y = xt, color = id, group = id)) + geom_line()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plot_2(times = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Ornstein-Uhlenbeck过程</span></span><br><span class="line">plot_3 &lt;- <span class="keyword">function</span>(times = <span class="number">5</span>) &#123;</span><br><span class="line">    data &lt;- gaussprocess(K = <span class="keyword">function</span>(s, t)&#123;<span class="built_in">exp</span>(-<span class="built_in">abs</span>(s - t))&#125;)</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:(times-<span class="number">1</span>))&#123;</span><br><span class="line">        temp &lt;- gaussprocess(K = <span class="keyword">function</span>(s, t)&#123;<span class="built_in">exp</span>(-<span class="built_in">abs</span>(s - t))&#125;)</span><br><span class="line">        temp[<span class="string">&#x27;id&#x27;</span>] = i</span><br><span class="line">        data &lt;- rbind(data, temp)</span><br><span class="line">    &#125;</span><br><span class="line">    data[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    ggplot(data = data, aes(x = t, y = xt, color = id, group = id)) + geom_line()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plot_3(times = <span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>高斯过程</tag>
        <tag>核函数</tag>
      </tags>
  </entry>
</search>
