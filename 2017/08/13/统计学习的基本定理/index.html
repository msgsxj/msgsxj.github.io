<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-fruit.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-fruit.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-fruit.png">
  <link rel="mask-icon" href="/images/logo.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://msgsxj.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:  统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理. 三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency). 三种学习范式:经验风险最小化">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习的基本定理">
<meta property="og:url" content="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/index.html">
<meta property="og:site_name" content="msgsxj&#39;blog">
<meta property="og:description" content="本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:  统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理. 三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency). 三种学习范式:经验风险最小化">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://msgsxj.cn/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_1%20.png">
<meta property="og:image" content="https://msgsxj.cn/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_2.png">
<meta property="og:image" content="https://msgsxj.cn/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_3.png">
<meta property="article:published_time" content="2017-08-13T15:47:14.000Z">
<meta property="article:modified_time" content="2020-01-01T07:53:45.381Z">
<meta property="article:author" content="msgsxj">
<meta property="article:tag" content="PAC可学习">
<meta property="article:tag" content="一致收敛性">
<meta property="article:tag" content="没有免费午餐定理">
<meta property="article:tag" content="VC维">
<meta property="article:tag" content="不一致可学习">
<meta property="article:tag" content="一致性">
<meta property="article:tag" content="ERM">
<meta property="article:tag" content="SRM">
<meta property="article:tag" content="MDL">
<meta property="article:tag" content="奥卡姆剃刀">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://msgsxj.cn/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_1%20.png">

<link rel="canonical" href="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>统计学习的基本定理 | msgsxj'blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-148974741-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-148974741-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?421443a76f7d2fd7ce01604d18772602";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">msgsxj'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="msgsxj">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="msgsxj'blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          统计学习的基本定理
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-08-13 23:47:14" itemprop="dateCreated datePublished" datetime="2017-08-13T23:47:14+08:00">2017-08-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-01-01 15:53:45" itemprop="dateModified" datetime="2020-01-01T15:53:45+08:00">2020-01-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:</p>
<ol type="1">
<li>统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理.</li>
<li>三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency).</li>
<li>三种学习范式:经验风险最小化(ERM)、结构风险最小化(SRM)及最小描述长度(MDL).</li>
</ol>
<p>最后要强调的是这块理论是针对二分类问题而言的,对于多分类问题,可将VC维拓展为<a href="https://link.springer.com/content/pdf/10.1007%2FBF00114804.pdf" target="_blank" rel="noopener">Natarajan维</a>.</p>
<a id="more"></a>
<h1 id="统计学习理论的基本定理">统计学习理论的基本定理</h1>
<p>在给出最核心的定理之前, 应当接触以下几个概念:</p>
<ul>
<li>领域集(domain set): 通常记为<span class="math inline">\(\mathcal{X}\)</span>, 由概率分布<span class="math inline">\(\mathcal{D}\)</span>从样本空间<span class="math inline">\(Z\)</span>中抽样得到.</li>
<li>目标函数(target function): 通常记为<span class="math inline">\(f\)</span>, 对于分类问题就是我们希望得到的真实的分类函数.</li>
<li>标签集(label set): 通常记为<span class="math inline">\(\mathcal{Y}\)</span>, 就二分类问题而言, 标签集为<span class="math inline">\(\{0, 1\}\)</span>.</li>
<li>训练数据(training data): 通常记为<span class="math inline">\(S\)</span>, 一般假定由目标函数<span class="math inline">\(f\)</span>生成: <span class="math inline">\(f(x)=y\)</span>. <span class="math inline">\(S=\{(x_{1},y_{1}),....,(x_{m},y_{m})\}\)</span>为一有限序列, 序列中元素以<span class="math inline">\(\mathcal{X}×\mathcal{Y}\)</span>形式成对出现, 记为<span class="math inline">\((x, y)\)</span>.</li>
<li>学习算法: 通常记为<span class="math inline">\(A\)</span>, 即我们构建的、能从数据中学到东西的学习算法, 期望其能从假设类中返回良好的假设.</li>
<li>假设类: 通常记为<span class="math inline">\(\mathcal{H}\)</span>, 对目标函数<span class="math inline">\(f\)</span>的假设构成的空间, 学习器从中选择.</li>
<li>预测器(predictor): 通常记为<span class="math inline">\(h\)</span>或者说分类器(classifier), 假设(hypothesis), 即学习器输出的一个函数:<span class="math inline">\(h:\mathcal{X}\to\mathcal{Y}\)</span>, 且应当和目标函数<span class="math inline">\(f\)</span>有相近的表现.</li>
<li>损失函数(loss function): 通常记为<span class="math inline">\(l(h, (x, y))\)</span>, 其定义在预测器<span class="math inline">\(h\)</span>和单个样本<span class="math inline">\((x, y)\)</span>上. 对于二分类问题, 一个常用的损失函数为<span class="math inline">\(l(h, (x, y))=I\{y\neq h(x)\}\)</span>, 即分对了就为0, 分错了就为1.</li>
<li>代价函数(cost function): 通常记为<span class="math inline">\(L_{S}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在训练集<span class="math inline">\(S\)</span>上的平均损失<span class="math inline">\(L_{S}(h)=\frac{1}{m}\sum_{i=1}^m l(h, (x_i, y_i))\)</span>, 有时又称为训练误差.</li>
<li>泛化误差: 通常记为<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在<span class="math inline">\((\mathcal{X}, \mathcal{Y})\)</span>上的平均损失<span class="math inline">\(\mathbb{E}_{(x, y)\sim (\mathcal{X}, \mathcal{Y})}l(h, (x, y))\)</span></li>
</ul>
<p>下图是机器学习的一个简单流程: <img src="/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_1%20.png" /></p>
<p>下面给出最核心的定理:</p>
<blockquote>
<p>令<span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>的映射函数构成的假设空间, 且令损失函数为0-1损失. 那么,下述陈述等价:</p>
</blockquote>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span>有一致收敛性.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</li>
</ul>
<p>下面我会一一解释上述出现的名词并证明该定理, 证明思路为1推2推3推4推6推1以及1推2推5推6推1, 其中6推1为难点.</p>
<h2 id="一致收敛性">一致收敛性</h2>
<p>一致收敛性(Uniform Convergence):</p>
<blockquote>
<p>存在一个函数<span class="math inline">\(m^{UC}_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 如果<span class="math inline">\(S\)</span>是从<span class="math inline">\(\mathcal{D}\)</span>得到的一个独立同分布的满足<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\)</span>的样本, 那么至少在概率<span class="math inline">\(1-\delta\)</span>下, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性的.</p>
</blockquote>
<p>满足一致收敛的类也叫做Glivenko-Cantelli类.</p>
<ul>
<li>这里的<span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性的可以先理解为训练数据是足够好的(好到能让我们通过<span class="math inline">\(S\)</span>得到一个不错的预测器<span class="math inline">\(h\)</span>), 假设类的一致收敛性即在说明不管<span class="math inline">\(\varepsilon,\delta\)</span>在<span class="math inline">\((0,1)\)</span>中怎么取,我总能根据<span class="math inline">\((\varepsilon,\delta)\to\mathbb{N}\)</span>的一个函数得到一个值<span class="math inline">\(m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\)</span>, 只要从<span class="math inline">\(S\)</span>中生成得到的样本量<span class="math inline">\(m\)</span>大于等于这个值, 那么此时抽到的训练数据就是足够好的. 假设类的一致收敛即确保了这样的函数的存在.</li>
</ul>
<h3 id="varepsilon-代表性样本"><span class="math inline">\(\varepsilon\)</span>-代表性样本</h3>
<p>那么所谓的训练数据是足够好的是什么意思呢? 下面来看. <span class="math inline">\(\varepsilon\)</span>-代表性样本(<span class="math inline">\(\varepsilon\)</span>-representative sample):</p>
<blockquote>
<p>如果满足下列不等式:<span class="math inline">\(\forall h \in\mathcal{H},|L_{S}(h)-L_{\mathcal{D}}(h)|\le\varepsilon\)</span>,训练集<span class="math inline">\(S\)</span>就称作<span class="math inline">\(\varepsilon\)</span>-代表性样本.</p>
</blockquote>
<p>由于学习器不知道<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(f\)</span>是什么样子的, 所以无法直接获知真实误差. 学习器能够计算出来的一个有用的概念是训练误差:</p>
<ul>
<li>分类器在训练样本<span class="math inline">\(S\)</span>上的误差:<span class="math inline">\(L_{S}(h)=\sum_{x\in S}I\{h(x)\neq f(x)\}\)</span>;</li>
<li>与之相对应的概念是泛化误差, 泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>为<span class="math inline">\(h\)</span>在<span class="math inline">\(\mathcal{D}\)</span>上的表现,定义为<span class="math inline">\(L_{\mathcal{D}}(h)=\mathbb{P}_{x\in \mathcal{D}}[h(x)\neq f(x)]\)</span>.</li>
</ul>
<p><span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本即说明哪怕<span class="math inline">\(h\)</span>是闭着眼睛瞎选的, 它在<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(S\)</span>上的表现都会差不多, 这说明抽样得到的<span class="math inline">\(S\)</span>是足以去代表<span class="math inline">\(\mathcal{D}\)</span>的. 且这里一致收敛中一致二字指的是对<span class="math inline">\(\mathcal{H}\)</span>中<span class="math inline">\(h\)</span>的一致性.</p>
<h3 id="经验风险最小化erm">经验风险最小化(ERM)</h3>
<p>经验风险最小化(ERM:Empirical Risk Minimization):</p>
<blockquote>
<p>对于学习器来说, 训练样本是真实世界的一个缩影, 一种很自然的想法就是通过最小化<span class="math inline">\(L_{S}(h)\)</span>来寻找<span class="math inline">\(h\)</span>, 这种最小化<span class="math inline">\(L_{S}(h)\)</span>的学习范式称为是经验风险最小化(ERM:Empirical Risk Minimization).</p>
</blockquote>
<h2 id="pac学习理论">PAC学习理论</h2>
<p>PAC学习理论(PAC Learning)主要包括不可知PAC可学习(Agnostic PAC Learnability), PAC可学习(PAC Learnability)及广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions).</p>
<h3 id="不可知pac可学习">不可知PAC可学习</h3>
<p>不可知PAC可学习(Agnostic PAC Learnability):</p>
<blockquote>
<p>若存在一个函数<span class="math inline">\(m_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 这里的PAC(probably approximately correct)即概率近似正确, 它是在说若样本量<span class="math inline">\(m\)</span>足够大, 学习算法<span class="math inline">\(A\)</span>会以不小的概率返回一个<span class="math inline">\(h\)</span>, 这个<span class="math inline">\(h\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>和假设类<span class="math inline">\(\mathcal{H}\)</span>中最好的分类器仅有<span class="math inline">\(\varepsilon\)</span>的差距.</p>
<ul>
<li>一致收敛性与不可知PAC可学习均是对假设类<span class="math inline">\(\mathcal{H}\)</span>的性质的刻画,它们都要求了从<span class="math inline">\((\varepsilon,\delta)\to\mathbb{N}\)</span>的函数的存在, 不同的是不可知PAC可学习的要求似乎更高些, 因为它能保证只要<span class="math inline">\(h\)</span>训练误差<span class="math inline">\(L_{S}(h)\)</span>还不赖, 那么泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>必然也还不赖.</li>
<li>而不可知PAC可学习只能保证以一定的概率返回一个泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>还不赖的<span class="math inline">\(h\)</span>, 从这个方面看, 一致收敛性的要求确实要略强于不可知PAC可学习.</li>
</ul>
<p>回头来看定理的第一第二条. <span class="math inline">\(\mathcal{H}\)</span>有一致收敛性等价于任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器, 也就是说不可知PAC学习器再加上一个不错的方法(ERM规则), 就能等同于一致收敛性.</p>
<h3 id="pac可学习">PAC可学习</h3>
<p>PAC可学习(PAC Learnability):</p>
<blockquote>
<p>若存在一个函数<span class="math inline">\(m_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 任意的标号函数<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>, 如果在<span class="math inline">\(\mathcal{H},\mathcal{D},f\)</span>下满足可实现的假设, 那么当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到并由<span class="math inline">\(f\)</span>标记, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D,f}}(h)\le \varepsilon\)</span>.</p>
</blockquote>
<p>不可知PAC可学习实际上是PAC可学习的范化, 原因是满足可实现这句话事实上假设了假设类<span class="math inline">\(\mathcal{H}\)</span>中必然有一个完美的分类器<span class="math inline">\(h\)</span>, 而事实上这不能保证.</p>
<p>如果假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 有很多函数<span class="math inline">\(m_{\mathcal{H}}\)</span>满足PAC可学习的定义给出的条件, 我们定义采样复杂度为满足条件的最小函数.</p>
<p>下面是有限假设类的一个例子:</p>
<blockquote>
<p>任意有限假设类是PAC可学习的,其采样复杂度满足:<span class="math inline">\(m_{\mathcal{H}}(\varepsilon,\delta)\le [\frac{log(|\mathcal{H}|)/\delta}{\varepsilon}]\)</span>.</p>
</blockquote>
<p>该定理用霍夫丁不等式(<a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality" target="_blank" rel="noopener">Hoeffding's inequality</a>)可证, 霍夫丁不等式的证明可见链接维基百科或是深入理解机器学习一书附录B.4, 该定理的详细证明可见深入理解机器学习一书2.3节.</p>
<h4 id="霍夫丁不等式">霍夫丁不等式</h4>
<p>霍夫丁不等式(Hoeffding's inequality):</p>
<blockquote>
<p><span class="math inline">\(\theta_{1},...,\theta_{m}\)</span>是一个独立同分布的r.v.序列, 假设对所有的<span class="math inline">\(i\)</span>, <span class="math inline">\(E[\theta_{i}]=\mu\)</span>,且<span class="math inline">\(\mathbb{P}[a\le\theta_{i}\le b]=1\)</span>, 则对于<span class="math inline">\(\forall\varepsilon&gt;0\)</span> <span class="math display">\[\mathbb{P}[|\frac{1}{m}\sum^{m}_{i=1}\theta_{i}-\mu|&gt;\varepsilon]\le2\exp(-2m\varepsilon^{2}/(b-a)^{2})\]</span></p>
</blockquote>
<h3 id="广义损失函数下的不可知pac可学习">广义损失函数下的不可知PAC可学习</h3>
<p>为了能处理其他学习任务比方说多分类, 回归问题, 我们将损失函数进行如下范化: 广义损失函数:</p>
<ul>
<li>0-1损失, 这个损失函数用在二分类或者多分类问题中. <span class="math display">\[ l_{0-1}(h,(x,y))=0,where\{h(x)=y\};1,where \{h(x)\neq y\}\]</span></li>
<li>平方损失,这个损失函数用在回归问题中. <span class="math display">\[l_{sq}(h,(x,y))=(h(x)-y)^{2}\]</span></li>
</ul>
<p>广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions):</p>
<blockquote>
<p>对于集合<span class="math inline">\(Z\)</span>和损失函数<span class="math inline">\(l:\mathcal{H}×Z\to \mathbb{R}_{+}\)</span>, 若存在一个函数<span class="math inline">\(m_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<h3 id="对不可知的探讨">对不可知的探讨</h3>
<p>上文提到不可知PAC可学习是对PAC可学习的范化, 因为假设类<span class="math inline">\(\mathcal{H}\)</span>中未必有一个完美的分类器<span class="math inline">\(h\)</span>, 而这又有两种可能的深层原因:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span>取得不够大, 没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>取得够大了, 但<span class="math inline">\(\mathcal{H}\)</span>仍没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>, 一种合理的推测就是根本不存在能将西瓜完美分类的<span class="math inline">\(f\)</span>. 比方说因为<span class="math inline">\(\mathcal{X}\)</span>中特征选取不是太好, 某好瓜和某坏瓜在选取的特征上的数据完全一致, 此时<span class="math inline">\(f\)</span>当然没法把他们区分开来.</li>
</ul>
<p>前者相对好办, 只要扩大<span class="math inline">\(\mathcal{H}\)</span>, 而后者就比较蛋疼了, 这节中不可知PAC可学习及广义损失函数下的不可知PAC可学习也应当做修改, 不应该是根据概率分布<span class="math inline">\(\mathcal{D}\)</span>在<span class="math inline">\(Z\)</span>上抽取样本, 而应当是根据概率分布<span class="math inline">\(\mathcal{D&#39;}\)</span>在<span class="math inline">\(Z×\{0,1\}\)</span>上抽取样本了.</p>
<p>此时修正后的机器学习一个简单流程如下:<img src="/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_2.png" alt="机器学习2" /></p>
<h2 id="vc维">VC维</h2>
<p>由上节PAC可学习我们已经知道, 有限假设类假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的充分条件. 但是否为必要条件呢? 答案是否定的. 事实上, 无限假设类也能是PAC可学习的, 只要它的VC维有限, 下面来介绍VC维.</p>
<p>我们先来看一个著名的定理: 没有免费的午餐定理(No-Free-Lunch), 这个定理说明了如果不对假设类加以限制, 任何学习算法总有很差表现的时候.</p>
<h3 id="没有免费的午餐定理">没有免费的午餐定理</h3>
<p>没有免费的午餐定理(No-Free-Lunch):</p>
<blockquote>
<p>对实例空间<span class="math inline">\(\mathcal{X}\)</span>上的0-1损失的二分任务, <span class="math inline">\(A\)</span>表示任意的学习算法. 样本大小<span class="math inline">\(m\)</span>为小于<span class="math inline">\(|\mathcal{X}|/2\)</span>的任意数.则在<span class="math inline">\(\mathcal{X}×\{0,1\}\)</span>上存在一个分布<span class="math inline">\(\mathcal{D}\)</span>, 使得:</p>
</blockquote>
<ol type="1">
<li>存在一个函数<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(f)=0\)</span>.</li>
<li>样本量为<span class="math inline">\(m\)</span>的样本集<span class="math inline">\(S\)</span>, 由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 以至少<span class="math inline">\(\frac{1}{7}\)</span>的概率满足:<span class="math inline">\(L_{\mathcal{D}}(A(S))\ge \frac{1}{8}\)</span>.</li>
</ol>
<p>这个定理陈述的是对每个学习器<span class="math inline">\(A\)</span>, 都存在一个学习任务(分布<span class="math inline">\(\mathcal{D}\)</span>)使其失败, 即使这个任务存能被另一个学习器成功学习(比方说关于假设类<span class="math inline">\(\mathcal{H}=\{f\}\)</span>的一个ERM学习器).</p>
<p>下面是没有免费的午餐定理的证明: 一个直观的看法如下, 令<span class="math inline">\(C\)</span>是大小为<span class="math inline">\(2m\)</span>的集合<span class="math inline">\(\mathcal{X}\)</span>的子集, 任何只观测到空间<span class="math inline">\(C\)</span>中一半样本的学习算法<span class="math inline">\(A\)</span>, 都不具有信息来反映<span class="math inline">\(C\)</span>中剩余样本. 因此存在一个可能性, 在<span class="math inline">\(C\)</span>中未观测到的样本上, 目标函数<span class="math inline">\(f\)</span>与<span class="math inline">\(A(S)\)</span>预测的完全不同(详见深入理解机器学习一书5.1节):</p>
<ul>
<li>从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>有<span class="math inline">\(T=2^{|\mathcal{X}|}\)</span>个函数,记为<span class="math inline">\(f_{1},...,f_{T}\)</span>. 对每个<span class="math inline">\(f_{i}\)</span>取分布<span class="math inline">\(\mathcal{D_{i}}\)</span>满足<span class="math inline">\(f_{i}\)</span>, 即如果<span class="math inline">\(y\neq f(x)\)</span>, 分布<span class="math inline">\(\mathcal{D_{i}}\)</span>抽取样本<span class="math inline">\(x×y\)</span>的概率置为0.</li>
<li>事实上, 此时<span class="math inline">\(|\mathcal{X}|\)</span>可直接取为<span class="math inline">\(2m\)</span>, 假定对<span class="math inline">\(|\mathcal{X}|=2m\)</span>定理得证, 那么此时任意加入新样本,只需将新样本的标签取成<span class="math inline">\(|1-A(S)|\)</span>, 就能保证<span class="math inline">\(L_{\mathcal{D}}(A(S))\)</span>仍大于<span class="math inline">\(\frac{1}{8}\)</span>, 而<span class="math inline">\(f\)</span>及<span class="math inline">\(\mathcal{D}\)</span>只需要做相应修改.</li>
<li>对任意学习算法<span class="math inline">\(A\)</span>从1.中的<span class="math inline">\(\{\mathcal{D}_{i}\}\)</span>找一个<span class="math inline">\(\mathcal{D}_{i}\)</span>使得<span class="math inline">\(E[L_{\mathcal{D_{i}}}(A(S))]\ge \frac{1}{4}\)</span>.</li>
<li><span class="math inline">\(P[L_{\mathcal{D}_{i}}(A(S))\ge\frac{1}{8}]\ge\frac{1}{7}\)</span></li>
</ul>
<p>注意到定理中并没有对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 假设类<span class="math inline">\(\mathcal{H}\)</span>的规模是随着样本量大小而指数型增长的, 从而导致学习失败. 于是乎如果我们要想学习成功, 就必须对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 那么如何去限制呢？</p>
<ul>
<li>我们已经知道任何有限假设类都是可学习的(Hoeffding测度集中不等式可证).</li>
<li>那么无限类呢?事实上, 有一个叫VC维的东西可以刻画出PAC可学习的假设类的增长速度如何, 直观的去看, VC维有限的假设类就是那些限制在集合<span class="math inline">\(C\)</span>上并且只能随着<span class="math inline">\(|C|\)</span>的增长而多项式增长而不是指数型增长的这类假设类, 这也被称为是小的有效规模.</li>
</ul>
<h3 id="打散">打散</h3>
<p>打散(Shattering):</p>
<blockquote>
<p>如果限制<span class="math inline">\(\mathcal{H}\)</span>在<span class="math inline">\(C\)</span>上是从<span class="math inline">\(C\)</span>到<span class="math inline">\(\{0,1\}\)</span>的所有函数的集合, 那么我们称<span class="math inline">\(\mathcal{H}\)</span>打散了集合<span class="math inline">\(C\)</span>, 此时<span class="math inline">\(|\mathcal{H}_{C}|=2^{|C|}\)</span>.</p>
</blockquote>
<p>这里<span class="math inline">\(\mathcal{H}\)</span>的状态也就是上文说的不加以限制的状态.</p>
<h3 id="vc维-1">VC维</h3>
<p>VC维(VC-dimension):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维, 记为<span class="math inline">\(VCdim(\mathcal{H})\)</span>可以打散的最大的集合的大小. 如果<span class="math inline">\(\mathcal{H}\)</span>可以打散任何集合大小,我们说<span class="math inline">\(\mathcal{H}\)</span>的VC维是无穷的.</p>
</blockquote>
<p>回头再来看没有免费午餐定理,我们易得下述推论:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{X}\)</span>为一个无限定义域集, <span class="math inline">\(\mathcal{H}\)</span>为从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>上的所有映射集, 即<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>,则<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的.</p>
</blockquote>
<h3 id="vc维的进一步探讨-偏差与复杂性权衡">VC维的进一步探讨-偏差与复杂性权衡</h3>
<p>上面我们已经讨论了VC维的定义,本节试图对VC维做进一步的理解.</p>
<p>在PAC可学习框架下, 我们所用的方式是: 对于给定的假设类<span class="math inline">\(\mathcal{H}\)</span>, 采用ERM范式从中选取分类器<span class="math inline">\(h\)</span>, 而这种假设类<span class="math inline">\(\mathcal{H}\)</span>的选取是我们凭借自己的先验知识做出的选择.</p>
<ul>
<li>比方说我已经很肯定的说好坏西瓜的唯一分界线就在于西瓜是否有籽(至于有籽好吃或是无籽好吃待定), 那么此时<span class="math inline">\(\mathcal{H}\)</span>就确定为只包含两种假设的集合, 此时<span class="math inline">\(VCdim(\mathcal{H})=1\)</span>.</li>
<li>另一种情况, 我对西瓜的好坏毫无头绪, 那我就把所有的可能都作为我的假设类, 此时<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>(更可能是某个很大的值)</li>
<li>第三种是最好的情况, 我能肯定西瓜的好坏一定是来自大小, 纹理清晰等20个特征, 那么此时我就把这些特征所有可能的组合写进我的假设类,此时<span class="math inline">\(VCdim(\mathcal{H})=20\)</span>.</li>
</ul>
<p>我们将ERM算法在假设类<span class="math inline">\(\mathcal{H}\)</span>分解为两部分, 第一部分叫偏差, 由假设类具有的最小风险<span class="math inline">\(\min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)\)</span>所决定, 它反映了先验知识的质量; 第二部分叫复杂性, 是有过拟合引起的误差, 取决于假设类的大小或是复杂度, 也称为估计误差. 这两项意味着, 在一个较为复杂的假设和一个简单的假设之间存在一个权衡. 这是因为, 复杂的假设对应较高的VC维, 可以减少偏差但会增加过拟合风险, 简单的假设对应较小的VC维, 可以减小过拟合风险但会增大偏差, 因此这两者之间应当有个最优值, 比方说上述第三种情况就是最优值. 这里引用Coursera上林轩田老师的机器学习基石课程中Lecture 7中的一张图加以辅助说明: <img src="/pictures/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_3.png" alt="机器学习3" /></p>
<h2 id="定理的证明">定理的证明</h2>
<p>定义说完了, 下面来看证明:</p>
<h3 id="推2">1推2</h3>
<p>一句话证明1推2:<span class="math inline">\(\mathcal{H}\)</span>一致收敛也就是说样本量足够时, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本, 现用<span class="math inline">\(\varepsilon/2\)</span>取代<span class="math inline">\(\varepsilon\)</span>, <span class="math inline">\(\varepsilon\)</span>-代表性样本的定义及ERM的返回规则得:对<span class="math inline">\(\forall h\in \mathcal{H}\)</span>,<span class="math display">\[L_{\mathcal{D}}(h_{S})\le L_{\mathcal{S}}(h_{S})+\varepsilon/2\le L_{\mathcal{S}}(h)+\varepsilon/2 \le L_{\mathcal{D}}(h)+\varepsilon/2+\varepsilon/2=L_{\mathcal{D}}(h)+\varepsilon\]</span>即由ERM(S)返回<span class="math inline">\(h_{S}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{S})\)</span>能保证学习成功.</p>
<h3 id="推3">2推3</h3>
<p>2推3就非常显然, 2是对于假设类<span class="math inline">\(\mathcal{H}\)</span>采用了ERM学习算法的学习器在<span class="math inline">\(\mathcal{H}\)</span>能成功, 要证明3只要证明存在一个学习算法能成功, 显然存在(ERM).</p>
<h3 id="推4">3推4</h3>
<p>3推4更显然, 3是4的泛化, 所以4的条件成立时3必然能用, 再加上<span class="math inline">\(f\)</span>满足可实现的假设, 即存在完美的分类器使得泛化误差为0, 得证.</p>
<h3 id="推5">2推5</h3>
<p>2推5同上可证.</p>
<h3 id="推6">4推6</h3>
<p>4推6用到了没有免费的午餐定理, 这个定理可以叙述为如果<span class="math inline">\(\mathcal{H}\)</span>的VC维无限, 那么<span class="math inline">\(\mathcal{H}\)</span>就不是PAC可学习的, 来看这个命题的逆否命题, 即如果<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 那么<span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</p>
<h3 id="推6-1">5推6</h3>
<p>5推6的话同样用到了免费的午餐定理, 假设<span class="math inline">\(\mathcal{H}\)</span>的VC维无穷, 那么<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的, 然后铁定不是采取了ERM之后的PAC可学习的了.</p>
<h3 id="推1">6推1</h3>
<p>这里只叙述证明思路:</p>
<ul>
<li>如果<span class="math inline">\(VCdim(\mathcal{H})=d&lt;+\infty\)</span>, 那么即使<span class="math inline">\(\mathcal{H}\)</span>无限, 当将其限制在一有限集合<span class="math inline">\(C\)</span>时, 其有效规模<span class="math inline">\(|\mathcal{H}_{C}|\)</span>只有<span class="math inline">\(O(|C|^{d})\)</span>.</li>
<li>假设类有一个小的有效规模时其一致收敛成立, 小的有效规模指的是<span class="math inline">\(|\mathcal{H}_{C}|\)</span>随<span class="math inline">\(|C|\)</span>按多项式方式增长.</li>
</ul>
<p>其中第一步实际上是将<span class="math inline">\(\mathcal{H}\)</span>的VC维有限时, <span class="math inline">\(\mathcal{H}\)</span>的增长速度为多项式增长这一事实明确下来, 证明用到了Sauer引理, 详细见深入理解机器学习一书6.5.1节; 2.的证明见深入理解机器学习一书6.5.2节.<span class="math inline">\(□\)</span></p>
<h1 id="统计学习的基本定理-定量形式">统计学习的基本定理-定量形式</h1>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to \{0,1\}\)</span>的映射函数构成的假设类, 且令损失函数为0-1损失. 假定<span class="math inline">\(VCdim(\mathcal{H})&lt;\infty\)</span>, 那么, 存在绝对常数<span class="math inline">\(C_{1},C_{2}\)</span>使得:</p>
</blockquote>
<blockquote>
<ol type="1">
<li><span class="math inline">\(\mathcal{H}\)</span>一致收敛, 若其样本复杂度满足: <span class="math inline">\(C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\)</span>.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 若其样本复杂度满足: <span class="math inline">\(C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\)</span>.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 若其样本复杂度满足: <span class="math inline">\(C_{1}\frac{d+log(1/\delta)}{\varepsilon}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{dlog(1/\varepsilon)+log(1/\delta)}{\varepsilon^{2}}\)</span>.</li>
</ol>
</blockquote>
<p><code>p.s.</code>由上述定理可以看出, 一致收敛与不可知PAC可学习的需要的样本量一致, 而PAC可学习的需要的样本量少一些. 该定理的证明在深入理解机器学习一书第28章给出.</p>
<h1 id="不一致可学习">不一致可学习</h1>
<p>上文所讨论的PAC可学习的概念是考虑精度和置信参数来决定样本数量, 且样本标签分布与内在的样本数据分布是一致的. 因此, PAC可学习等价于VC维有限. 下面要讨论的是不一致可学习这个概念, 这个概念是不可知PAC可学习的严格松弛, 它允许样本数量依赖假设空间<span class="math inline">\(\mathcal{H}\)</span>, 下面来看其定义:</p>
<p>不一致可学习(Nonuniform learnability):</p>
<blockquote>
<p>若存在一个学习算法<span class="math inline">\(A\)</span>和一个函数<span class="math inline">\(m^{NUL}_{\mathcal{H}}:(0,1)^{2}×\mathcal{H}\to\mathbb{N}\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和<span class="math inline">\(h\in\mathcal{H}\)</span>, 如果样本数量<span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\)</span>, 那么对每个分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>, <span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>, 则假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote>
<p>这个概念要求输出假设与假设类中其他假设相比具有<span class="math inline">\((\varepsilon,\delta)\)</span>的竞争力, 同时从定义也很容易得到不可知PAC可学习一定能推得不一致可学习, 且不一致可学习是不可知PAC可学习的严格松弛. 下面举个例子说明两者的严格松弛关系:</p>
<ul>
<li>考虑二分问题, 假设<span class="math inline">\(\mathcal{H}_{n}\)</span>是<span class="math inline">\(n\)</span>次多项式构成的假设类, 即<span class="math inline">\(\mathcal{H}_{n}\)</span>为<span class="math inline">\(h(x)=sign(p(x))\)</span>的分类器的集合.这里<span class="math inline">\(p(x)\)</span>是<span class="math inline">\(\mathbb{R}\to\mathbb{R}\)</span>的<span class="math inline">\(n\)</span>次多项式.令<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 容易得到<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>, 但<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</li>
<li><p>事实上, 有一个定理清晰的描述了两者的关系:</p>
<blockquote>
<p>两分类器的假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 当且仅当它是不可知PAC可学习的可数并.</p>
</blockquote></li>
</ul>
<p>下面来看这个定理的证明:</p>
<ul>
<li>必要性: 假定<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 对于每个<span class="math inline">\(n\in\mathbb{N}\)</span>,令<span class="math inline">\(\mathcal{H}_{n}=\{h\in\mathcal{H}:m^{NUL}_{\mathcal{H}}(\frac{1}{8},\frac{1}{7},h)\le n\}\)</span>. 显然<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 此外, 由<span class="math inline">\(m^{NUL}_{\mathcal{H}}\)</span>定义知, 对于任何关于<span class="math inline">\(\mathcal{H}_{n}\)</span>满足可实现性假设的分布<span class="math inline">\(\mathcal{D}\)</span>, <span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 选择样本<span class="math inline">\(S\)</span>的概率大于等于<span class="math inline">\(\frac{6}{7}\)</span>. 则<span class="math inline">\(L_{\mathcal{D}}(A(S))\le\frac{1}{8}\)</span>. 由统计学习基本定理知, <span class="math inline">\(VCdim(\mathcal{H}_{n})&lt;+\infty\)</span>, 因此<span class="math inline">\(\mathcal{H}_{n}\)</span>是不可知PAC可学习的.</li>
<li><p>充分性: 充分性的证明依赖于下述引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote></li>
</ul>
<p>该引理证明留给下小节, 那么由统计学习基本定理知, 不可知PAC可学习与一致收敛性等价,即得证<span class="math inline">\(□\)</span></p>
<h2 id="结构风险最小化srm">结构风险最小化SRM</h2>
<p>本节主要是关于结构风险最小化(SRM:Structural Risk Minimization)以及利用结构风险最小化算法来证明上节未证明的引理.</p>
<p>目前为止, 我们都是通过具体化一个假设类<span class="math inline">\(\mathcal{H}\)</span>来利用先验知识, 并且相信这样一个假设类中包含完成当前任务的有效预测器. 而另一种表达先验知识的方法则是将假设类<span class="math inline">\(\mathcal{H}\)</span>上的偏好具体化.</p>
<p>在结构风险最小化范式中, 我们首先假定假设类<span class="math inline">\(\mathcal{H}\)</span>能够写成<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛, 且样本复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\)</span>, 然后具体化一个权重函数<span class="math display">\[\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\]</span>这个权重函数可也反映每个假设类的重要性, 或是假设类复杂性的度量.例如<span class="math inline">\(\mathcal{H}\)</span>是所有多项式分类器构成的类, <span class="math inline">\(\mathcal{H}_{n}\)</span>表示<span class="math inline">\(n\)</span>次多项式分类器构成的类.定义下式函数<span class="math inline">\(\varepsilon_{n}:\mathbb{N}×(0,1)\to(0,1)\)</span>,<span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\]</span>由<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛性, <span class="math inline">\(\forall m\in\mathbb{N}\)</span>,<span class="math inline">\(\forall\delta\in(0,1)\)</span>, 大小为<span class="math inline">\(m\)</span>的样本<span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span> <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta)\]</span>这就是我们所关心的训练误差与泛化误差之间的差值. 此时权重函数可取成<span class="math inline">\(\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>或是<span class="math inline">\(\omega(n)=2^{-n}\)</span>.</p>
<p>下面将上述例子写成定理的形式,并由此引出结构风险最小化(SRM):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>. 令<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>为一权重函数. 令<span class="math inline">\(\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\)</span>. 则对<span class="math inline">\(\forall\delta\in(0,1)\)</span>,任意的分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>其中<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>.</p>
</blockquote>
<p>证明: 对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>, 定义<span class="math inline">\(\delta_{n}=\omega(n)\delta\)</span>, 固定<span class="math inline">\(n\)</span>, 由一致收敛性, 在选择样本<span class="math inline">\(S\)</span>的概率不低于<span class="math inline">\(1-\delta\)</span>的条件下, <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta_{n})\]</span>应用<span class="math inline">\(n=1,2,...,\)</span>的联合界, 我们得到上述结论以不低于<span class="math inline">\(1-\sum_{n}\delta_{n}=1-\delta\sum_{n}\omega(n)\ge1-\delta\)</span>的概率对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>都成立, 最后令<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>自然也成立.<span class="math inline">\(□\)</span></p>
<p>结构风险最小化(SRM)则是寻找<span class="math inline">\(h\)</span>来最小化这个上界, 以保证<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>不是那么大, 下面是结构风险最小化(SRM)的定义:</p>
<blockquote>
<p>先验:<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>; 权重函数<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>. 定义:<span class="math inline">\(\varepsilon_{n}(m,\delta)=min\{\varepsilon\in(0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\)</span>; <span class="math inline">\(n(h)=min\{n:h\in\mathcal{H}_{n}\}\)</span>. 输入:训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>. 输出:<span class="math inline">\(h\in argmin_{h\in\mathcal{H}}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\)</span>.</p>
</blockquote>
<p><code>p.s.</code>结构风险最小化(SRM)与经验风险最小化(ERM)最大的不同就是, 经验风险最小化(ERM)只关心训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>, 而结构风险最小化(SRM)兼顾了训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>和训练误差与泛化误差之间的差值<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\)</span>.</p>
<p>下面用SRM来证明上节未能证明的一个引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote>
<p>事实上利用SRM算法我们能得到下述更进一步的结论:</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>. 如果<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 且满足有<span class="math inline">\(m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\le m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\frac{6\delta}{(\pi n(h))^{2}})\)</span></p>
</blockquote>
<p>证明: 假定<span class="math inline">\(A\)</span>是考虑权重函数<span class="math inline">\(\omega\)</span>的结构风险最小化算法, 对于<span class="math inline">\(\forall h\in\mathcal{H}_{n},\forall\varepsilon,\delta\in(0,1)\)</span>, 令<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\varepsilon,\omega(n(h))·\delta)\)</span>,应用上述定理, 可以得到<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>这个定理对于由结构风险规则返回的假设<span class="math inline">\(A(S)\)</span>成立. 通过结构风险最小化的定义可得,<span class="math display">\[L_{\mathcal{D}}(A(S))\le\min_{h}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>如果<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\omega(n(h))·\delta)\)</span>, 那么<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\le\frac{\varepsilon}{2}\)</span>. 此外, 从每个<span class="math inline">\(\mathcal{H}_{n}\)</span>的一致收敛性, 我们可得到下式成立的概率大于<span class="math inline">\(1-\delta\)</span>,<span class="math display">\[L_{S}(h)\le L_{\mathcal{D}}(h)+\frac{\varepsilon}{2}\]</span>综上所述, 可得<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>, 定理得证.<span class="math inline">\(□\)</span></p>
<p>单独比较不一致可学习和不可知PAC可学习是非常有意思的, 算法会在全空间<span class="math inline">\(\mathcal{H}\)</span>上搜索一个模型, 而不是在特定的<span class="math inline">\(\mathcal{H}_{n}\)</span>上搜索一个模型, 利用先验知识的缺陷所带来的成本就是增加复杂度与特定的<span class="math inline">\(h\in\mathcal{H}_{n}\)</span>相竞争.假定对于所有的<span class="math inline">\(n\)</span>, <span class="math inline">\(VCdim(\mathcal{H}_{n})=n\)</span>, 由统计学习的基本定理-定量形式知<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le C\frac{n+log(1/\delta)}{\varepsilon^{2}}\)</span>, 一个直接的计算表明<span class="math display">\[m^{NUL}_{\mathcal{H}_{n}}(\varepsilon,\delta,h)-m^{UC}_{\mathcal{H}_{n}}(\frac{\varepsilon}{2},\delta)\le 4C\frac{2\log(2n)}{\varepsilon^{2}}\]</span>代价增加了类的索引, 可以解释为反映已知的假设类<span class="math inline">\(\mathcal{H}\)</span>的好的先验知识的排序值.</p>
<h3 id="不一致可学习的没有免费午餐定理">不一致可学习的没有免费午餐定理</h3>
<p>在不一致可学习框架下,没有免费的午餐定理(No-Free-Lunch for Nonuniform Learnability)也是成立的,也就是说, 当样本域无限时, 不存在关于所有确定性二分类器所构成的类的不一致学习器(尽管对于每一个分类器存在一个尝试算法能够学习包含这些分类器假设的结构风险最小化).</p>
<h2 id="最小描述长度mdl">最小描述长度MDL</h2>
<p><span class="math inline">\(\mathcal{H}\)</span>为假设类, 将<span class="math inline">\(\mathcal{H}\)</span>写成单个类的可数并, 即<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\{h_{n}\}\)</span>.</p>
<p>由Hoeffding不等式知, 每一个单类有一致收敛性, 收敛速率<span class="math inline">\(m^{UC}(\varepsilon,\delta)=\frac{\log(2/ \delta)}{2\varepsilon^{2}}\)</span>, 该结果的证明见深入理解机器学习一书page26. 因此, <span class="math inline">\(\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}(\varepsilon,\delta)\le m\}\)</span>所给出的函数<span class="math inline">\(\varepsilon_{n}\)</span>变成了<span class="math inline">\(\varepsilon_{n}(m,\delta)=\sqrt{\frac{\log(2/\delta)}{2m}}\)</span>,且结构风险最小化SRM变成了:<span class="math display">\[argmin_{h_{n}\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(n))+\log(2/\delta)}{2m}}\]</span>等价的, 我们可以认为<span class="math inline">\(\omega\)</span>是从<span class="math inline">\(\mathcal{H}\to[0,1]\)</span>的函数, 然后结构结构风险最小化SRM变成了:<span class="math display">\[argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(h))+\log(2/\delta)}{2m}}\]</span>这节讨论一种特别方便的方式定义权重函数<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>, 这个函数与假设的描述长度有关. 下面介绍其背景:</p>
<p>对于每个假设类<span class="math inline">\(\mathcal{H}\)</span>, 我们想知道如何描述和表示每一个类的假设<span class="math inline">\(h\)</span>, 英语, 汇编语言, 数学公式等等形式, 当我们选定某种语言形式, 那么一个假设<span class="math inline">\(h\)</span>肯定能被一些特定字母<span class="math inline">\(\Sigma\)</span>组成的有限字符串所描述, <span class="math inline">\(\Sigma\)</span>我们称之为字母表. 例如, 令<span class="math inline">\(\Sigma=\{0,1\}\)</span>, <span class="math inline">\(\sigma=(0,1,1,1,0)\)</span>为一字符串且字符串的长度<span class="math inline">\(|\sigma|=5\)</span>, 所有有限长度的字符串用<span class="math inline">\(\Sigma^{+}\)</span>表示, 对假设类<span class="math inline">\(\mathcal{H}\)</span>的描述可用一个函数<span class="math inline">\(d:\mathcal{H}\to\Sigma^{+}\)</span>表示, <span class="math inline">\(d(h)\)</span>将<span class="math inline">\(\mathcal{H}\)</span>中每个假设<span class="math inline">\(h\)</span>映射为一个字符串<span class="math inline">\(d(h)\)</span>, <span class="math inline">\(d(h)\)</span>称为<span class="math inline">\(h\)</span>的描述长度, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度. 我们要求描述语言<span class="math inline">\(d(h)\)</span>无前缀, 即不同的<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(d(h)\)</span>不是<span class="math inline">\(d(h&#39;)\)</span>的前缀. 无前缀的字符串满足如下组合性质:</p>
<p>Kraft不等式:</p>
<blockquote>
<p>如果<span class="math inline">\(S\subset\{0,1\}^{+}\)</span>是一个无前缀的字符串集合,则<span class="math inline">\(\sum_{\sigma\in S}\frac{1}{2^{|\sigma|}}\le1\)</span>.</p>
</blockquote>
<p>根据Kraft不等式, 任何无前缀的语言都能给出假设类<span class="math inline">\(\mathcal{H}\)</span>的权重函数<span class="math inline">\(\omega\)</span>, 我们可简单的设置为<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>,于是便有了下述定理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一假设类, <span class="math inline">\(d:\mathcal{H}\to\{0,1\}^{+}\)</span>是<span class="math inline">\(\mathcal{H}\)</span>的一无前缀描述语言,对于样本量<span class="math inline">\(m\)</span>, 置信参数<span class="math inline">\(\delta&gt;0\)</span>和概率分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率大于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H},L_{\mathcal{D}}(h)\le L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\]</span>这里<span class="math inline">\(|h|\)</span>是指<span class="math inline">\(d(h)\)</span>的长度.</p>
</blockquote>
<p>类似结构风险最小化(SRM), 最小描述长度(MDL)这种考虑了训练误差和减小描述长度, 这就得到了减小描述长度的学习范式: 最小描述长度(MDL:Minimum Description Length):</p>
<blockquote>
<p>先验: 假设类<span class="math inline">\(\mathcal{H}\)</span>由定义在<span class="math inline">\(\{0,1\}\)</span>上的无前缀语言<span class="math inline">\(d\)</span>描述;对于任何一个<span class="math inline">\(h\in\mathcal{H}\)</span>, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度. 输入: 训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>. 输出: <span class="math inline">\(h\in argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\)</span></p>
</blockquote>
<h3 id="奥卡姆剃刀原理">奥卡姆剃刀原理</h3>
<p>上届的定理指出, 对于经验风险相同的两个假设, 描述长度较小的假设, 其真实风险的误差界更小, 因此这个结果表达了一个哲学理念, 这也是著名的奥卡姆剃刀原理(Occam’s Razor):</p>
<blockquote>
<p>短的解析更有效.</p>
</blockquote>
<p><code>p.s.</code>重新审视之下, 奥卡姆剃刀原理似乎有一些问题, 假象下列情形, 两个假设<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>在语言<span class="math inline">\(A\)</span>下的描述长度分别为10,100; 但在语言<span class="math inline">\(B\)</span>下的长度分别为100,10.那么这里的问题在哪里?</p>
<h1 id="一致性">一致性</h1>
<p>最后给出一个比不一致可学习更松弛的可学习概念: 一致性, 这个概念允许所欲要的样本不仅依赖于<span class="math inline">\(\varepsilon,\delta\)</span>和<span class="math inline">\(h\)</span>,而且依赖于数据所依托的分布<span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>一致性(Consistency):</p>
<blockquote>
<p><span class="math inline">\(\mathcal{P}\)</span>表示<span class="math inline">\(Z\)</span>上的概率分布, 若存在一个函数<span class="math inline">\(m^{CON}_{\mathcal{H}}:(0,1)^{2}×\mathcal{H}×\mathcal{P}\to\mathbb{N}\)</span>使得对任意一个<span class="math inline">\(h\in\mathcal{H}\)</span>,<span class="math inline">\(\mathcal{D}\in\mathcal{P}\)</span>,<span class="math inline">\(\varepsilon\)</span>,<span class="math inline">\(\delta\in(0,1)\)</span>, <span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h,\mathcal{D})\)</span>,下式成立的概率不小于1-δ,<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>.我们就认为一个学习规则<span class="math inline">\(A\)</span>关于<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{P}\)</span>满足一致性.</p>
</blockquote>
<h2 id="memorize算法">Memorize算法</h2>
<p>考虑如下定义的分类预测算法Memorize, 这个算法对新样本的预测永远都是某个固定的标签,比如说是训练集中出现次数最多的标签. 事实上, Memorize算法对于任何数据集上所有函数构成的类都满足一致性, 但这个算法很明显没有什么用,这让我们对一致性的用途产生了怀疑.</p>
<p><code>写在最后</code>: 本文主要是写了写PAC可学习框架下有关VC维的内容, 这个VC维(Vapnik–Chervonenkis dimension)其实代表两位大牛:svm发明者<a href="https://en.wikipedia.org/wiki/Vladimir_Vapnik" target="_blank" rel="noopener">Vapnik</a>和俄罗斯的数学家<a href="https://en.wikipedia.org/wiki/Alexey_Chervonenkis" target="_blank" rel="noopener">Chervonenkis</a>. 进一步内容的Rademacher复杂度则是我下一篇博客的内容, 西瓜书将其放在12.5节, 深入理解机器学习一书则是将其放在第四部分高级理论26-27章, 针对多分类问题的Natarajan维在29章.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PAC%E5%8F%AF%E5%AD%A6%E4%B9%A0/" rel="tag"># PAC可学习</a>
              <a href="/tags/%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7/" rel="tag"># 一致收敛性</a>
              <a href="/tags/%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86/" rel="tag"># 没有免费午餐定理</a>
              <a href="/tags/VC%E7%BB%B4/" rel="tag"># VC维</a>
              <a href="/tags/%E4%B8%8D%E4%B8%80%E8%87%B4%E5%8F%AF%E5%AD%A6%E4%B9%A0/" rel="tag"># 不一致可学习</a>
              <a href="/tags/%E4%B8%80%E8%87%B4%E6%80%A7/" rel="tag"># 一致性</a>
              <a href="/tags/ERM/" rel="tag"># ERM</a>
              <a href="/tags/SRM/" rel="tag"># SRM</a>
              <a href="/tags/MDL/" rel="tag"># MDL</a>
              <a href="/tags/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80/" rel="tag"># 奥卡姆剃刀</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2017/12/06/Getting-and-Cleaning-Data/" rel="next" title="Getting and Cleaning Data">
      Getting and Cleaning Data <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#统计学习理论的基本定理"><span class="nav-number">1.</span> <span class="nav-text">统计学习理论的基本定理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一致收敛性"><span class="nav-number">1.1.</span> <span class="nav-text">一致收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#varepsilon-代表性样本"><span class="nav-number">1.1.1.</span> <span class="nav-text">\(\varepsilon\)-代表性样本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#经验风险最小化erm"><span class="nav-number">1.1.2.</span> <span class="nav-text">经验风险最小化(ERM)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pac学习理论"><span class="nav-number">1.2.</span> <span class="nav-text">PAC学习理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#不可知pac可学习"><span class="nav-number">1.2.1.</span> <span class="nav-text">不可知PAC可学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pac可学习"><span class="nav-number">1.2.2.</span> <span class="nav-text">PAC可学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#霍夫丁不等式"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">霍夫丁不等式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广义损失函数下的不可知pac可学习"><span class="nav-number">1.2.3.</span> <span class="nav-text">广义损失函数下的不可知PAC可学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对不可知的探讨"><span class="nav-number">1.2.4.</span> <span class="nav-text">对不可知的探讨</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vc维"><span class="nav-number">1.3.</span> <span class="nav-text">VC维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#没有免费的午餐定理"><span class="nav-number">1.3.1.</span> <span class="nav-text">没有免费的午餐定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#打散"><span class="nav-number">1.3.2.</span> <span class="nav-text">打散</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vc维-1"><span class="nav-number">1.3.3.</span> <span class="nav-text">VC维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vc维的进一步探讨-偏差与复杂性权衡"><span class="nav-number">1.3.4.</span> <span class="nav-text">VC维的进一步探讨-偏差与复杂性权衡</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定理的证明"><span class="nav-number">1.4.</span> <span class="nav-text">定理的证明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#推2"><span class="nav-number">1.4.1.</span> <span class="nav-text">1推2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推3"><span class="nav-number">1.4.2.</span> <span class="nav-text">2推3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推4"><span class="nav-number">1.4.3.</span> <span class="nav-text">3推4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推5"><span class="nav-number">1.4.4.</span> <span class="nav-text">2推5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推6"><span class="nav-number">1.4.5.</span> <span class="nav-text">4推6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推6-1"><span class="nav-number">1.4.6.</span> <span class="nav-text">5推6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推1"><span class="nav-number">1.4.7.</span> <span class="nav-text">6推1</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#统计学习的基本定理-定量形式"><span class="nav-number">2.</span> <span class="nav-text">统计学习的基本定理-定量形式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#不一致可学习"><span class="nav-number">3.</span> <span class="nav-text">不一致可学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#结构风险最小化srm"><span class="nav-number">3.1.</span> <span class="nav-text">结构风险最小化SRM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#不一致可学习的没有免费午餐定理"><span class="nav-number">3.1.1.</span> <span class="nav-text">不一致可学习的没有免费午餐定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小描述长度mdl"><span class="nav-number">3.2.</span> <span class="nav-text">最小描述长度MDL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#奥卡姆剃刀原理"><span class="nav-number">3.2.1.</span> <span class="nav-text">奥卡姆剃刀原理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一致性"><span class="nav-number">4.</span> <span class="nav-text">一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#memorize算法"><span class="nav-number">4.1.</span> <span class="nav-text">Memorize算法</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">msgsxj</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/msgsxj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;msgsxj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:msgsxj@gmail.com" title="E-Mail → mailto:msgsxj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">msgsxj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
