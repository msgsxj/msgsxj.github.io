<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-fruit.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-fruit.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-fruit.png">
  <link rel="mask-icon" href="/images/logo.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://msgsxj.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:  统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理. 三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency). 三种学习范式:经验风险最小化">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习的基本定理">
<meta property="og:url" content="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/index.html">
<meta property="og:site_name" content="msgsxj&#39;blog">
<meta property="og:description" content="本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:  统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理. 三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency). 三种学习范式:经验风险最小化">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://msgsxj.cn/pictures/vc2.svg">
<meta property="article:published_time" content="2017-08-13T15:47:14.000Z">
<meta property="article:modified_time" content="2020-05-27T12:35:32.970Z">
<meta property="article:author" content="msgsxj">
<meta property="article:tag" content="PAC可学习">
<meta property="article:tag" content="一致收敛性">
<meta property="article:tag" content="没有免费午餐定理">
<meta property="article:tag" content="VC维">
<meta property="article:tag" content="不一致可学习">
<meta property="article:tag" content="一致性">
<meta property="article:tag" content="ERM">
<meta property="article:tag" content="SRM">
<meta property="article:tag" content="MDL">
<meta property="article:tag" content="奥卡姆剃刀">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://msgsxj.cn/pictures/vc2.svg">

<link rel="canonical" href="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>统计学习的基本定理 | msgsxj'blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-148974741-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-148974741-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?1bc9ef392b6302f7b4918f79882fbe82";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">msgsxj'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://msgsxj.cn/2017/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="msgsxj">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="msgsxj'blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          统计学习的基本定理
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-08-13 23:47:14" itemprop="dateCreated datePublished" datetime="2017-08-13T23:47:14+08:00">2017-08-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-27 20:35:32" itemprop="dateModified" datetime="2020-05-27T20:35:32+08:00">2020-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要参考了Shai Shalev-Shwartz的深入理解机器学习第2-7章以及coursera上林轩田的机器学习基石课程,这部分内容较为详细的描述了机器学习的理论部分,主要包括以下三部分内容:</p>
<ol type="1">
<li>统计学习理论的基本定理,涉及:一致收敛性、PAC学习理论、VC维及没有免费的午餐定理.</li>
<li>三个可学习的概念:PAC可学习、不一致可学习及一致性(Consistency).</li>
<li>三种学习范式:经验风险最小化(ERM)、结构风险最小化(SRM)及最小描述长度(MDL).</li>
</ol>
<p>最后要强调的是这块理论是针对监督学习中的二分类问题而言的,对于多分类问题,可将VC维拓展为<a target="_blank" rel="noopener" href="https://link.springer.com/content/pdf/10.1007%2FBF00114804.pdf">Natarajan维</a>.</p>
<span id="more"></span>
<h1 id="统计学习理论的基本定理">统计学习理论的基本定理</h1>
<p>在给出最核心的定理之前, 应当接触以下几个概念:</p>
<ul>
<li>样本空间: 对于监督学习而言为带标签的样本<span class="math inline">\((x,y)\)</span>的取值空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>, 学习的目标是映射<span class="math inline">\(f: \mathcal{X}\to\mathcal{Y}\)</span>, 以用于产生对新样本<span class="math inline">\(x_{new}\)</span>的标签的可靠预测<span class="math inline">\(f(x_{new})\)</span>. <code>p.s.</code>对无监督学习而言为不带标签的样本<span class="math inline">\(x\)</span>的取值空间<span class="math inline">\(\mathcal{X}\)</span>, 学习目标为对<span class="math inline">\(\mathcal{X}\)</span>的可靠分类.</li>
<li>目标函数: 通常记为<span class="math inline">\(f\)</span>, 即上述监督学习中所说的映射<span class="math inline">\(f\)</span>. 对于分类问题就是我们希望得到的真实的分类函数.</li>
<li>训练集: 通常记为<span class="math inline">\(S\)</span>, 由定义在样本空间<span class="math inline">\(Z=(\mathcal{X}, \mathcal{Y})\)</span>上的概率分布<span class="math inline">\(\mathcal{D}\)</span>抽样得到 <span class="math display">\[S=\{(x_{1},y_{1}),....,(x_{m},y_{m})\}\]</span></li>
<li>学习算法: 通常记为<span class="math inline">\(A\)</span>, 即我们构建的、能从数据中学到东西的学习算法, 期望其能从假设类中返回良好的假设.</li>
<li>假设类: 通常记为<span class="math inline">\(\mathcal{H}\)</span>, 对目标函数<span class="math inline">\(f\)</span>的假设构成的空间, 学习器从中选择.</li>
<li>预测器(predictor): 或者说分类器(classifier)、假设(hypothesis), 通常记为<span class="math inline">\(h\)</span>, 即学习器输出的一个函数:<span class="math inline">\(h:\mathcal{X}\to\mathcal{Y}\)</span>, 且应当和目标函数<span class="math inline">\(f\)</span>有相近的函数值.</li>
<li>损失函数: 通常记为<span class="math inline">\(l(h, (x, y))\)</span>, 其定义在预测器<span class="math inline">\(h\)</span>和单个样本<span class="math inline">\((x, y)\)</span>上. 对于二分类问题, 一个常用的损失函数为<span class="math inline">\(l(h, (x, y))=I\{y\neq h(x)\}\)</span>, 即分对了就为0, 分错了就为1.</li>
<li>训练误差: 通常记为<span class="math inline">\(L_{S}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在训练集<span class="math inline">\(S\)</span>上的平均损失<span class="math display">\[L_{S}(h)=\frac{1}{m}\sum_{i=1}^m l(h, (x_i, y_i))\]</span></li>
<li>泛化误差: 通常记为<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>, 其定义为预测器<span class="math inline">\(h\)</span>在<span class="math inline">\((\mathcal{X}, \mathcal{Y})\)</span>上的平均损失<span class="math display">\[\mathbb{E}_{(x, y)\sim (\mathcal{X}, \mathcal{Y})}l(h, (x, y))\]</span></li>
</ul>
<p>这里给出的概念及记号借鉴了深入理解机器学习一书中的写法. 下面的内容适合有一定基础的同学阅读, 这里给出最核心的定理:</p>
<blockquote>
<p>令<span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>的映射函数构成的假设空间, 且令损失函数为0-1损失. 那么,下述陈述等价:</p>
<ol type="1">
<li><span class="math inline">\(\mathcal{H}\)</span>有一致收敛性.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的.</li>
<li>任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的PAC学习器.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</li>
</ol>
</blockquote>
<p>下面我会一一解释上述出现的名词并证明该定理, 证明分两块</p>
<ul>
<li>1推2推3推4推6推1</li>
<li>1推2推5推6推1</li>
</ul>
<p>其中6推1为难点.</p>
<h2 id="一致收敛性">一致收敛性</h2>
<h3 id="varepsilon-代表性样本"><span class="math inline">\(\varepsilon\)</span>-代表性样本</h3>
<p><span class="math inline">\(\varepsilon\)</span>-代表性样本(<span class="math inline">\(\varepsilon\)</span>-representative sample):</p>
<blockquote>
<p>如果满足下列不等式:<span class="math inline">\(\forall h \in\mathcal{H},|L_{S}(h)-L_{\mathcal{D}}(h)|\le\varepsilon\)</span>,训练集<span class="math inline">\(S\)</span>就称作<span class="math inline">\(\varepsilon\)</span>-代表性样本.</p>
</blockquote>
<p>泛化误差<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>为<span class="math inline">\(h\)</span>在概率分布<span class="math inline">\(\mathcal{D}\)</span>上的表现, 定义为<span class="math display">\[L_{\mathcal{D}}(h)=\mathbb{P}_{(x,y)\in \mathcal{D}}[h(x)\neq y]\]</span>它是能反应假设<span class="math inline">\(h\)</span>的性能的最真实的误差. 但一般情况下, 我们无法知道<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(f\)</span>是什么样子的, 所以无法直接获知真实的误差. 一种思路是转而去关心训练误差和泛化误差界:</p>
<ul>
<li>训练误差<span class="math inline">\(L_{S}(h)\)</span>为<span class="math inline">\(h\)</span>在训练样本<span class="math inline">\(S\)</span>上的表现, 定义为<span class="math display">\[L_{S}(h)=\frac{1}{|S|}\sum_{(x,y)\in S}I\{h(x)\neq y\}\]</span>由于这里的<span class="math inline">\(S\)</span>已知, 因此能够计算得到训练误差.</li>
<li>泛化误差界定义为<span class="math display">\[\sup_{h\in\mathcal{H}}|L_{S}(h)-L_{\mathcal{D}}(h)|\]</span>一般来说, 有关泛化误差界的一些结果如<a href="https://msgsxj.cn/2018/06/14/VC-bound/">VC界</a>等总是在以<span class="math inline">\(1-\delta\)</span>概率成立的意义下, 显然这个界会与假设类<span class="math inline">\(\mathcal{H}\)</span>的选择有关.</li>
</ul>
<p>控制好了这两个量, 就可以认为泛化误差也被控制住了.</p>
<p><code>p.s.</code> <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本即说明哪怕<span class="math inline">\(h\)</span>是闭着眼睛瞎选的, 它在<span class="math inline">\(\mathcal{D}\)</span>和<span class="math inline">\(S\)</span>上的表现都会差不多, 这说明此时训练集<span class="math inline">\(S\)</span>是足以去代表<span class="math inline">\(\mathcal{D}\)</span>的.</p>
<h3 id="一致收敛性-1">一致收敛性</h3>
<p>一致收敛性(Uniform Convergence, UC)这一概念从学习成功所需训练集大小的角度刻画了假设类的复杂度:</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>具有一致收敛性, 若存在一个函数<span class="math display">\[m^{UC}_{\mathcal{H}}:(0,1)\times (0,1)\to\mathbb{N}\]</span> 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>、<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>、从<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到的大小为<span class="math inline">\(m\)</span>的训练集<span class="math inline">\(S\)</span>, 只要<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\)</span>, 就有至少在概率<span class="math inline">\(1-\delta\)</span>下, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性的.</p>
</blockquote>
<p>这里的一致指的是对假设类<span class="math inline">\(\mathcal{H}\)</span>中的假设<span class="math inline">\(h\)</span>和<span class="math inline">\(Z\)</span>上任意概率分布<span class="math inline">\(\mathcal{D}\)</span>的一致性, 满足一致收敛的类也叫做Glivenko-Cantelli类.</p>
<p><code>p.s.</code> <span class="math inline">\(\varepsilon\)</span>-代表性可以理解为训练集足够好, 好到泛化误差界<span class="math display">\[\sup_{h\in\mathcal{H}}|L_{S}(h)-L_{\mathcal{D}}(h)|\]</span>能被<span class="math inline">\(\varepsilon\)</span>控制住. 若假设类<span class="math inline">\(\mathcal{H}\)</span>具有一致收敛性, 那么不管是什么样的要求, 只要训练集的样本数<span class="math inline">\(m\)</span>足够大且是独立抽样得到, 那我就能在一定概率下保证泛化误差能被<span class="math inline">\(\varepsilon\)</span>控制住. 假想这么一种情况, 假设类<span class="math inline">\(\mathcal{H}\)</span>足够复杂, 因此对于任意的<span class="math inline">\(S\)</span>, 遍历<span class="math inline">\(h\in\mathcal{H}\)</span>就能让<span class="math inline">\(L_{S}(h)\)</span>遍历<span class="math inline">\([0,1]\)</span>, 继而我总能找到一个<span class="math inline">\(h\)</span>, 使得<span class="math display">\[|L_{S}(h)-L_{\mathcal{D}}(h)|&gt;\varepsilon\]</span>事实上, 一致收敛性中对<span class="math inline">\(h\in\mathcal{H}\)</span>的一致性的要求本质上要求了假设类<span class="math inline">\(\mathcal{H}\)</span>不能过于复杂.</p>
<h3 id="经验风险最小化erm">经验风险最小化(ERM)</h3>
<p>经验风险最小化(ERM:Empirical Risk Minimization):</p>
<blockquote>
<p>对于学习器来说, 训练样本是真实世界的一个缩影, 一种很自然的想法就是通过最小化训练误差<span class="math inline">\(L_{S}(h)\)</span>来寻找<span class="math inline">\(h\)</span>, 这种最小化<span class="math inline">\(L_{S}(h)\)</span>的学习范式称为是经验风险最小化.</p>
</blockquote>
<p><code>p.s.</code> 按照同时控制训练误差和泛化误差界这两个量的思路, 我们只需要设计好算法来最小化训练误差, 同时恰当选择假设类以控制好泛化误差界, 就能控制住泛化误差, 即所谓的学习成功.</p>
<h2 id="pac可学习">PAC可学习</h2>
<p>PAC(Probably Approximately Correct)可学习的概念主要包括不可知PAC可学习(Agnostic PAC Learnability), PAC可学习(PAC Learnability)及广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions)这三个概念.</p>
<h3 id="不可知pac可学习">不可知PAC可学习</h3>
<p>不可知PAC可学习(Agnostic PAC Learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 若存在一个函数<span class="math display">\[m_{\mathcal{H}}:(0,1)\times(0,1)\to\mathbb{N}\]</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>、<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>、从<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到的大小为<span class="math inline">\(m\)</span>的训练集<span class="math inline">\(S\)</span>, 只要<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon, \delta)\)</span>, 算法<span class="math inline">\(A\)</span>通过最小化训练误差<span class="math inline">\(L_{S}(h)\)</span>将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<p>倘若此时假设类<span class="math inline">\(\mathcal{H}\)</span>只包含贝叶斯最优分类器, 即以概率1满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>, 此时我只需将贝叶斯最优分类器的函数值取其对立面(将函数输出结果的01对调), 就能将概率1降为概率0.5. 从这个层面看, 不可知PAC可学习也要求了假设类不能过于复杂.</p>
<p><code>p.s.</code> 与一致收敛性不同的是, 不可知PAC可学习通过考察<strong>假设类中的最优假设</strong>和<strong>最优秀的算法返回的假设</strong>之间的泛化误差差距来控制假设类的复杂度, 因而需要剔除训练误差优秀、泛化误差垃圾的假设; 而一致收敛性要求假设类中的一致性同样要求将训练误差优秀、泛化误差垃圾的假设剔除.</p>
<p><code>p.s.</code>回头来看定理的第一第二条. <span class="math inline">\(\mathcal{H}\)</span>有一致收敛性等价于任何ERM规则都是对于<span class="math inline">\(\mathcal{H}\)</span>成功的不可知PAC学习器, 也就是说不可知PAC学习器再加上一个不错的方法(ERM规则), 就能等同于一致收敛性.</p>
<h3 id="pac可学习-1">PAC可学习</h3>
<p>PAC可学习(PAC Learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 若存在一个函数<span class="math display">\[m_{\mathcal{H}}:(0,1)\times(0,1)\to\mathbb{N}\]</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 任意的<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>, 如果在<span class="math inline">\(\mathcal{H},\mathcal{D},f\)</span>下满足可实现的假设, 那么当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到并由<span class="math inline">\(f\)</span>标记, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \varepsilon\)</span>.</p>
</blockquote>
<p>不可知PAC可学习实际上是PAC可学习的范化, 这里假定样本的标签由<span class="math inline">\(f(x)\)</span>给出, 这实际上假设了不出现样本相同标签不同的情况. 另外由<span class="math inline">\(L_{\mathcal{D}}(f)=0\)</span>可知, PAC可学习还要求了假设类<span class="math inline">\(\mathcal{H}\)</span>必然包含<span class="math inline">\(f\)</span>.</p>
<p>如果假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 有很多函数<span class="math inline">\(m_{\mathcal{H}}\)</span>满足PAC可学习的定义给出的条件, 我们定义采样复杂度为满足条件的最小函数, 下面是有限假设类的一个例子:</p>
<blockquote>
<p>任意有限假设类是PAC可学习的,其采样复杂度满足:<span class="math inline">\(m_{\mathcal{H}}(\varepsilon,\delta)\le [\frac{log(|\mathcal{H}|)/\delta}{\varepsilon}]\)</span>.</p>
</blockquote>
<p>该定理用霍夫丁不等式(<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding's inequality</a>)可证, 霍夫丁不等式的证明可见链接或是深入理解机器学习一书附录B.4, 该定理的详细证明可见深入理解机器学习一书2.3节.</p>
<p><code>p.s.</code> 霍夫丁不等式(Hoeffding's inequality):</p>
<blockquote>
<p><span class="math inline">\(\theta_{1},...,\theta_{m}\)</span>是一个独立同分布的r.v.序列, 假设对所有的<span class="math inline">\(i\)</span>, <span class="math inline">\(E[\theta_{i}]=\mu\)</span>,且<span class="math inline">\(\mathbb{P}[a\le\theta_{i}\le b]=1\)</span>, 则对于<span class="math inline">\(\forall\varepsilon&gt;0\)</span> <span class="math display">\[\mathbb{P}[|\frac{1}{m}\sum^{m}_{i=1}\theta_{i}-\mu|&gt;\varepsilon]\le2\exp(-2m\varepsilon^{2}/(b-a)^{2})\]</span></p>
</blockquote>
<h3 id="广义损失函数下的不可知pac可学习">广义损失函数下的不可知PAC可学习</h3>
<p>为了能处理其他学习任务比方说多分类, 回归问题, 我们将损失函数进行如下范化: 广义损失函数:</p>
<ul>
<li>0-1损失, 这个损失函数用在二分类或者多分类问题中. <span class="math display">\[ l_{0-1}(h,(x,y))=0,where\{h(x)=y\};1,where \{h(x)\neq y\}\]</span></li>
<li>平方损失,这个损失函数用在回归问题中. <span class="math display">\[l_{sq}(h,(x,y))=(h(x)-y)^{2}\]</span></li>
</ul>
<p>广义损失函数下的不可知PAC可学习(Agnostic PAC Learnability for General Loss Functions):</p>
<blockquote>
<p>对于集合<span class="math inline">\(Z\)</span>和损失函数<span class="math inline">\(l:\mathcal{H}×Z\to \mathbb{R}_{+}\)</span>, 若存在一个函数<span class="math inline">\(m_{\mathcal{H}}:(0,1)^{2}\to\mathbb{N}\)</span>和一个学习算法<span class="math inline">\(A\)</span>, 使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和在<span class="math inline">\(Z\)</span>上的任一概率分布<span class="math inline">\(\mathcal{D}\)</span>, 当样本数量<span class="math inline">\(m\ge m_{\mathcal{H}}(\varepsilon,\delta)\)</span>时, 其中的样本由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 算法将以不小于<span class="math inline">\(1-\delta\)</span>的概率返回一个假设类<span class="math inline">\(h\)</span>, 使该假设类<span class="math inline">\(h\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(h)\le \min_{h&#39;\in\mathcal{H}}L_{\mathcal{D}}(h&#39;)+\varepsilon\)</span>.</p>
</blockquote>
<h3 id="对不可知的探讨">对不可知的探讨</h3>
<p>上文提到不可知PAC可学习是对PAC可学习的范化, 因为假设类<span class="math inline">\(\mathcal{H}\)</span>中未必有一个完美的分类器<span class="math inline">\(h\)</span>, 而这又有两种可能的深层原因:</p>
<ul>
<li><span class="math inline">\(\mathcal{H}\)</span>取得不够大, 没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>.</li>
<li><span class="math inline">\(\mathcal{H}\)</span>取得够大了, 但<span class="math inline">\(\mathcal{H}\)</span>仍没能包含能将西瓜完美分类的目标函数<span class="math inline">\(f\)</span>, 一种合理的推测就是根本不存在能将西瓜完美分类的<span class="math inline">\(f\)</span>. 比方说因为<span class="math inline">\(\mathcal{X}\)</span>中特征选取不是太好, 某好瓜和某坏瓜在选取的特征上的数据完全一致, 此时<span class="math inline">\(f\)</span>当然没法把他们区分开来.</li>
</ul>
<p>前者相对好办, 只要扩大<span class="math inline">\(\mathcal{H}\)</span>, 而后者就需要去寻找或者构造新的特征以区分好坏瓜.</p>
<h2 id="vc维">VC维</h2>
<p>由上节PAC可学习我们已经知道, 有限假设类假设类<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的充分条件. 但是否为必要条件呢? 答案是否定的. 事实上, 无限假设类也能是PAC可学习的, 只要它的VC维有限, 下面来介绍VC维.</p>
<p>我们先来看一个著名的定理: 没有免费的午餐定理(No-Free-Lunch), 这个定理说明了如果不对假设类加以限制, 任何学习算法总有很差表现的时候.</p>
<h3 id="没有免费的午餐定理">没有免费的午餐定理</h3>
<p>没有免费的午餐定理(No-Free-Lunch):</p>
<blockquote>
<p>对实例空间<span class="math inline">\(\mathcal{X}\)</span>上的0-1损失的二分任务, <span class="math inline">\(A\)</span>表示任意的学习算法. 样本大小<span class="math inline">\(m\)</span>为小于<span class="math inline">\(|\mathcal{X}|/2\)</span>的任意数.则在<span class="math inline">\(\mathcal{X}×\{0,1\}\)</span>上存在一个分布<span class="math inline">\(\mathcal{D}\)</span>, 使得:</p>
<ol type="1">
<li>存在一个函数<span class="math inline">\(f:\mathcal{X}\to\{0,1\}\)</span>满足<span class="math inline">\(L_{\mathcal{D}}(f)=0\)</span>.</li>
<li>样本量为<span class="math inline">\(m\)</span>的样本集<span class="math inline">\(S\)</span>, 由分布<span class="math inline">\(\mathcal{D}\)</span>独立同分布采样得到, 以至少<span class="math inline">\(\frac{1}{7}\)</span>的概率满足:<span class="math inline">\(L_{\mathcal{D}}(A(S))\ge \frac{1}{8}\)</span>.</li>
</ol>
</blockquote>
<p>这个定理陈述的是对每个学习器<span class="math inline">\(A\)</span>, 都存在一个学习任务(分布<span class="math inline">\(\mathcal{D}\)</span>)使其失败, 即使这个任务存能被另一个学习器成功学习(比方说关于假设类<span class="math inline">\(\mathcal{H}=\{f\}\)</span>的一个ERM学习器).</p>
<p>下面是没有免费的午餐定理的证明: 一个直观的看法如下, 令<span class="math inline">\(C\)</span>是大小为<span class="math inline">\(2m\)</span>的集合<span class="math inline">\(\mathcal{X}\)</span>的子集, 任何只观测到空间<span class="math inline">\(C\)</span>中一半样本的学习算法<span class="math inline">\(A\)</span>, 都不具有信息来反映<span class="math inline">\(C\)</span>中剩余样本. 因此存在一个可能性, 在<span class="math inline">\(C\)</span>中未观测到的样本上, 目标函数<span class="math inline">\(f\)</span>与<span class="math inline">\(A(S)\)</span>预测的完全不同(详细证明见深入理解机器学习一书5.1节):</p>
<ol type="1">
<li>从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>有<span class="math inline">\(T=2^{|\mathcal{X}|}\)</span>个函数,记为<span class="math inline">\(f_{1},...,f_{T}\)</span>. 对每个<span class="math inline">\(f_{i}\)</span>取分布<span class="math inline">\(\mathcal{D_{i}}\)</span>满足<span class="math inline">\(f_{i}\)</span>, 即如果<span class="math inline">\(y\neq f(x)\)</span>, 分布<span class="math inline">\(\mathcal{D_{i}}\)</span>抽取样本<span class="math inline">\(x×y\)</span>的概率置为0.</li>
<li>事实上, 此时<span class="math inline">\(|\mathcal{X}|\)</span>可直接取为<span class="math inline">\(2m\)</span>, 假定对<span class="math inline">\(|\mathcal{X}|=2m\)</span>定理得证, 那么此时任意加入新样本,只需将新样本的标签取成<span class="math inline">\(|1-A(S)|\)</span>, 就能保证<span class="math inline">\(L_{\mathcal{D}}(A(S))\)</span>仍大于<span class="math inline">\(\frac{1}{8}\)</span>, 而<span class="math inline">\(f\)</span>及<span class="math inline">\(\mathcal{D}\)</span>只需要做相应修改.</li>
<li>对任意学习算法<span class="math inline">\(A\)</span>从1.中的<span class="math inline">\(\{\mathcal{D}_{i}\}\)</span>找一个<span class="math inline">\(\mathcal{D}_{i}\)</span>使得<span class="math inline">\(E[L_{\mathcal{D_{i}}}(A(S))]\ge \frac{1}{4}\)</span>.</li>
<li><span class="math inline">\(P[L_{\mathcal{D}_{i}}(A(S))\ge\frac{1}{8}]\ge\frac{1}{7}\)</span></li>
</ol>
<p>注意到定理中并没有对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 假设类<span class="math inline">\(\mathcal{H}\)</span>的规模是随着样本量大小而指数型增长的, 从而导致学习失败. 于是乎如果我们要想学习成功, 就必须对假设类<span class="math inline">\(\mathcal{H}\)</span>加以限制, 那么如何去限制呢？</p>
<ul>
<li>我们已经知道任何有限假设类都是可学习的(Hoeffding测度集中不等式可证).</li>
<li>那么无限类呢?事实上, 有一个叫VC维的东西可以刻画出PAC可学习的假设类的增长速度如何, 直观的去看, VC维有限的假设类就是那些限制在集合<span class="math inline">\(C\)</span>上并且只能随着<span class="math inline">\(|C|\)</span>的增长而多项式增长而不是指数型增长的这类假设类, 这也被称为是小的有效规模.</li>
</ul>
<h3 id="打散">打散</h3>
<p>打散(Shattering):</p>
<blockquote>
<p>如果限制<span class="math inline">\(\mathcal{H}\)</span>在<span class="math inline">\(C\)</span>上是从<span class="math inline">\(C\)</span>到<span class="math inline">\(\{0,1\}\)</span>的所有函数的集合, 那么我们称<span class="math inline">\(\mathcal{H}\)</span>打散了集合<span class="math inline">\(C\)</span>, 此时<span class="math inline">\(|\mathcal{H}_{C}|=2^{|C|}\)</span>.</p>
</blockquote>
<p>这里<span class="math inline">\(\mathcal{H}\)</span>的状态也就是上文说的不加以限制的状态.</p>
<h3 id="vc维-1">VC维</h3>
<p>VC维(VC-dimension):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}\)</span>的VC维, 记为<span class="math inline">\(VCdim(\mathcal{H})\)</span>可以打散的最大的集合的大小. 如果<span class="math inline">\(\mathcal{H}\)</span>可以打散任何集合大小,我们说<span class="math inline">\(\mathcal{H}\)</span>的VC维是无穷的.</p>
</blockquote>
<p>回头再来看没有免费午餐定理,我们易得下述推论:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{X}\)</span>为一个无限定义域集, <span class="math inline">\(\mathcal{H}\)</span>为从<span class="math inline">\(\mathcal{X}\to\{0,1\}\)</span>上的所有映射集, 即<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>,则<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的.</p>
</blockquote>
<h3 id="偏差与复杂性权衡与偏差方差权衡">偏差与复杂性权衡与偏差方差权衡</h3>
<p>在PAC可学习框架下, 我们所用的方式是: 对于给定的假设类<span class="math inline">\(\mathcal{H}\)</span>(先验知识), 采用ERM范式(最小化训练误差)从中选取分类器<span class="math inline">\(h\)</span>.</p>
<p>我们将ERM范式返回的假设<span class="math inline">\(h_{ERM}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})\)</span>分解为两部分, 第一部分叫偏差, 由假设类具有的最小风险<span class="math inline">\(\min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)\)</span>所决定, 它反映了先验知识的质量; 第二部分叫复杂性<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})-\min_{h\in\mathcal{H}}L_{\mathcal{D}}(h)\)</span>, 是由过拟合引起的误差, 取决于假设类的大小或是复杂度, 又称为估计误差. 随着假设类变得越复杂, 偏差会变小, 但是复杂性会变大.</p>
<p>另一种更常见的分解方式是将ERM范式返回的假设<span class="math inline">\(h_{ERM}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})\)</span>分解为训练误差<span class="math inline">\(\min_{h\in\mathcal{H}}L_{S}(h)\)</span>和方差<span class="math inline">\(L_{\mathcal{D}}(h_{ERM})-\min_{h\in\mathcal{H}}L_{S}(h)\)</span>, 随着假设类变得越复杂, 训练误差会越来越小, 而泛化误差会有一个先增后减的过程. <img src="/pictures/vc2.svg" alt="机器学习3" /></p>
<h2 id="定理的证明">定理的证明</h2>
<p>定义说完了, 下面来看证明:</p>
<h3 id="推2">1推2</h3>
<p>一句话证明1推2:<span class="math inline">\(\mathcal{H}\)</span>一致收敛也就是说样本量足够时, <span class="math inline">\(S\)</span>是<span class="math inline">\(\varepsilon\)</span>-代表性样本, 现用<span class="math inline">\(\varepsilon/2\)</span>取代<span class="math inline">\(\varepsilon\)</span>, <span class="math inline">\(\varepsilon\)</span>-代表性样本的定义及ERM的返回规则得:对<span class="math inline">\(\forall h\in \mathcal{H}\)</span>,<span class="math display">\[L_{\mathcal{D}}(h_{S})\le L_{\mathcal{S}}(h_{S})+\varepsilon/2\le L_{\mathcal{S}}(h)+\varepsilon/2 \le L_{\mathcal{D}}(h)+\varepsilon/2+\varepsilon/2=L_{\mathcal{D}}(h)+\varepsilon\]</span>即由ERM(S)返回<span class="math inline">\(h_{S}\)</span>的泛化误差<span class="math inline">\(L_{\mathcal{D}}(h_{S})\)</span>能保证学习成功.</p>
<h3 id="推3">2推3</h3>
<p>2推3就非常显然, 2是对于假设类<span class="math inline">\(\mathcal{H}\)</span>采用了ERM学习算法的学习器在<span class="math inline">\(\mathcal{H}\)</span>能成功, 要证明3只要证明存在一个学习算法能成功, 显然存在(ERM).</p>
<h3 id="推4">3推4</h3>
<p>3推4更显然, 3是4的泛化, 所以4的条件成立时3必然能用, 再加上<span class="math inline">\(f\)</span>满足可实现的假设, 即存在完美的分类器使得泛化误差为0, 得证.</p>
<h3 id="推5">2推5</h3>
<p>2推5同上可证.</p>
<h3 id="推6">4推6</h3>
<p>4推6用到了没有免费的午餐定理, 这个定理可以叙述为如果<span class="math inline">\(\mathcal{H}\)</span>的VC维无限, 那么<span class="math inline">\(\mathcal{H}\)</span>就不是PAC可学习的, 来看这个命题的逆否命题, 即如果<span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 那么<span class="math inline">\(\mathcal{H}\)</span>的VC维有限.</p>
<h3 id="推6-1">5推6</h3>
<p>5推6的话同样用到了免费的午餐定理, 假设<span class="math inline">\(\mathcal{H}\)</span>的VC维无穷, 那么<span class="math inline">\(\mathcal{H}\)</span>不是PAC可学习的, 然后铁定不是采取了ERM之后的PAC可学习的了.</p>
<h3 id="推1">6推1</h3>
<p>这里只叙述证明思路:</p>
<ul>
<li>如果<span class="math inline">\(VCdim(\mathcal{H})=d&lt;+\infty\)</span>, 那么即使<span class="math inline">\(\mathcal{H}\)</span>无限, 当将其限制在一有限集合<span class="math inline">\(C\)</span>时, 其有效规模<span class="math inline">\(|\mathcal{H}_{C}|\)</span>只有<span class="math inline">\(O(|C|^{d})\)</span>.</li>
<li>假设类有一个小的有效规模时其一致收敛成立, 小的有效规模指的是<span class="math inline">\(|\mathcal{H}_{C}|\)</span>随<span class="math inline">\(|C|\)</span>按多项式方式增长.</li>
</ul>
<p>其中第一步实际上是将<span class="math inline">\(\mathcal{H}\)</span>的VC维有限时, <span class="math inline">\(\mathcal{H}\)</span>的增长速度为多项式增长这一事实明确下来, 证明用到了Sauer引理, 详细见深入理解机器学习一书6.5.1节; 2.的证明见深入理解机器学习一书6.5.2节.<span class="math inline">\(□\)</span></p>
<h1 id="统计学习的基本定理-定量形式">统计学习的基本定理-定量形式</h1>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一个由<span class="math inline">\(\mathcal{X}\to \{0,1\}\)</span>的映射函数构成的假设类, 且令损失函数为0-1损失. 假定<span class="math inline">\(VCdim(\mathcal{H})&lt;\infty\)</span>, 那么, 存在绝对常数<span class="math inline">\(C_{1},C_{2}\)</span>使得:</p>
</blockquote>
<blockquote>
<ol type="1">
<li><span class="math inline">\(\mathcal{H}\)</span>一致收敛, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m^{UC}_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
<li><span class="math inline">\(\mathcal{H}\)</span>是不可知PAC可学习的, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon^{2}}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{d+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
<li><span class="math inline">\(\mathcal{H}\)</span>是PAC可学习的, 若其样本复杂度满足: <span class="math display">\[C_{1}\frac{d+log(1/\delta)}{\varepsilon}\le m_{\mathcal{H}}(\varepsilon,\delta)\le C_{2}\frac{dlog(1/\varepsilon)+log(1/\delta)}{\varepsilon^{2}}\]</span></li>
</ol>
</blockquote>
<p><code>p.s.</code>由上述定理可以看出, 一致收敛与不可知PAC可学习的需要的样本量一致, 而PAC可学习的需要的样本量少一些. 该定理的证明在深入理解机器学习一书第28章给出.</p>
<h1 id="不一致可学习">不一致可学习</h1>
<p>上文所讨论的PAC可学习的概念是考虑精度和置信参数来决定样本数量, 且样本标签分布与内在的样本数据分布是一致的. 因此, PAC可学习等价于VC维有限. 下面要讨论的是不一致可学习这个概念, 这个概念是不可知PAC可学习的严格松弛, 它允许样本数量依赖假设空间<span class="math inline">\(\mathcal{H}\)</span>, 下面来看其定义:</p>
<p>不一致可学习(Nonuniform learnability):</p>
<blockquote>
<p>称假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的若存在一个学习算法<span class="math inline">\(A\)</span>和一个函数<span class="math display">\[m^{NUL}_{\mathcal{H}}:(0,1)\times(0,1)×\mathcal{H}\to\mathbb{N}\]</span>使得对于<span class="math inline">\(\forall\varepsilon,\delta\in(0,1)\)</span>和<span class="math inline">\(h\in\mathcal{H}\)</span>, 如果样本数量<span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\)</span>, 那么对每个分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>, <span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>.</p>
</blockquote>
<p>这个概念要求输出假设与假设类中其他假设相比具有<span class="math inline">\((\varepsilon,\delta)\)</span>的竞争力, 同时从定义也很容易得到不可知PAC可学习一定能推得不一致可学习, 且不一致可学习是不可知PAC可学习的严格松弛. 下面举个例子说明两者的严格松弛关系:</p>
<ul>
<li>考虑二分问题, 假设<span class="math inline">\(\mathcal{H}_{n}\)</span>是<span class="math inline">\(n\)</span>次多项式构成的假设类, 即<span class="math inline">\(\mathcal{H}_{n}\)</span>为<span class="math inline">\(h(x)=sign(p(x))\)</span>的分类器的集合.这里<span class="math inline">\(p(x)\)</span>是<span class="math inline">\(\mathbb{R}\to\mathbb{R}\)</span>的<span class="math inline">\(n\)</span>次多项式.令<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 容易得到<span class="math inline">\(VCdim(\mathcal{H})=+\infty\)</span>, 但<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</li>
</ul>
<p>事实上, 有一个定理清晰的描述了两者的关系:</p>
<blockquote>
<p>两分类器的假设类<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 当且仅当它是不可知PAC可学习的可数并.</p>
</blockquote>
<p>下面来看这个定理的证明:</p>
<ul>
<li><p>必要性: 假定<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 对于每个<span class="math inline">\(n\in\mathbb{N}\)</span>,令<span class="math display">\[\mathcal{H}_{n}=\{h\in\mathcal{H}:m^{NUL}_{\mathcal{H}}(\frac{1}{8},\frac{1}{7},h)\le n\}\]</span>显然<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>. 此外, 由<span class="math inline">\(m^{NUL}_{\mathcal{H}}\)</span>定义知, 对于任何关于<span class="math inline">\(\mathcal{H}_{n}\)</span>满足可实现性假设的分布<span class="math inline">\(\mathcal{D}\)</span>, <span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 选择样本<span class="math inline">\(S\)</span>的概率大于等于<span class="math inline">\(\frac{6}{7}\)</span>. 则<span class="math inline">\(L_{\mathcal{D}}(A(S))\le\frac{1}{8}\)</span>. 由统计学习基本定理知, <span class="math inline">\(VCdim(\mathcal{H}_{n})&lt;+\infty\)</span>, 因此<span class="math inline">\(\mathcal{H}_{n}\)</span>是不可知PAC可学习的.</p></li>
<li><p>充分性: 充分性的证明依赖于下述引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote></li>
</ul>
<p>该引理证明留给下小节, 那么由统计学习基本定理知, 不可知PAC可学习与一致收敛性等价,即得证<span class="math inline">\(□\)</span></p>
<h2 id="结构风险最小化srm">结构风险最小化SRM</h2>
<p>本节主要是关于结构风险最小化(SRM:Structural Risk Minimization)以及利用结构风险最小化算法来证明上节未证明的引理.</p>
<p>目前为止, 我们都是通过具体化一个假设类<span class="math inline">\(\mathcal{H}\)</span>来利用先验知识, 并且相信这样一个假设类中包含完成当前任务的有效预测器. 而另一种表达先验知识的方法则是将假设类<span class="math inline">\(\mathcal{H}\)</span>上的偏好具体化.</p>
<p>在结构风险最小化范式中, 我们首先假定假设类<span class="math inline">\(\mathcal{H}\)</span>能够写成<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛, 且样本复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\)</span>, 然后具体化一个权重函数<span class="math display">\[\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\]</span>这个权重函数可也反映每个假设类的重要性, 或是假设类复杂性的度量.例如<span class="math inline">\(\mathcal{H}\)</span>是所有多项式分类器构成的类, <span class="math inline">\(\mathcal{H}_{n}\)</span>表示<span class="math inline">\(n\)</span>次多项式分类器构成的类.定义下式函数<span class="math inline">\(\varepsilon_{n}\)</span>:<span class="math inline">\(\mathbb{N}×(0,1)\to(0,1)\)</span> <span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\]</span>由<span class="math inline">\(\mathcal{H}_{n}\)</span>满足一致收敛性, <span class="math inline">\(\forall m\in\mathbb{N}\)</span>,<span class="math inline">\(\forall\delta\in(0,1)\)</span>, 大小为<span class="math inline">\(m\)</span>的样本<span class="math inline">\(S\)</span>由<span class="math inline">\(\mathcal{D}\)</span>独立抽样得到, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span> <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta)\]</span>这就是我们所关心的训练误差与泛化误差之间的差值. 此时权重函数可取成<span class="math inline">\(\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>或是<span class="math inline">\(\omega(n)=2^{-n}\)</span>.</p>
<p>下面将上述例子写成定理的形式,并由此引出结构风险最小化(SRM):</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>,<span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>. 令<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>为一权重函数. 令<span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\]</span> 则对<span class="math inline">\(\forall\delta\in(0,1)\)</span>,任意的分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率不小于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>其中<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>.</p>
</blockquote>
<p>证明: 对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>, 定义<span class="math inline">\(\delta_{n}=\omega(n)\delta\)</span>, 固定<span class="math inline">\(n\)</span>, 由一致收敛性, 在选择样本<span class="math inline">\(S\)</span>的概率不低于<span class="math inline">\(1-\delta\)</span>的条件下, <span class="math display">\[\forall h \in \mathcal{H}_{n},|L_{\mathcal{D}}(h)-L_{S}(h)|\le\varepsilon_{n}(m,\delta_{n})\]</span>应用<span class="math inline">\(n=1,2,...,\)</span>的联合界, 我们得到上述结论以不低于<span class="math display">\[1-\sum_{n}\delta_{n}=1-\delta\sum_{n}\omega(n)\ge1-\delta\]</span>的概率对<span class="math inline">\(\forall n\in\mathbb{N}\)</span>都成立, 最后令<span class="math inline">\(n(h)=\min\{n:h\in\mathcal{H}_{n}\}\)</span>自然也成立.<span class="math inline">\(□\)</span></p>
<p>结构风险最小化(SRM)则是寻找<span class="math inline">\(h\)</span>来最小化这个上界, 以保证<span class="math inline">\(L_{\mathcal{D}}(h)\)</span>不是那么大, 下面是结构风险最小化(SRM)的定义:</p>
<ol type="1">
<li>先验:<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>; 权重函数<span class="math inline">\(\omega:\mathbb{N}\to[0,1],s.t.\sum_{n}\omega(n)\le1\)</span>.</li>
<li>定义:<span class="math inline">\(\varepsilon_{n}(m,\delta)=min\{\varepsilon\in(0,1):m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le m\}\)</span>; <span class="math inline">\(n(h)=min\{n:h\in\mathcal{H}_{n}\}\)</span>.</li>
<li>输入:训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>.</li>
<li>输出:<span class="math inline">\(h\in argmin_{h\in\mathcal{H}}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\)</span>.</li>
</ol>
<p><code>p.s.</code>结构风险最小化(SRM)与经验风险最小化(ERM)最大的不同就是, 经验风险最小化(ERM)只关心训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>, 而结构风险最小化(SRM)兼顾了训练误差<span class="math inline">\(L_{\mathcal{S}}(h)\)</span>和训练误差与泛化误差之间的差值<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\)</span>.</p>
<p>下面用SRM来证明上节未能证明的一个引理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, 如果<span class="math inline">\(\mathcal{H}_{n}\)</span>是一致收敛的, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的.</p>
</blockquote>
<p>事实上利用SRM算法我们能得到下述更进一步的结论:</p>
<blockquote>
<p>假设类<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\mathcal{H}_{n}\)</span>, <span class="math inline">\(\mathcal{H}_{n}\)</span>一致收敛, 且复杂度函数为<span class="math inline">\(m^{UC}_{\mathcal{H}_{n}}\)</span>.如果<span class="math inline">\(\omega\)</span>:<span class="math inline">\(\mathbb{N}\to[0,1]\)</span>, <span class="math inline">\(s.t.\omega(n)=\frac{6}{\pi^{2}n^{2}}\)</span>, 那么<span class="math inline">\(\mathcal{H}\)</span>是不一致可学习的, 且满足有<span class="math display">\[m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h)\le m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\frac{6\delta}{(\pi n(h))^{2}})\]</span></p>
</blockquote>
<p>证明: 假定<span class="math inline">\(A\)</span>是考虑权重函数<span class="math inline">\(\omega\)</span>的结构风险最小化算法, 对于<span class="math inline">\(\forall h\in\mathcal{H}_{n},\forall\varepsilon,\delta\in(0,1)\)</span>, 令<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\varepsilon,\omega(n(h))·\delta)\)</span>,应用上述定理, 可以得到<span class="math display">\[\forall h \in \mathcal{H}_{n},L_{\mathcal{D}}(h)\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>这个定理对于由结构风险规则返回的假设<span class="math inline">\(A(S)\)</span>成立. 通过结构风险最小化的定义可得,<span class="math display">\[L_{\mathcal{D}}(A(S))\le\min_{h}[L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)]\le L_{S}(h)+\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\]</span>如果<span class="math inline">\(m\ge m^{UC}_{\mathcal{H}_{n(h)}}(\frac{\varepsilon}{2},\omega(n(h))·\delta)\)</span>, 那么<span class="math inline">\(\varepsilon_{n(h)}(m,\omega(n(h))·\delta)\le\frac{\varepsilon}{2}\)</span>. 此外, 从每个<span class="math inline">\(\mathcal{H}_{n}\)</span>的一致收敛性, 我们可得到下式成立的概率大于<span class="math inline">\(1-\delta\)</span>,<span class="math display">\[L_{S}(h)\le L_{\mathcal{D}}(h)+\frac{\varepsilon}{2}\]</span>综上所述, 可得<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>, 定理得证.<span class="math inline">\(□\)</span></p>
<p>单独比较不一致可学习和不可知PAC可学习是非常有意思的, 算法会在全空间<span class="math inline">\(\mathcal{H}\)</span>上搜索一个模型, 而不是在特定的<span class="math inline">\(\mathcal{H}_{n}\)</span>上搜索一个模型, 利用先验知识的缺陷所带来的成本就是增加复杂度与特定的<span class="math inline">\(h\in\mathcal{H}_{n}\)</span>相竞争.假定对于所有的<span class="math inline">\(n\)</span>, <span class="math inline">\(VCdim(\mathcal{H}_{n})=n\)</span>, 由统计学习的基本定理-定量形式知<span class="math display">\[m^{UC}_{\mathcal{H}_{n}}(\varepsilon,\delta)\le C\frac{n+log(1/\delta)}{\varepsilon^{2}}\]</span> 一个直接的计算表明<span class="math display">\[m^{NUL}_{\mathcal{H}_{n}}(\varepsilon,\delta,h)-m^{UC}_{\mathcal{H}_{n}}(\frac{\varepsilon}{2},\delta)\le 4C\frac{2\log(2n)}{\varepsilon^{2}}\]</span>代价增加了类的索引, 可以解释为反映已知的假设类<span class="math inline">\(\mathcal{H}\)</span>的好的先验知识的排序值.</p>
<h3 id="不一致可学习的没有免费午餐定理">不一致可学习的没有免费午餐定理</h3>
<p>在不一致可学习框架下,没有免费的午餐定理(No-Free-Lunch for Nonuniform Learnability)也是成立的,也就是说, 当样本域无限时, 不存在关于所有确定性二分类器所构成的类的不一致学习器(尽管对于每一个分类器存在一个尝试算法能够学习包含这些分类器假设的结构风险最小化).</p>
<h2 id="最小描述长度mdl">最小描述长度MDL</h2>
<p><span class="math inline">\(\mathcal{H}\)</span>为假设类, 将<span class="math inline">\(\mathcal{H}\)</span>写成单个类的可数并, 即<span class="math inline">\(\mathcal{H}=\cup_{n\in\mathbb{N}}\{h_{n}\}\)</span>.</p>
<p>由Hoeffding不等式知, 每一个单类有一致收敛性, 收敛速率<span class="math inline">\(m^{UC}(\varepsilon,\delta)=\frac{\log(2/ \delta)}{2\varepsilon^{2}}\)</span>, 该结果的证明见深入理解机器学习一书page26. 因此, <span class="math display">\[\varepsilon_{n}(m,\delta)=\min\{\varepsilon\in (0,1):m^{UC}(\varepsilon,\delta)\le m\}\]</span>所给出的函数<span class="math inline">\(\varepsilon_{n}\)</span>变成了<span class="math display">\[\varepsilon_{n}(m,\delta)=\sqrt{\frac{\log(2/\delta)}{2m}}\]</span>且结构风险最小化SRM变成了:<span class="math display">\[argmin_{h_{n}\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(n))+\log(2/\delta)}{2m}}\]</span>等价的, 我们可以认为<span class="math inline">\(\omega\)</span>是从<span class="math inline">\(\mathcal{H}\to[0,1]\)</span>的函数, 然后结构结构风险最小化SRM变成了:<span class="math display">\[argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{-\log(\omega(h))+\log(2/\delta)}{2m}}\]</span>这节讨论一种特别方便的方式定义权重函数<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>, 这个函数与假设的描述长度有关. 下面介绍其背景:</p>
<p>对于每个假设类<span class="math inline">\(\mathcal{H}\)</span>, 我们想知道如何描述和表示每一个类的假设<span class="math inline">\(h\)</span>, 英语, 汇编语言, 数学公式等等形式, 当我们选定某种语言形式, 那么一个假设<span class="math inline">\(h\)</span>肯定能被一些特定字母<span class="math inline">\(\Sigma\)</span>组成的有限字符串所描述, <span class="math inline">\(\Sigma\)</span>我们称之为字母表. 例如, 令<span class="math inline">\(\Sigma=\{0,1\}\)</span>, <span class="math inline">\(\sigma=(0,1,1,1,0)\)</span>为一字符串且字符串的长度<span class="math inline">\(|\sigma|=5\)</span>, 所有有限长度的字符串用<span class="math inline">\(\Sigma^{+}\)</span>表示, 对假设类<span class="math inline">\(\mathcal{H}\)</span>的描述可用一个函数<span class="math inline">\(d:\mathcal{H}\to\Sigma^{+}\)</span>表示, <span class="math inline">\(d(h)\)</span>将<span class="math inline">\(\mathcal{H}\)</span>中每个假设<span class="math inline">\(h\)</span>映射为一个字符串<span class="math inline">\(d(h)\)</span>, <span class="math inline">\(d(h)\)</span>称为<span class="math inline">\(h\)</span>的描述长度, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度. 我们要求描述语言<span class="math inline">\(d(h)\)</span>无前缀, 即不同的<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>, <span class="math inline">\(d(h)\)</span>不是<span class="math inline">\(d(h&#39;)\)</span>的前缀. 无前缀的字符串满足如下组合性质:</p>
<p>Kraft不等式:</p>
<blockquote>
<p>如果<span class="math inline">\(S\subset\{0,1\}^{+}\)</span>是一个无前缀的字符串集合,则<span class="math inline">\(\sum_{\sigma\in S}\frac{1}{2^{|\sigma|}}\le1\)</span>.</p>
</blockquote>
<p>根据Kraft不等式, 任何无前缀的语言都能给出假设类<span class="math inline">\(\mathcal{H}\)</span>的权重函数<span class="math inline">\(\omega\)</span>, 我们可简单的设置为<span class="math inline">\(\omega(h)=\frac{1}{2^{|h|}}\)</span>,于是便有了下述定理:</p>
<blockquote>
<p><span class="math inline">\(\mathcal{H}\)</span>是一假设类, <span class="math inline">\(d:\mathcal{H}\to\{0,1\}^{+}\)</span>是<span class="math inline">\(\mathcal{H}\)</span>的一无前缀描述语言,对于样本量<span class="math inline">\(m\)</span>, 置信参数<span class="math inline">\(\delta&gt;0\)</span>和概率分布<span class="math inline">\(\mathcal{D}\)</span>, 下式成立的概率大于<span class="math inline">\(1-\delta\)</span>:<span class="math display">\[\forall h \in \mathcal{H},L_{\mathcal{D}}(h)\le L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\]</span>这里<span class="math inline">\(|h|\)</span>是指<span class="math inline">\(d(h)\)</span>的长度.</p>
</blockquote>
<p>类似结构风险最小化(SRM), 最小描述长度(MDL)这种考虑了训练误差和减小描述长度, 这就得到了减小描述长度的学习范式. 最小描述长度(MDL:Minimum Description Length):</p>
<ol type="1">
<li>先验: 假设类<span class="math inline">\(\mathcal{H}\)</span>由定义在<span class="math inline">\(\{0,1\}\)</span>上的无前缀语言<span class="math inline">\(d\)</span>描述;对于任何一个<span class="math inline">\(h\in\mathcal{H}\)</span>, <span class="math inline">\(|h|\)</span>表示<span class="math inline">\(d(h)\)</span>的长度.</li>
<li>输入: 训练集<span class="math inline">\(S\)</span>, 置信度<span class="math inline">\(\delta\)</span>.</li>
<li>输出: <span class="math inline">\(h\in argmin_{h\in\mathcal{H}}L_{S}(h)+\sqrt{\frac{|h|+\log(2/\delta)}{2m}}\)</span></li>
</ol>
<h3 id="奥卡姆剃刀原理">奥卡姆剃刀原理</h3>
<p>上届的定理指出, 对于经验风险相同的两个假设, 描述长度较小的假设, 其真实风险的误差界更小, 因此这个结果表达了一个哲学理念, 这也是著名的奥卡姆剃刀原理(Occam’s Razor):</p>
<blockquote>
<p>短的解析更有效.</p>
</blockquote>
<p><code>p.s.</code>假象下列情形, 两个假设<span class="math inline">\(h\)</span>, <span class="math inline">\(h&#39;\)</span>在数学语言下的描述长度分别为10,100; 但在中文下的长度分别为100,10. 因此这里笔者理解为对大多数语言的来说短的解析, 或是期望意义下的最短的解析.</p>
<h1 id="一致性">一致性</h1>
<p>最后给出一个比不一致可学习更松弛的可学习概念: 一致性, 这个概念允许所欲要的样本不仅依赖于<span class="math inline">\(\varepsilon,\delta\)</span>和<span class="math inline">\(h\)</span>,而且依赖于数据所依托的分布<span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>一致性(Consistency):</p>
<blockquote>
<p><span class="math inline">\(\mathcal{P}\)</span>表示<span class="math inline">\(Z\)</span>上的概率分布, 称一个学习规则<span class="math inline">\(A\)</span>关于<span class="math inline">\(\mathcal{H}\)</span>和<span class="math inline">\(\mathcal{P}\)</span>满足一致性, 若存在一个函数<span class="math display">\[m^{CON}_{\mathcal{H}}:(0,1)^{2}×\mathcal{H}×\mathcal{P}\to\mathbb{N}\]</span>使得对<span class="math inline">\(\forall h\in\mathcal{H}\)</span>,<span class="math inline">\(\forall\mathcal{D}\in\mathcal{P}\)</span>,<span class="math inline">\(\forall\varepsilon\)</span>,<span class="math inline">\(\delta\in(0,1)\)</span>, <span class="math inline">\(m\ge m^{NUL}_{\mathcal{H}}(\varepsilon,\delta,h,\mathcal{D})\)</span>,下式成立的概率不小于1-δ<span class="math inline">\(L_{\mathcal{D}}(A(S))\le L_{\mathcal{D}}(h)+\varepsilon\)</span>.</p>
</blockquote>
<h2 id="memorize算法">Memorize算法</h2>
<p>考虑如下定义的分类预测算法Memorize, 这个算法对新样本的预测永远都是某个固定的标签,比如说是训练集中出现次数最多的标签. 事实上, Memorize算法对于任何数据集上所有函数构成的类都满足一致性, 但这个算法很明显没有什么用.</p>
<p><code>写在最后</code>: 本文主要是写了写PAC可学习框架下有关VC维的内容, 这个VC维(Vapnik–Chervonenkis dimension)其实代表两位大牛:svm发明者<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Vladimir_Vapnik">Vapnik</a>和俄罗斯的数学家<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Alexey_Chervonenkis">Chervonenkis</a>. 进一步内容的Rademacher复杂度则是我下一篇博客的内容, 西瓜书将其放在12.5节, 深入理解机器学习一书则是将其放在第四部分高级理论26-27章, 针对多分类问题的Natarajan维在29章.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PAC%E5%8F%AF%E5%AD%A6%E4%B9%A0/" rel="tag"># PAC可学习</a>
              <a href="/tags/%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7/" rel="tag"># 一致收敛性</a>
              <a href="/tags/%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86/" rel="tag"># 没有免费午餐定理</a>
              <a href="/tags/VC%E7%BB%B4/" rel="tag"># VC维</a>
              <a href="/tags/%E4%B8%8D%E4%B8%80%E8%87%B4%E5%8F%AF%E5%AD%A6%E4%B9%A0/" rel="tag"># 不一致可学习</a>
              <a href="/tags/%E4%B8%80%E8%87%B4%E6%80%A7/" rel="tag"># 一致性</a>
              <a href="/tags/ERM/" rel="tag"># ERM</a>
              <a href="/tags/SRM/" rel="tag"># SRM</a>
              <a href="/tags/MDL/" rel="tag"># MDL</a>
              <a href="/tags/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80/" rel="tag"># 奥卡姆剃刀</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2017/12/06/Getting-and-Cleaning-Data/" rel="next" title="Getting and Cleaning Data">
      Getting and Cleaning Data <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">统计学习理论的基本定理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7"><span class="nav-number">1.1.</span> <span class="nav-text">一致收敛性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#varepsilon-%E4%BB%A3%E8%A1%A8%E6%80%A7%E6%A0%B7%E6%9C%AC"><span class="nav-number">1.1.1.</span> <span class="nav-text">\(\varepsilon\)-代表性样本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7-1"><span class="nav-number">1.1.2.</span> <span class="nav-text">一致收敛性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96erm"><span class="nav-number">1.1.3.</span> <span class="nav-text">经验风险最小化(ERM)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pac%E5%8F%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">PAC可学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%8F%AF%E7%9F%A5pac%E5%8F%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.1.</span> <span class="nav-text">不可知PAC可学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pac%E5%8F%AF%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">PAC可学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8B%E7%9A%84%E4%B8%8D%E5%8F%AF%E7%9F%A5pac%E5%8F%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.3.</span> <span class="nav-text">广义损失函数下的不可知PAC可学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E4%B8%8D%E5%8F%AF%E7%9F%A5%E7%9A%84%E6%8E%A2%E8%AE%A8"><span class="nav-number">1.2.4.</span> <span class="nav-text">对不可知的探讨</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vc%E7%BB%B4"><span class="nav-number">1.3.</span> <span class="nav-text">VC维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E7%9A%84%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86"><span class="nav-number">1.3.1.</span> <span class="nav-text">没有免费的午餐定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%93%E6%95%A3"><span class="nav-number">1.3.2.</span> <span class="nav-text">打散</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vc%E7%BB%B4-1"><span class="nav-number">1.3.3.</span> <span class="nav-text">VC维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%8E%E5%A4%8D%E6%9D%82%E6%80%A7%E6%9D%83%E8%A1%A1%E4%B8%8E%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E6%9D%83%E8%A1%A1"><span class="nav-number">1.3.4.</span> <span class="nav-text">偏差与复杂性权衡与偏差方差权衡</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E7%90%86%E7%9A%84%E8%AF%81%E6%98%8E"><span class="nav-number">1.4.</span> <span class="nav-text">定理的证明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A82"><span class="nav-number">1.4.1.</span> <span class="nav-text">1推2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A83"><span class="nav-number">1.4.2.</span> <span class="nav-text">2推3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A84"><span class="nav-number">1.4.3.</span> <span class="nav-text">3推4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A85"><span class="nav-number">1.4.4.</span> <span class="nav-text">2推5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A86"><span class="nav-number">1.4.5.</span> <span class="nav-text">4推6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A86-1"><span class="nav-number">1.4.6.</span> <span class="nav-text">5推6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A81"><span class="nav-number">1.4.7.</span> <span class="nav-text">6推1</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86-%E5%AE%9A%E9%87%8F%E5%BD%A2%E5%BC%8F"><span class="nav-number">2.</span> <span class="nav-text">统计学习的基本定理-定量形式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8D%E4%B8%80%E8%87%B4%E5%8F%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">不一致可学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96srm"><span class="nav-number">3.1.</span> <span class="nav-text">结构风险最小化SRM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E4%B8%80%E8%87%B4%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86"><span class="nav-number">3.1.1.</span> <span class="nav-text">不一致可学习的没有免费午餐定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E6%8F%8F%E8%BF%B0%E9%95%BF%E5%BA%A6mdl"><span class="nav-number">3.2.</span> <span class="nav-text">最小描述长度MDL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86"><span class="nav-number">3.2.1.</span> <span class="nav-text">奥卡姆剃刀原理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">4.</span> <span class="nav-text">一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#memorize%E7%AE%97%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">Memorize算法</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">msgsxj</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/msgsxj" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;msgsxj" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:msgsxj@gmail.com" title="E-Mail → mailto:msgsxj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">msgsxj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v5.4.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.6.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
